Training parameters: {'iterations': 20000, 'position_lr_init': 0.00019, 'position_lr_final': 1.9e-06, 'position_lr_delay_mult': 0.01, 'position_lr_max_steps': 20000, 'feature_lr': 0.002, 'opacity_lr': 0.008, 'scaling_lr': 0.005, 'rotation_lr': 0.001, 'percent_dense': 0.01, 'lambda_dssim': 0.2, 'densification_interval': 200, 'opacity_reset_interval': 4000, 'densify_from_iter': 500, 'densify_until_iter': 8000, 'densify_grad_threshold': 2.6e-05, 'random_background': False, 'sample_pseudo_interval': 1, 'start_sample_pseudo': 0, 'end_sample_pseudo': 30000}
Iter:99, L1 loss=0.05724, Total loss=0.07876, Time:14
Iter:199, L1 loss=0.0344, Total loss=0.04979, Time:13
Iter:299, L1 loss=0.02211, Total loss=0.03344, Time:13
Iter:399, L1 loss=0.01344, Total loss=0.02137, Time:8
Iter:499, L1 loss=0.008442, Total loss=0.01517, Time:13
Iter:599, L1 loss=0.006814, Total loss=0.0118, Time:9
Iter:699, L1 loss=0.005665, Total loss=0.009678, Time:9
Iter:799, L1 loss=0.005006, Total loss=0.008088, Time:13
Iter:899, L1 loss=0.003726, Total loss=0.005711, Time:13
Iter:999, L1 loss=0.00447, Total loss=0.006746, Time:12
Iter:1099, L1 loss=0.003391, Total loss=0.005109, Time:11
Iter:1199, L1 loss=0.003447, Total loss=0.004956, Time:13
Iter:1299, L1 loss=0.002725, Total loss=0.003588, Time:11
Iter:1399, L1 loss=0.002296, Total loss=0.002899, Time:13
Iter:1499, L1 loss=0.002552, Total loss=0.003298, Time:14
Iter:1599, L1 loss=0.002679, Total loss=0.003367, Time:11
Iter:1699, L1 loss=0.002572, Total loss=0.003219, Time:11
Iter:1799, L1 loss=0.001685, Total loss=0.001878, Time:15
Iter:1899, L1 loss=0.001719, Total loss=0.001961, Time:15
Iter:1999, L1 loss=0.001495, Total loss=0.001825, Time:14
Testing Speed: 232.60337178349602 fps
Testing Time: 0.21495819091796875 s

[ITER 2000] Evaluating test: SSIM = 0.8523035597801208, PSNR = 17.89594018936157
Testing Speed: 264.4691243852201 fps
Testing Time: 0.01134347915649414 s

[ITER 2000] Evaluating train: SSIM = 0.9999495546023051, PSNR = 48.407441457112625
Iter:2000, total_points:42307
Iter:2099, L1 loss=0.001586, Total loss=0.00167, Time:13
Iter:2199, L1 loss=0.001425, Total loss=0.001538, Time:15
Iter:2299, L1 loss=0.001361, Total loss=0.001513, Time:16
Iter:2399, L1 loss=0.00122, Total loss=0.001268, Time:13
Iter:2499, L1 loss=0.001178, Total loss=0.001229, Time:16
Iter:2599, L1 loss=0.001003, Total loss=0.00103, Time:15
Iter:2699, L1 loss=0.001149, Total loss=0.001179, Time:14
Iter:2799, L1 loss=0.001243, Total loss=0.001256, Time:16
Iter:2899, L1 loss=0.0008724, Total loss=0.0008436, Time:16
Iter:2999, L1 loss=0.0007018, Total loss=0.0006691, Time:17
Iter:3099, L1 loss=0.0008893, Total loss=0.0008751, Time:15
Iter:3199, L1 loss=0.0007627, Total loss=0.0007432, Time:15
Iter:3299, L1 loss=0.001269, Total loss=0.001258, Time:18
Iter:3399, L1 loss=0.00118, Total loss=0.001247, Time:18
Iter:3499, L1 loss=0.0006467, Total loss=0.00062, Time:19
Iter:3599, L1 loss=0.0006355, Total loss=0.0005817, Time:19
Iter:3699, L1 loss=0.001023, Total loss=0.000943, Time:18
Iter:3799, L1 loss=0.0007808, Total loss=0.0007401, Time:19
Iter:3899, L1 loss=0.0007424, Total loss=0.0006813, Time:20
Iter:3999, L1 loss=0.0008476, Total loss=0.0007745, Time:20
Iter:4099, L1 loss=0.000955, Total loss=0.0009384, Time:25
Iter:4199, L1 loss=0.0007765, Total loss=0.0007352, Time:22
Iter:4299, L1 loss=0.0008438, Total loss=0.0007938, Time:26
Iter:4399, L1 loss=0.0005786, Total loss=0.0005268, Time:26
Iter:4499, L1 loss=0.0007967, Total loss=0.0007482, Time:26
Iter:4599, L1 loss=0.0006712, Total loss=0.0006169, Time:26
Iter:4699, L1 loss=0.000607, Total loss=0.0005596, Time:25
Iter:4799, L1 loss=0.0005986, Total loss=0.0005477, Time:26
Iter:4899, L1 loss=0.000659, Total loss=0.0005755, Time:25
Iter:4999, L1 loss=0.0005059, Total loss=0.0004477, Time:27
Iter:5099, L1 loss=0.0005907, Total loss=0.0005129, Time:27
Iter:5199, L1 loss=0.0005408, Total loss=0.0004812, Time:27
Iter:5299, L1 loss=0.0006348, Total loss=0.0005375, Time:27
Iter:5399, L1 loss=0.0005166, Total loss=0.000426, Time:28
Iter:5499, L1 loss=0.0005198, Total loss=0.0004835, Time:28
Iter:5599, L1 loss=0.0004713, Total loss=0.0004164, Time:27
Iter:5699, L1 loss=0.0004889, Total loss=0.0004444, Time:27
Iter:5799, L1 loss=0.0004453, Total loss=0.000397, Time:28
Iter:5899, L1 loss=0.0004812, Total loss=0.0004241, Time:25
Iter:5999, L1 loss=0.0004717, Total loss=0.0004155, Time:25
Iter:6099, L1 loss=0.0004513, Total loss=0.0003996, Time:23
Iter:6199, L1 loss=0.0003857, Total loss=0.0003308, Time:24
Iter:6299, L1 loss=0.0005063, Total loss=0.0004601, Time:25
Iter:6399, L1 loss=0.0004382, Total loss=0.0003889, Time:25
Iter:6499, L1 loss=0.0005152, Total loss=0.0004645, Time:24
Iter:6599, L1 loss=0.0004187, Total loss=0.0003702, Time:24
Iter:6699, L1 loss=0.0004783, Total loss=0.0004212, Time:25
Iter:6799, L1 loss=0.0003596, Total loss=0.0003069, Time:26
Iter:6899, L1 loss=0.0004742, Total loss=0.0004197, Time:25
Iter:6999, L1 loss=0.0004685, Total loss=0.0004293, Time:24
Iter:7099, L1 loss=0.0004339, Total loss=0.0003744, Time:29
Iter:7199, L1 loss=0.000374, Total loss=0.0003278, Time:29
Iter:7299, L1 loss=0.0004398, Total loss=0.0003914, Time:26
Iter:7399, L1 loss=0.0005606, Total loss=0.0004383, Time:29
Iter:7499, L1 loss=0.0003999, Total loss=0.0003504, Time:29
Iter:7599, L1 loss=0.0003626, Total loss=0.0003137, Time:30
Iter:7699, L1 loss=0.000337, Total loss=0.0002931, Time:25
Iter:7799, L1 loss=0.0003467, Total loss=0.0003055, Time:26
Iter:7899, L1 loss=0.0004065, Total loss=0.0003607, Time:25
Iter:7999, L1 loss=0.0003744, Total loss=0.0003238, Time:28
Iter:8099, L1 loss=0.0005297, Total loss=0.0004776, Time:32
Iter:8199, L1 loss=0.000487, Total loss=0.0004319, Time:31
Iter:8299, L1 loss=0.0003785, Total loss=0.0003302, Time:30
Iter:8399, L1 loss=0.0003566, Total loss=0.0003064, Time:33
Iter:8499, L1 loss=0.000439, Total loss=0.0003848, Time:31
Iter:8599, L1 loss=0.0003104, Total loss=0.0002635, Time:30
Iter:8699, L1 loss=0.0003606, Total loss=0.0003182, Time:32
Iter:8799, L1 loss=0.0004147, Total loss=0.0003635, Time:31
Iter:8899, L1 loss=0.0003271, Total loss=0.0002886, Time:30
Iter:8999, L1 loss=0.0003149, Total loss=0.0002709, Time:28
Iter:9099, L1 loss=0.0004465, Total loss=0.0003815, Time:31
Iter:9199, L1 loss=0.0003206, Total loss=0.0002911, Time:31
Iter:9299, L1 loss=0.0003016, Total loss=0.0002498, Time:28
Iter:9399, L1 loss=0.0004391, Total loss=0.0003671, Time:31
Iter:9499, L1 loss=0.0003331, Total loss=0.0002792, Time:30
Iter:9599, L1 loss=0.0003941, Total loss=0.0003497, Time:30
Iter:9699, L1 loss=0.0004889, Total loss=0.000414, Time:27
Iter:9799, L1 loss=0.0003028, Total loss=0.0002585, Time:31
Iter:9899, L1 loss=0.0004588, Total loss=0.0003546, Time:30
Iter:9999, L1 loss=0.0004926, Total loss=0.0004419, Time:31
Iter:10099, L1 loss=0.0002988, Total loss=0.0002563, Time:30
Iter:10199, L1 loss=0.0003408, Total loss=0.0002862, Time:31
Iter:10299, L1 loss=0.0002803, Total loss=0.0002477, Time:31
Iter:10399, L1 loss=0.0002891, Total loss=0.0002442, Time:31
Iter:10499, L1 loss=0.000288, Total loss=0.0002404, Time:28
Iter:10599, L1 loss=0.0002687, Total loss=0.000233, Time:30
Iter:10699, L1 loss=0.0003083, Total loss=0.0002678, Time:30
Iter:10799, L1 loss=0.0002668, Total loss=0.000231, Time:27
Iter:10899, L1 loss=0.0003279, Total loss=0.0002914, Time:31
Iter:10999, L1 loss=0.0002984, Total loss=0.0002487, Time:30
Iter:11099, L1 loss=0.0002956, Total loss=0.000249, Time:31
Iter:11199, L1 loss=0.0002896, Total loss=0.0002436, Time:32
Iter:11299, L1 loss=0.0002445, Total loss=0.0002143, Time:26
Iter:11399, L1 loss=0.0003176, Total loss=0.0002663, Time:30
Iter:11499, L1 loss=0.000385, Total loss=0.0003163, Time:29
Iter:11599, L1 loss=0.000273, Total loss=0.0002251, Time:30
Iter:11699, L1 loss=0.0002393, Total loss=0.0002088, Time:27
Iter:11799, L1 loss=0.000269, Total loss=0.0002254, Time:26
Iter:11899, L1 loss=0.0002931, Total loss=0.0002349, Time:31
Iter:11999, L1 loss=0.000243, Total loss=0.0002085, Time:30
Iter:12099, L1 loss=0.0005629, Total loss=0.0004567, Time:29
Iter:12199, L1 loss=0.000337, Total loss=0.000299, Time:32
Iter:12299, L1 loss=0.0003609, Total loss=0.0003142, Time:32
Iter:12399, L1 loss=0.0002521, Total loss=0.0002191, Time:31
Iter:12499, L1 loss=0.0003061, Total loss=0.0002393, Time:29
Iter:12599, L1 loss=0.0003578, Total loss=0.0003086, Time:32
Iter:12699, L1 loss=0.0002996, Total loss=0.0002664, Time:33
Iter:12799, L1 loss=0.000275, Total loss=0.0002156, Time:33
Iter:12899, L1 loss=0.0002581, Total loss=0.000206, Time:33
Iter:12999, L1 loss=0.000228, Total loss=0.0002017, Time:33
Iter:13099, L1 loss=0.0002748, Total loss=0.0002217, Time:33
Iter:13199, L1 loss=0.0002671, Total loss=0.0002291, Time:33
Iter:13299, L1 loss=0.000227, Total loss=0.0001865, Time:29
Iter:13399, L1 loss=0.000228, Total loss=0.0001908, Time:33
Iter:13499, L1 loss=0.0002637, Total loss=0.000222, Time:32
Iter:13599, L1 loss=0.0002417, Total loss=0.0002005, Time:31
Iter:13699, L1 loss=0.0002844, Total loss=0.0002404, Time:28
Iter:13799, L1 loss=0.0002688, Total loss=0.0002288, Time:31
Iter:13899, L1 loss=0.000293, Total loss=0.0002526, Time:32
Iter:13999, L1 loss=0.0002232, Total loss=0.0001879, Time:31
Iter:14099, L1 loss=0.000254, Total loss=0.0002058, Time:28
Iter:14199, L1 loss=0.0002246, Total loss=0.000194, Time:31
Iter:14299, L1 loss=0.000231, Total loss=0.0001917, Time:31
Iter:14399, L1 loss=0.0002118, Total loss=0.0001747, Time:28
Iter:14499, L1 loss=0.0002318, Total loss=0.0002032, Time:31
Iter:14599, L1 loss=0.00026, Total loss=0.000217, Time:31
Iter:14699, L1 loss=0.0002375, Total loss=0.0002036, Time:30
Iter:14799, L1 loss=0.0003409, Total loss=0.0002763, Time:27
Iter:14899, L1 loss=0.0002126, Total loss=0.0001797, Time:31
Iter:14999, L1 loss=0.000319, Total loss=0.0002575, Time:31
Iter:15099, L1 loss=0.0001921, Total loss=0.000165, Time:29
Iter:15199, L1 loss=0.0002116, Total loss=0.0001759, Time:29
Iter:15299, L1 loss=0.000229, Total loss=0.0002036, Time:33
Iter:15399, L1 loss=0.0001839, Total loss=0.0001618, Time:32
Iter:15499, L1 loss=0.0002376, Total loss=0.0001883, Time:30
Iter:15599, L1 loss=0.0002028, Total loss=0.0001715, Time:30
Iter:15699, L1 loss=0.0002148, Total loss=0.0001744, Time:31
Iter:15799, L1 loss=0.0001905, Total loss=0.0001572, Time:28
Iter:15899, L1 loss=0.0001778, Total loss=0.000149, Time:26
Iter:15999, L1 loss=0.0002109, Total loss=0.0001738, Time:27
Iter:16099, L1 loss=0.0003108, Total loss=0.0002674, Time:28
Iter:16199, L1 loss=0.0002941, Total loss=0.0002509, Time:28
Iter:16299, L1 loss=0.000316, Total loss=0.0002486, Time:32
Iter:16399, L1 loss=0.0002187, Total loss=0.0001815, Time:31
Iter:16499, L1 loss=0.0002516, Total loss=0.0002389, Time:32
Iter:16599, L1 loss=0.0002315, Total loss=0.0001875, Time:28
Iter:16699, L1 loss=0.0002251, Total loss=0.0001938, Time:31
Iter:16799, L1 loss=0.0002606, Total loss=0.0002125, Time:27
Iter:16899, L1 loss=0.0002623, Total loss=0.0002115, Time:32
Iter:16999, L1 loss=0.0002153, Total loss=0.0001864, Time:28
Iter:17099, L1 loss=0.0002045, Total loss=0.000161, Time:33
Iter:17199, L1 loss=0.0001946, Total loss=0.0001674, Time:31
Iter:17299, L1 loss=0.0002156, Total loss=0.0001791, Time:31
Iter:17399, L1 loss=0.0002059, Total loss=0.0001654, Time:32
Iter:17499, L1 loss=0.0001963, Total loss=0.0001686, Time:31
Iter:17599, L1 loss=0.0002088, Total loss=0.000181, Time:31
Iter:17699, L1 loss=0.0001843, Total loss=0.0001522, Time:28
Iter:17799, L1 loss=0.0003292, Total loss=0.0002682, Time:33
Iter:17899, L1 loss=0.0001841, Total loss=0.0001594, Time:31
Iter:17999, L1 loss=0.000183, Total loss=0.0001482, Time:28
Iter:18099, L1 loss=0.0002902, Total loss=0.0002518, Time:31
Iter:18199, L1 loss=0.0001956, Total loss=0.0001692, Time:32
Iter:18299, L1 loss=0.0002808, Total loss=0.0002251, Time:32
Iter:18399, L1 loss=0.000193, Total loss=0.0001622, Time:29
Iter:18499, L1 loss=0.0002007, Total loss=0.0001688, Time:27
Iter:18599, L1 loss=0.0002509, Total loss=0.0001885, Time:28
Iter:18699, L1 loss=0.0001812, Total loss=0.0001489, Time:31
Iter:18799, L1 loss=0.0001766, Total loss=0.0001397, Time:31
Iter:18899, L1 loss=0.0001907, Total loss=0.0001682, Time:27
Iter:18999, L1 loss=0.0001702, Total loss=0.0001408, Time:31
Iter:19099, L1 loss=0.0001982, Total loss=0.0001628, Time:31
Iter:19199, L1 loss=0.0002636, Total loss=0.0001944, Time:30
Iter:19299, L1 loss=0.0001783, Total loss=0.000148, Time:30
Iter:19399, L1 loss=0.0001872, Total loss=0.0001552, Time:30
Iter:19499, L1 loss=0.0002107, Total loss=0.000179, Time:27
Iter:19599, L1 loss=0.000198, Total loss=0.0001662, Time:28
Iter:19699, L1 loss=0.0001898, Total loss=0.0001645, Time:30
Iter:19799, L1 loss=0.0001923, Total loss=0.0001608, Time:31
Iter:19899, L1 loss=0.0002077, Total loss=0.0001764, Time:30
Iter:19999, L1 loss=0.0001657, Total loss=0.0001416, Time:30
Testing Speed: 136.2222216448372 fps
Testing Time: 0.3670473098754883 s

[ITER 20000] Evaluating test: SSIM = 0.8506614017486572, PSNR = 18.51963394165039
Testing Speed: 149.42123950552778 fps
Testing Time: 0.02007746696472168 s

[ITER 20000] Evaluating train: SSIM = 0.9999996622403462, PSNR = 69.42729949951172
Iter:20000, total_points:199287

[ITER 20000] Saving Gaussians
Optimizing 
Training parameters: {'iterations': 20000, 'position_lr_init': 0.00019, 'position_lr_final': 1.9e-06, 'position_lr_delay_mult': 0.01, 'position_lr_max_steps': 20000, 'feature_lr': 0.002, 'opacity_lr': 0.008, 'scaling_lr': 0.005, 'rotation_lr': 0.001, 'percent_dense': 0.01, 'lambda_dssim': 0.2, 'densification_interval': 200, 'opacity_reset_interval': 4000, 'densify_from_iter': 500, 'densify_until_iter': 8000, 'densify_grad_threshold': 2.6e-05, 'random_background': False, 'sample_pseudo_interval': 1, 'start_sample_pseudo': 0, 'end_sample_pseudo': 30000}
Training parameters: {'iterations': 20000, 'position_lr_init': 0.00019, 'position_lr_final': 1.9e-06, 'position_lr_delay_mult': 0.01, 'position_lr_max_steps': 20000, 'feature_lr': 0.002, 'opacity_lr': 0.008, 'scaling_lr': 0.005, 'rotation_lr': 0.001, 'percent_dense': 0.01, 'lambda_dssim': 0.2, 'densification_interval': 200, 'opacity_reset_interval': 4000, 'densify_from_iter': 500, 'densify_until_iter': 8000, 'densify_grad_threshold': 2.6e-05, 'random_background': False, 'sample_pseudo_interval': 1, 'start_sample_pseudo': 0, 'end_sample_pseudo': 30000}
Create gaussians1
Create gaussians1
GsDict.keys() is dict_keys(['gs0', 'gs1'])
GsDict.keys() is dict_keys(['gs0', 'gs1'])

Starting training...

Starting training...
Total iterations: 20000
Total iterations: 20000
==================================================
==================================================
[Iter 10/20000] Loss: 0.2179127 (Best: 0.2126908 @iter10) [100.00% of initial]
[Iter 10/20000] Loss: 0.2179127 (Best: 0.2126908 @iter10) [100.00% of initial]
[Iter 20/20000] Loss: 0.1746702 (Best: 0.1693031 @iter20) ([92m↓19.84%[0m) [69.39% of initial]
[Iter 20/20000] Loss: 0.1746702 (Best: 0.1693031 @iter20) ([92m↓19.84%[0m) [69.39% of initial]
[Iter 30/20000] Loss: 0.1374906 (Best: 0.1327880 @iter30) ([92m↓21.29%[0m) [54.62% of initial]
[Iter 30/20000] Loss: 0.1374906 (Best: 0.1327880 @iter30) ([92m↓21.29%[0m) [54.62% of initial]
[Iter 40/20000] Loss: 0.1123930 (Best: 0.1098392 @iter40) ([92m↓18.25%[0m) [44.65% of initial]
[Iter 40/20000] Loss: 0.1123930 (Best: 0.1098392 @iter40) ([92m↓18.25%[0m) [44.65% of initial]
[Iter 50/20000] Loss: 0.0993453 (Best: 0.0965435 @iter49) ([92m↓11.61%[0m) [39.47% of initial]
[Iter 50/20000] Loss: 0.0993453 (Best: 0.0965435 @iter49) ([92m↓11.61%[0m) [39.47% of initial]
[Iter 60/20000] Loss: 0.0936766 (Best: 0.0908527 @iter59) ([92m↓5.71%[0m) [37.22% of initial]
[Iter 60/20000] Loss: 0.0936766 (Best: 0.0908527 @iter59) ([92m↓5.71%[0m) [37.22% of initial]
[Iter 70/20000] Loss: 0.0884504 (Best: 0.0869384 @iter70) ([92m↓5.58%[0m) [35.14% of initial]
[Iter 70/20000] Loss: 0.0884504 (Best: 0.0869384 @iter70) ([92m↓5.58%[0m) [35.14% of initial]
[Iter 80/20000] Loss: 0.0851861 (Best: 0.0831026 @iter80) ([92m↓3.69%[0m) [33.84% of initial]
[Iter 80/20000] Loss: 0.0851861 (Best: 0.0831026 @iter80) ([92m↓3.69%[0m) [33.84% of initial]
[Iter 90/20000] Loss: 0.0824127 (Best: 0.0801649 @iter88) ([92m↓3.26%[0m) [32.74% of initial]
[Iter 90/20000] Loss: 0.0824127 (Best: 0.0801649 @iter88) ([92m↓3.26%[0m) [32.74% of initial]
Iter:99, L1 loss=0.05723, Total loss=0.07878, Time:14
Iter:99, L1 loss=0.05723, Total loss=0.07878, Time:14
[Iter 100/20000] Loss: 0.0786700 (Best: 0.0766176 @iter97) ([92m↓4.54%[0m) [31.25% of initial]
[Iter 100/20000] Loss: 0.0786700 (Best: 0.0766176 @iter97) ([92m↓4.54%[0m) [31.25% of initial]
[Iter 110/20000] Loss: 0.0753264 (Best: 0.0731388 @iter106) ([92m↓4.25%[0m) [29.93% of initial]
[Iter 110/20000] Loss: 0.0753264 (Best: 0.0731388 @iter106) ([92m↓4.25%[0m) [29.93% of initial]
[Iter 120/20000] Loss: 0.0714306 (Best: 0.0685537 @iter118) ([92m↓5.17%[0m) [28.38% of initial]
[Iter 120/20000] Loss: 0.0714306 (Best: 0.0685537 @iter118) ([92m↓5.17%[0m) [28.38% of initial]
[Iter 130/20000] Loss: 0.0666951 (Best: 0.0641874 @iter130) ([92m↓6.63%[0m) [26.50% of initial]
[Iter 130/20000] Loss: 0.0666951 (Best: 0.0641874 @iter130) ([92m↓6.63%[0m) [26.50% of initial]
[Iter 140/20000] Loss: 0.0635276 (Best: 0.0612695 @iter140) ([92m↓4.75%[0m) [25.24% of initial]
[Iter 140/20000] Loss: 0.0635276 (Best: 0.0612695 @iter140) ([92m↓4.75%[0m) [25.24% of initial]
[Iter 150/20000] Loss: 0.0612530 (Best: 0.0583437 @iter148) ([92m↓3.58%[0m) [24.34% of initial]
[Iter 150/20000] Loss: 0.0612530 (Best: 0.0583437 @iter148) ([92m↓3.58%[0m) [24.34% of initial]
[Iter 160/20000] Loss: 0.0590401 (Best: 0.0559317 @iter157) ([92m↓3.61%[0m) [23.46% of initial]
[Iter 160/20000] Loss: 0.0590401 (Best: 0.0559317 @iter157) ([92m↓3.61%[0m) [23.46% of initial]
[Iter 170/20000] Loss: 0.0563388 (Best: 0.0534640 @iter167) ([92m↓4.58%[0m) [22.38% of initial]
[Iter 170/20000] Loss: 0.0563388 (Best: 0.0534640 @iter167) ([92m↓4.58%[0m) [22.38% of initial]
[Iter 180/20000] Loss: 0.0523150 (Best: 0.0500024 @iter179) ([92m↓7.14%[0m) [20.78% of initial]
[Iter 180/20000] Loss: 0.0523150 (Best: 0.0500024 @iter179) ([92m↓7.14%[0m) [20.78% of initial]
[Iter 190/20000] Loss: 0.0495360 (Best: 0.0478174 @iter188) ([92m↓5.31%[0m) [19.68% of initial]
[Iter 190/20000] Loss: 0.0495360 (Best: 0.0478174 @iter188) ([92m↓5.31%[0m) [19.68% of initial]
Iter:199, L1 loss=0.0344, Total loss=0.04975, Time:15
Iter:199, L1 loss=0.0344, Total loss=0.04975, Time:15
[Iter 200/20000] Loss: 0.0477899 (Best: 0.0456806 @iter198) ([92m↓3.52%[0m) [18.99% of initial]
[Iter 200/20000] Loss: 0.0477899 (Best: 0.0456806 @iter198) ([92m↓3.52%[0m) [18.99% of initial]
[Iter 210/20000] Loss: 0.0451126 (Best: 0.0429467 @iter209) ([92m↓5.60%[0m) [17.92% of initial]
[Iter 210/20000] Loss: 0.0451126 (Best: 0.0429467 @iter209) ([92m↓5.60%[0m) [17.92% of initial]
[Iter 220/20000] Loss: 0.0440793 (Best: 0.0411946 @iter219) ([92m↓2.29%[0m) [17.51% of initial]
[Iter 220/20000] Loss: 0.0440793 (Best: 0.0411946 @iter219) ([92m↓2.29%[0m) [17.51% of initial]
[Iter 230/20000] Loss: 0.0423805 (Best: 0.0399105 @iter227) ([92m↓3.85%[0m) [16.84% of initial]
[Iter 230/20000] Loss: 0.0423805 (Best: 0.0399105 @iter227) ([92m↓3.85%[0m) [16.84% of initial]
[Iter 240/20000] Loss: 0.0402768 (Best: 0.0377957 @iter238) ([92m↓4.96%[0m) [16.00% of initial]
[Iter 240/20000] Loss: 0.0402768 (Best: 0.0377957 @iter238) ([92m↓4.96%[0m) [16.00% of initial]
[Iter 250/20000] Loss: 0.0379678 (Best: 0.0362037 @iter248) ([92m↓5.73%[0m) [15.08% of initial]
[Iter 250/20000] Loss: 0.0379678 (Best: 0.0362037 @iter248) ([92m↓5.73%[0m) [15.08% of initial]
[Iter 260/20000] Loss: 0.0358894 (Best: 0.0343046 @iter260) ([92m↓5.47%[0m) [14.26% of initial]
[Iter 260/20000] Loss: 0.0358894 (Best: 0.0343046 @iter260) ([92m↓5.47%[0m) [14.26% of initial]
[Iter 270/20000] Loss: 0.0350643 (Best: 0.0329351 @iter269) ([92m↓2.30%[0m) [13.93% of initial]
[Iter 270/20000] Loss: 0.0350643 (Best: 0.0329351 @iter269) ([92m↓2.30%[0m) [13.93% of initial]
[Iter 280/20000] Loss: 0.0346071 (Best: 0.0318444 @iter277) ([92m↓1.30%[0m) [13.75% of initial]
[Iter 280/20000] Loss: 0.0346071 (Best: 0.0318444 @iter277) ([92m↓1.30%[0m) [13.75% of initial]
[Iter 290/20000] Loss: 0.0330132 (Best: 0.0304691 @iter287) ([92m↓4.61%[0m) [13.12% of initial]
[Iter 290/20000] Loss: 0.0330132 (Best: 0.0304691 @iter287) ([92m↓4.61%[0m) [13.12% of initial]
Iter:299, L1 loss=0.02227, Total loss=0.03323, Time:14
Iter:299, L1 loss=0.02227, Total loss=0.03323, Time:14
[Iter 300/20000] Loss: 0.0307210 (Best: 0.0289390 @iter300) ([92m↓6.94%[0m) [12.21% of initial]
[Iter 300/20000] Loss: 0.0307210 (Best: 0.0289390 @iter300) ([92m↓6.94%[0m) [12.21% of initial]
[Iter 310/20000] Loss: 0.0291664 (Best: 0.0271414 @iter310) ([92m↓5.06%[0m) [11.59% of initial]
[Iter 310/20000] Loss: 0.0291664 (Best: 0.0271414 @iter310) ([92m↓5.06%[0m) [11.59% of initial]
[Iter 320/20000] Loss: 0.0278591 (Best: 0.0263752 @iter320) ([92m↓4.48%[0m) [11.07% of initial]
[Iter 320/20000] Loss: 0.0278591 (Best: 0.0263752 @iter320) ([92m↓4.48%[0m) [11.07% of initial]
[Iter 330/20000] Loss: 0.0274840 (Best: 0.0255639 @iter330) ([92m↓1.35%[0m) [10.92% of initial]
[Iter 330/20000] Loss: 0.0274840 (Best: 0.0255639 @iter330) ([92m↓1.35%[0m) [10.92% of initial]
[Iter 340/20000] Loss: 0.0251910 (Best: 0.0240706 @iter340) ([92m↓8.34%[0m) [10.01% of initial]
[Iter 340/20000] Loss: 0.0251910 (Best: 0.0240706 @iter340) ([92m↓8.34%[0m) [10.01% of initial]
[Iter 350/20000] Loss: 0.0257059 (Best: 0.0233537 @iter349) ([91m↑2.04%[0m) [10.21% of initial]
[Iter 350/20000] Loss: 0.0257059 (Best: 0.0233537 @iter349) ([91m↑2.04%[0m) [10.21% of initial]
[Iter 360/20000] Loss: 0.0243030 (Best: 0.0223619 @iter358) ([92m↓5.46%[0m) [9.66% of initial]
[Iter 360/20000] Loss: 0.0243030 (Best: 0.0223619 @iter358) ([92m↓5.46%[0m) [9.66% of initial]
[Iter 370/20000] Loss: 0.0238550 (Best: 0.0216121 @iter368) ([92m↓1.84%[0m) [9.48% of initial]
[Iter 370/20000] Loss: 0.0238550 (Best: 0.0216121 @iter368) ([92m↓1.84%[0m) [9.48% of initial]
[Iter 380/20000] Loss: 0.0217723 (Best: 0.0206253 @iter379) ([92m↓8.73%[0m) [8.65% of initial]
[Iter 380/20000] Loss: 0.0217723 (Best: 0.0206253 @iter379) ([92m↓8.73%[0m) [8.65% of initial]
[Iter 390/20000] Loss: 0.0212732 (Best: 0.0198484 @iter385) ([92m↓2.29%[0m) [8.45% of initial]
[Iter 390/20000] Loss: 0.0212732 (Best: 0.0198484 @iter385) ([92m↓2.29%[0m) [8.45% of initial]
Iter:399, L1 loss=0.01368, Total loss=0.02116, Time:15
Iter:399, L1 loss=0.01368, Total loss=0.02116, Time:15
[Iter 400/20000] Loss: 0.0203142 (Best: 0.0188691 @iter400) ([92m↓4.51%[0m) [8.07% of initial]
[Iter 400/20000] Loss: 0.0203142 (Best: 0.0188691 @iter400) ([92m↓4.51%[0m) [8.07% of initial]
[Iter 410/20000] Loss: 0.0192205 (Best: 0.0182095 @iter410) ([92m↓5.38%[0m) [7.64% of initial]
[Iter 410/20000] Loss: 0.0192205 (Best: 0.0182095 @iter410) ([92m↓5.38%[0m) [7.64% of initial]
[Iter 420/20000] Loss: 0.0200487 (Best: 0.0179178 @iter413) ([91m↑4.31%[0m) [7.97% of initial]
[Iter 420/20000] Loss: 0.0200487 (Best: 0.0179178 @iter413) ([91m↑4.31%[0m) [7.97% of initial]
[Iter 430/20000] Loss: 0.0178783 (Best: 0.0169955 @iter430) ([92m↓10.83%[0m) [7.10% of initial]
[Iter 430/20000] Loss: 0.0178783 (Best: 0.0169955 @iter430) ([92m↓10.83%[0m) [7.10% of initial]
[Iter 440/20000] Loss: 0.0181216 (Best: 0.0163775 @iter438) ([91m↑1.36%[0m) [7.20% of initial]
[Iter 440/20000] Loss: 0.0181216 (Best: 0.0163775 @iter438) ([91m↑1.36%[0m) [7.20% of initial]
[Iter 450/20000] Loss: 0.0171034 (Best: 0.0152643 @iter449) ([92m↓5.62%[0m) [6.80% of initial]
[Iter 450/20000] Loss: 0.0171034 (Best: 0.0152643 @iter449) ([92m↓5.62%[0m) [6.80% of initial]
[Iter 460/20000] Loss: 0.0163657 (Best: 0.0146089 @iter458) ([92m↓4.31%[0m) [6.50% of initial]
[Iter 460/20000] Loss: 0.0163657 (Best: 0.0146089 @iter458) ([92m↓4.31%[0m) [6.50% of initial]
[Iter 470/20000] Loss: 0.0149304 (Best: 0.0138979 @iter470) ([92m↓8.77%[0m) [5.93% of initial]
[Iter 470/20000] Loss: 0.0149304 (Best: 0.0138979 @iter470) ([92m↓8.77%[0m) [5.93% of initial]
[Iter 480/20000] Loss: 0.0155361 (Best: 0.0134709 @iter475) ([91m↑4.06%[0m) [6.17% of initial]
[Iter 480/20000] Loss: 0.0155361 (Best: 0.0134709 @iter475) ([91m↑4.06%[0m) [6.17% of initial]
[Iter 490/20000] Loss: 0.0138112 (Best: 0.0125381 @iter490) ([92m↓11.10%[0m) [5.49% of initial]
[Iter 490/20000] Loss: 0.0138112 (Best: 0.0125381 @iter490) ([92m↓11.10%[0m) [5.49% of initial]
Iter:499, L1 loss=0.008315, Total loss=0.01456, Time:13
Iter:499, L1 loss=0.008315, Total loss=0.01456, Time:13
[Iter 500/20000] Loss: 0.0138060 (Best: 0.0124244 @iter493) ([92m↓0.04%[0m) [5.48% of initial]
[Iter 500/20000] Loss: 0.0138060 (Best: 0.0124244 @iter493) ([92m↓0.04%[0m) [5.48% of initial]
[Iter 510/20000] Loss: 0.0134649 (Best: 0.0120804 @iter507) ([92m↓2.47%[0m) [5.35% of initial]
[Iter 510/20000] Loss: 0.0134649 (Best: 0.0120804 @iter507) ([92m↓2.47%[0m) [5.35% of initial]
[Iter 520/20000] Loss: 0.0128722 (Best: 0.0115923 @iter514) ([92m↓4.40%[0m) [5.11% of initial]
[Iter 520/20000] Loss: 0.0128722 (Best: 0.0115923 @iter514) ([92m↓4.40%[0m) [5.11% of initial]
[Iter 530/20000] Loss: 0.0129946 (Best: 0.0115923 @iter514) ([91m↑0.95%[0m) [5.16% of initial]
[Iter 530/20000] Loss: 0.0129946 (Best: 0.0115923 @iter514) ([91m↑0.95%[0m) [5.16% of initial]
[Iter 540/20000] Loss: 0.0126180 (Best: 0.0112063 @iter538) ([92m↓2.90%[0m) [5.01% of initial]
[Iter 540/20000] Loss: 0.0126180 (Best: 0.0112063 @iter538) ([92m↓2.90%[0m) [5.01% of initial]
[Iter 550/20000] Loss: 0.0120649 (Best: 0.0108897 @iter548) ([92m↓4.38%[0m) [4.79% of initial]
[Iter 550/20000] Loss: 0.0120649 (Best: 0.0108897 @iter548) ([92m↓4.38%[0m) [4.79% of initial]
[Iter 560/20000] Loss: 0.0127218 (Best: 0.0108897 @iter548) ([91m↑5.44%[0m) [5.05% of initial]
[Iter 560/20000] Loss: 0.0127218 (Best: 0.0108897 @iter548) ([91m↑5.44%[0m) [5.05% of initial]
[Iter 570/20000] Loss: 0.0119315 (Best: 0.0105568 @iter569) ([92m↓6.21%[0m) [4.74% of initial]
[Iter 570/20000] Loss: 0.0119315 (Best: 0.0105568 @iter569) ([92m↓6.21%[0m) [4.74% of initial]
[Iter 580/20000] Loss: 0.0117174 (Best: 0.0103750 @iter572) ([92m↓1.79%[0m) [4.66% of initial]
[Iter 580/20000] Loss: 0.0117174 (Best: 0.0103750 @iter572) ([92m↓1.79%[0m) [4.66% of initial]
[Iter 590/20000] Loss: 0.0116257 (Best: 0.0103601 @iter583) ([92m↓0.78%[0m) [4.62% of initial]
[Iter 590/20000] Loss: 0.0116257 (Best: 0.0103601 @iter583) ([92m↓0.78%[0m) [4.62% of initial]
Iter:599, L1 loss=0.006954, Total loss=0.01233, Time:11
Iter:599, L1 loss=0.006954, Total loss=0.01233, Time:11
[Iter 600/20000] Loss: 0.0113687 (Best: 0.0101016 @iter598) ([92m↓2.21%[0m) [4.52% of initial]
[Iter 600/20000] Loss: 0.0113687 (Best: 0.0101016 @iter598) ([92m↓2.21%[0m) [4.52% of initial]
[Iter 610/20000] Loss: 0.0218150 (Best: 0.0101016 @iter598) ([91m↑91.89%[0m) [8.67% of initial]
[Iter 610/20000] Loss: 0.0218150 (Best: 0.0101016 @iter598) ([91m↑91.89%[0m) [8.67% of initial]
[Iter 620/20000] Loss: 0.0147221 (Best: 0.0101016 @iter598) ([92m↓32.51%[0m) [5.85% of initial]
[Iter 620/20000] Loss: 0.0147221 (Best: 0.0101016 @iter598) ([92m↓32.51%[0m) [5.85% of initial]
[Iter 630/20000] Loss: 0.0124683 (Best: 0.0101016 @iter598) ([92m↓15.31%[0m) [4.95% of initial]
[Iter 630/20000] Loss: 0.0124683 (Best: 0.0101016 @iter598) ([92m↓15.31%[0m) [4.95% of initial]
[Iter 640/20000] Loss: 0.0106826 (Best: 0.0097433 @iter640) ([92m↓14.32%[0m) [4.24% of initial]
[Iter 640/20000] Loss: 0.0106826 (Best: 0.0097433 @iter640) ([92m↓14.32%[0m) [4.24% of initial]
[Iter 650/20000] Loss: 0.0108664 (Best: 0.0094847 @iter646) ([91m↑1.72%[0m) [4.32% of initial]
[Iter 650/20000] Loss: 0.0108664 (Best: 0.0094847 @iter646) ([91m↑1.72%[0m) [4.32% of initial]
[Iter 660/20000] Loss: 0.0104396 (Best: 0.0090039 @iter655) ([92m↓3.93%[0m) [4.15% of initial]
[Iter 660/20000] Loss: 0.0104396 (Best: 0.0090039 @iter655) ([92m↓3.93%[0m) [4.15% of initial]
[Iter 670/20000] Loss: 0.0096440 (Best: 0.0086746 @iter667) ([92m↓7.62%[0m) [3.83% of initial]
[Iter 670/20000] Loss: 0.0096440 (Best: 0.0086746 @iter667) ([92m↓7.62%[0m) [3.83% of initial]
[Iter 680/20000] Loss: 0.0092896 (Best: 0.0085333 @iter680) ([92m↓3.67%[0m) [3.69% of initial]
[Iter 680/20000] Loss: 0.0092896 (Best: 0.0085333 @iter680) ([92m↓3.67%[0m) [3.69% of initial]
[Iter 690/20000] Loss: 0.0093479 (Best: 0.0080952 @iter682) ([91m↑0.63%[0m) [3.71% of initial]
[Iter 690/20000] Loss: 0.0093479 (Best: 0.0080952 @iter682) ([91m↑0.63%[0m) [3.71% of initial]
Iter:699, L1 loss=0.005645, Total loss=0.009931, Time:13
Iter:699, L1 loss=0.005645, Total loss=0.009931, Time:13
[Iter 700/20000] Loss: 0.0090446 (Best: 0.0080371 @iter695) ([92m↓3.25%[0m) [3.59% of initial]
[Iter 700/20000] Loss: 0.0090446 (Best: 0.0080371 @iter695) ([92m↓3.25%[0m) [3.59% of initial]
[Iter 710/20000] Loss: 0.0084209 (Best: 0.0078206 @iter710) ([92m↓6.90%[0m) [3.35% of initial]
[Iter 710/20000] Loss: 0.0084209 (Best: 0.0078206 @iter710) ([92m↓6.90%[0m) [3.35% of initial]
[Iter 720/20000] Loss: 0.0084288 (Best: 0.0076561 @iter715) ([91m↑0.09%[0m) [3.35% of initial]
[Iter 720/20000] Loss: 0.0084288 (Best: 0.0076561 @iter715) ([91m↑0.09%[0m) [3.35% of initial]
[Iter 730/20000] Loss: 0.0085844 (Best: 0.0072767 @iter727) ([91m↑1.85%[0m) [3.41% of initial]
[Iter 730/20000] Loss: 0.0085844 (Best: 0.0072767 @iter727) ([91m↑1.85%[0m) [3.41% of initial]
[Iter 740/20000] Loss: 0.0086158 (Best: 0.0072767 @iter727) ([91m↑0.37%[0m) [3.42% of initial]
[Iter 740/20000] Loss: 0.0086158 (Best: 0.0072767 @iter727) ([91m↑0.37%[0m) [3.42% of initial]
[Iter 750/20000] Loss: 0.0082090 (Best: 0.0070241 @iter748) ([92m↓4.72%[0m) [3.26% of initial]
[Iter 750/20000] Loss: 0.0082090 (Best: 0.0070241 @iter748) ([92m↓4.72%[0m) [3.26% of initial]
[Iter 760/20000] Loss: 0.0074935 (Best: 0.0070044 @iter754) ([92m↓8.72%[0m) [2.98% of initial]
[Iter 760/20000] Loss: 0.0074935 (Best: 0.0070044 @iter754) ([92m↓8.72%[0m) [2.98% of initial]
[Iter 770/20000] Loss: 0.0076322 (Best: 0.0069560 @iter764) ([91m↑1.85%[0m) [3.03% of initial]
[Iter 770/20000] Loss: 0.0076322 (Best: 0.0069560 @iter764) ([91m↑1.85%[0m) [3.03% of initial]
[Iter 780/20000] Loss: 0.0078663 (Best: 0.0067595 @iter778) ([91m↑3.07%[0m) [3.13% of initial]
[Iter 780/20000] Loss: 0.0078663 (Best: 0.0067595 @iter778) ([91m↑3.07%[0m) [3.13% of initial]
[Iter 790/20000] Loss: 0.0075725 (Best: 0.0066825 @iter787) ([92m↓3.74%[0m) [3.01% of initial]
[Iter 790/20000] Loss: 0.0075725 (Best: 0.0066825 @iter787) ([92m↓3.74%[0m) [3.01% of initial]
Iter:799, L1 loss=0.005003, Total loss=0.008577, Time:13
Iter:799, L1 loss=0.005003, Total loss=0.008577, Time:13
[Iter 800/20000] Loss: 0.0077045 (Best: 0.0066825 @iter787) ([91m↑1.74%[0m) [3.06% of initial]
[Iter 800/20000] Loss: 0.0077045 (Best: 0.0066825 @iter787) ([91m↑1.74%[0m) [3.06% of initial]
[Iter 810/20000] Loss: 0.0157766 (Best: 0.0066825 @iter787) ([91m↑104.77%[0m) [6.27% of initial]
[Iter 810/20000] Loss: 0.0157766 (Best: 0.0066825 @iter787) ([91m↑104.77%[0m) [6.27% of initial]
[Iter 820/20000] Loss: 0.0106748 (Best: 0.0066825 @iter787) ([92m↓32.34%[0m) [4.24% of initial]
[Iter 820/20000] Loss: 0.0106748 (Best: 0.0066825 @iter787) ([92m↓32.34%[0m) [4.24% of initial]
[Iter 830/20000] Loss: 0.0089570 (Best: 0.0066825 @iter787) ([92m↓16.09%[0m) [3.56% of initial]
[Iter 830/20000] Loss: 0.0089570 (Best: 0.0066825 @iter787) ([92m↓16.09%[0m) [3.56% of initial]
[Iter 840/20000] Loss: 0.0080939 (Best: 0.0066825 @iter787) ([92m↓9.64%[0m) [3.22% of initial]
[Iter 840/20000] Loss: 0.0080939 (Best: 0.0066825 @iter787) ([92m↓9.64%[0m) [3.22% of initial]
[Iter 850/20000] Loss: 0.0075539 (Best: 0.0066441 @iter847) ([92m↓6.67%[0m) [3.00% of initial]
[Iter 850/20000] Loss: 0.0075539 (Best: 0.0066441 @iter847) ([92m↓6.67%[0m) [3.00% of initial]
[Iter 860/20000] Loss: 0.0070855 (Best: 0.0061816 @iter856) ([92m↓6.20%[0m) [2.81% of initial]
[Iter 860/20000] Loss: 0.0070855 (Best: 0.0061816 @iter856) ([92m↓6.20%[0m) [2.81% of initial]
[Iter 870/20000] Loss: 0.0067098 (Best: 0.0060721 @iter862) ([92m↓5.30%[0m) [2.67% of initial]
[Iter 870/20000] Loss: 0.0067098 (Best: 0.0060721 @iter862) ([92m↓5.30%[0m) [2.67% of initial]
[Iter 880/20000] Loss: 0.0067138 (Best: 0.0060175 @iter872) ([91m↑0.06%[0m) [2.67% of initial]
[Iter 880/20000] Loss: 0.0067138 (Best: 0.0060175 @iter872) ([91m↑0.06%[0m) [2.67% of initial]
[Iter 890/20000] Loss: 0.0063600 (Best: 0.0057006 @iter887) ([92m↓5.27%[0m) [2.53% of initial]
[Iter 890/20000] Loss: 0.0063600 (Best: 0.0057006 @iter887) ([92m↓5.27%[0m) [2.53% of initial]
Iter:899, L1 loss=0.003705, Total loss=0.005563, Time:14
Iter:899, L1 loss=0.003705, Total loss=0.005563, Time:14
[Iter 900/20000] Loss: 0.0064237 (Best: 0.0055635 @iter899) ([91m↑1.00%[0m) [2.55% of initial]
[Iter 900/20000] Loss: 0.0064237 (Best: 0.0055635 @iter899) ([91m↑1.00%[0m) [2.55% of initial]
[Iter 910/20000] Loss: 0.0065461 (Best: 0.0054166 @iter907) ([91m↑1.91%[0m) [2.60% of initial]
[Iter 910/20000] Loss: 0.0065461 (Best: 0.0054166 @iter907) ([91m↑1.91%[0m) [2.60% of initial]
[Iter 920/20000] Loss: 0.0058887 (Best: 0.0052917 @iter916) ([92m↓10.04%[0m) [2.34% of initial]
[Iter 920/20000] Loss: 0.0058887 (Best: 0.0052917 @iter916) ([92m↓10.04%[0m) [2.34% of initial]
[Iter 930/20000] Loss: 0.0061841 (Best: 0.0052397 @iter928) ([91m↑5.02%[0m) [2.46% of initial]
[Iter 930/20000] Loss: 0.0061841 (Best: 0.0052397 @iter928) ([91m↑5.02%[0m) [2.46% of initial]
[Iter 940/20000] Loss: 0.0062588 (Best: 0.0050953 @iter938) ([91m↑1.21%[0m) [2.49% of initial]
[Iter 940/20000] Loss: 0.0062588 (Best: 0.0050953 @iter938) ([91m↑1.21%[0m) [2.49% of initial]
[Iter 950/20000] Loss: 0.0057767 (Best: 0.0050953 @iter938) ([92m↓7.70%[0m) [2.30% of initial]
[Iter 950/20000] Loss: 0.0057767 (Best: 0.0050953 @iter938) ([92m↓7.70%[0m) [2.30% of initial]
[Iter 960/20000] Loss: 0.0059417 (Best: 0.0050953 @iter938) ([91m↑2.86%[0m) [2.36% of initial]
[Iter 960/20000] Loss: 0.0059417 (Best: 0.0050953 @iter938) ([91m↑2.86%[0m) [2.36% of initial]
[Iter 970/20000] Loss: 0.0058643 (Best: 0.0049995 @iter964) ([92m↓1.30%[0m) [2.33% of initial]
[Iter 970/20000] Loss: 0.0058643 (Best: 0.0049995 @iter964) ([92m↓1.30%[0m) [2.33% of initial]
[Iter 980/20000] Loss: 0.0059217 (Best: 0.0049995 @iter964) ([91m↑0.98%[0m) [2.35% of initial]
[Iter 980/20000] Loss: 0.0059217 (Best: 0.0049995 @iter964) ([91m↑0.98%[0m) [2.35% of initial]
[Iter 990/20000] Loss: 0.0059092 (Best: 0.0049785 @iter988) ([92m↓0.21%[0m) [2.35% of initial]
[Iter 990/20000] Loss: 0.0059092 (Best: 0.0049785 @iter988) ([92m↓0.21%[0m) [2.35% of initial]
Iter:999, L1 loss=0.004417, Total loss=0.006591, Time:13
Iter:999, L1 loss=0.004417, Total loss=0.006591, Time:13
[Iter 1000/20000] Loss: 0.0061891 (Best: 0.0049785 @iter988) ([91m↑4.74%[0m) [2.46% of initial]
[Iter 1000/20000] Loss: 0.0061891 (Best: 0.0049785 @iter988) ([91m↑4.74%[0m) [2.46% of initial]
[Iter 1010/20000] Loss: 0.0119219 (Best: 0.0049785 @iter988) ([91m↑92.63%[0m) [4.74% of initial]
[Iter 1010/20000] Loss: 0.0119219 (Best: 0.0049785 @iter988) ([91m↑92.63%[0m) [4.74% of initial]
[Iter 1020/20000] Loss: 0.0084147 (Best: 0.0049785 @iter988) ([92m↓29.42%[0m) [3.34% of initial]
[Iter 1020/20000] Loss: 0.0084147 (Best: 0.0049785 @iter988) ([92m↓29.42%[0m) [3.34% of initial]
[Iter 1030/20000] Loss: 0.0067732 (Best: 0.0049785 @iter988) ([92m↓19.51%[0m) [2.69% of initial]
[Iter 1030/20000] Loss: 0.0067732 (Best: 0.0049785 @iter988) ([92m↓19.51%[0m) [2.69% of initial]
[Iter 1040/20000] Loss: 0.0059919 (Best: 0.0049785 @iter988) ([92m↓11.53%[0m) [2.38% of initial]
[Iter 1040/20000] Loss: 0.0059919 (Best: 0.0049785 @iter988) ([92m↓11.53%[0m) [2.38% of initial]
[Iter 1050/20000] Loss: 0.0058304 (Best: 0.0049785 @iter988) ([92m↓2.70%[0m) [2.32% of initial]
[Iter 1050/20000] Loss: 0.0058304 (Best: 0.0049785 @iter988) ([92m↓2.70%[0m) [2.32% of initial]
[Iter 1060/20000] Loss: 0.0057052 (Best: 0.0048423 @iter1051) ([92m↓2.15%[0m) [2.27% of initial]
[Iter 1060/20000] Loss: 0.0057052 (Best: 0.0048423 @iter1051) ([92m↓2.15%[0m) [2.27% of initial]
[Iter 1070/20000] Loss: 0.0054270 (Best: 0.0044673 @iter1066) ([92m↓4.88%[0m) [2.16% of initial]
[Iter 1070/20000] Loss: 0.0054270 (Best: 0.0044673 @iter1066) ([92m↓4.88%[0m) [2.16% of initial]
[Iter 1080/20000] Loss: 0.0052490 (Best: 0.0044624 @iter1075) ([92m↓3.28%[0m) [2.09% of initial]
[Iter 1080/20000] Loss: 0.0052490 (Best: 0.0044624 @iter1075) ([92m↓3.28%[0m) [2.09% of initial]
[Iter 1090/20000] Loss: 0.0050781 (Best: 0.0044526 @iter1082) ([92m↓3.26%[0m) [2.02% of initial]
[Iter 1090/20000] Loss: 0.0050781 (Best: 0.0044526 @iter1082) ([92m↓3.26%[0m) [2.02% of initial]
Iter:1099, L1 loss=0.003366, Total loss=0.005153, Time:13
Iter:1099, L1 loss=0.003366, Total loss=0.005153, Time:13
[Iter 1100/20000] Loss: 0.0050017 (Best: 0.0042022 @iter1093) ([92m↓1.51%[0m) [1.99% of initial]
[Iter 1100/20000] Loss: 0.0050017 (Best: 0.0042022 @iter1093) ([92m↓1.51%[0m) [1.99% of initial]
[Iter 1110/20000] Loss: 0.0050290 (Best: 0.0042022 @iter1093) ([91m↑0.55%[0m) [2.00% of initial]
[Iter 1110/20000] Loss: 0.0050290 (Best: 0.0042022 @iter1093) ([91m↑0.55%[0m) [2.00% of initial]
[Iter 1120/20000] Loss: 0.0049670 (Best: 0.0041100 @iter1117) ([92m↓1.23%[0m) [1.97% of initial]
[Iter 1120/20000] Loss: 0.0049670 (Best: 0.0041100 @iter1117) ([92m↓1.23%[0m) [1.97% of initial]
[Iter 1130/20000] Loss: 0.0052191 (Best: 0.0041100 @iter1117) ([91m↑5.07%[0m) [2.07% of initial]
[Iter 1130/20000] Loss: 0.0052191 (Best: 0.0041100 @iter1117) ([91m↑5.07%[0m) [2.07% of initial]
[Iter 1140/20000] Loss: 0.0048011 (Best: 0.0040585 @iter1135) ([92m↓8.01%[0m) [1.91% of initial]
[Iter 1140/20000] Loss: 0.0048011 (Best: 0.0040585 @iter1135) ([92m↓8.01%[0m) [1.91% of initial]
[Iter 1150/20000] Loss: 0.0044463 (Best: 0.0039652 @iter1145) ([92m↓7.39%[0m) [1.77% of initial]
[Iter 1150/20000] Loss: 0.0044463 (Best: 0.0039652 @iter1145) ([92m↓7.39%[0m) [1.77% of initial]
[Iter 1160/20000] Loss: 0.0049878 (Best: 0.0039652 @iter1145) ([91m↑12.18%[0m) [1.98% of initial]
[Iter 1160/20000] Loss: 0.0049878 (Best: 0.0039652 @iter1145) ([91m↑12.18%[0m) [1.98% of initial]
[Iter 1170/20000] Loss: 0.0046104 (Best: 0.0039652 @iter1145) ([92m↓7.57%[0m) [1.83% of initial]
[Iter 1170/20000] Loss: 0.0046104 (Best: 0.0039652 @iter1145) ([92m↓7.57%[0m) [1.83% of initial]
[Iter 1180/20000] Loss: 0.0042875 (Best: 0.0039009 @iter1180) ([92m↓7.01%[0m) [1.70% of initial]
[Iter 1180/20000] Loss: 0.0042875 (Best: 0.0039009 @iter1180) ([92m↓7.01%[0m) [1.70% of initial]
[Iter 1190/20000] Loss: 0.0045635 (Best: 0.0038843 @iter1186) ([91m↑6.44%[0m) [1.81% of initial]
[Iter 1190/20000] Loss: 0.0045635 (Best: 0.0038843 @iter1186) ([91m↑6.44%[0m) [1.81% of initial]
Iter:1199, L1 loss=0.003426, Total loss=0.004866, Time:10
Iter:1199, L1 loss=0.003426, Total loss=0.004866, Time:10
[Iter 1200/20000] Loss: 0.0045477 (Best: 0.0037491 @iter1195) ([92m↓0.35%[0m) [1.81% of initial]
[Iter 1200/20000] Loss: 0.0045477 (Best: 0.0037491 @iter1195) ([92m↓0.35%[0m) [1.81% of initial]
[Iter 1210/20000] Loss: 0.0109541 (Best: 0.0037491 @iter1195) ([91m↑140.87%[0m) [4.35% of initial]
[Iter 1210/20000] Loss: 0.0109541 (Best: 0.0037491 @iter1195) ([91m↑140.87%[0m) [4.35% of initial]
[Iter 1220/20000] Loss: 0.0070703 (Best: 0.0037491 @iter1195) ([92m↓35.46%[0m) [2.81% of initial]
[Iter 1220/20000] Loss: 0.0070703 (Best: 0.0037491 @iter1195) ([92m↓35.46%[0m) [2.81% of initial]
[Iter 1230/20000] Loss: 0.0059922 (Best: 0.0037491 @iter1195) ([92m↓15.25%[0m) [2.38% of initial]
[Iter 1230/20000] Loss: 0.0059922 (Best: 0.0037491 @iter1195) ([92m↓15.25%[0m) [2.38% of initial]
[Iter 1240/20000] Loss: 0.0054752 (Best: 0.0037491 @iter1195) ([92m↓8.63%[0m) [2.18% of initial]
[Iter 1240/20000] Loss: 0.0054752 (Best: 0.0037491 @iter1195) ([92m↓8.63%[0m) [2.18% of initial]
[Iter 1250/20000] Loss: 0.0048716 (Best: 0.0037491 @iter1195) ([92m↓11.02%[0m) [1.94% of initial]
[Iter 1250/20000] Loss: 0.0048716 (Best: 0.0037491 @iter1195) ([92m↓11.02%[0m) [1.94% of initial]
[Iter 1260/20000] Loss: 0.0045982 (Best: 0.0037491 @iter1195) ([92m↓5.61%[0m) [1.83% of initial]
[Iter 1260/20000] Loss: 0.0045982 (Best: 0.0037491 @iter1195) ([92m↓5.61%[0m) [1.83% of initial]
[Iter 1270/20000] Loss: 0.0041154 (Best: 0.0037491 @iter1195) ([92m↓10.50%[0m) [1.64% of initial]
[Iter 1270/20000] Loss: 0.0041154 (Best: 0.0037491 @iter1195) ([92m↓10.50%[0m) [1.64% of initial]
[Iter 1280/20000] Loss: 0.0043320 (Best: 0.0034406 @iter1273) ([91m↑5.26%[0m) [1.72% of initial]
[Iter 1280/20000] Loss: 0.0043320 (Best: 0.0034406 @iter1273) ([91m↑5.26%[0m) [1.72% of initial]
[Iter 1290/20000] Loss: 0.0043017 (Best: 0.0034406 @iter1273) ([92m↓0.70%[0m) [1.71% of initial]
[Iter 1290/20000] Loss: 0.0043017 (Best: 0.0034406 @iter1273) ([92m↓0.70%[0m) [1.71% of initial]
Iter:1299, L1 loss=0.002798, Total loss=0.00382, Time:13
Iter:1299, L1 loss=0.002798, Total loss=0.00382, Time:13
[Iter 1300/20000] Loss: 0.0040387 (Best: 0.0034406 @iter1273) ([92m↓6.11%[0m) [1.60% of initial]
[Iter 1300/20000] Loss: 0.0040387 (Best: 0.0034406 @iter1273) ([92m↓6.11%[0m) [1.60% of initial]
[Iter 1310/20000] Loss: 0.0040355 (Best: 0.0034337 @iter1301) ([92m↓0.08%[0m) [1.60% of initial]
[Iter 1310/20000] Loss: 0.0040355 (Best: 0.0034337 @iter1301) ([92m↓0.08%[0m) [1.60% of initial]
[Iter 1320/20000] Loss: 0.0039112 (Best: 0.0031813 @iter1319) ([92m↓3.08%[0m) [1.55% of initial]
[Iter 1320/20000] Loss: 0.0039112 (Best: 0.0031813 @iter1319) ([92m↓3.08%[0m) [1.55% of initial]
[Iter 1330/20000] Loss: 0.0039328 (Best: 0.0031390 @iter1321) ([91m↑0.55%[0m) [1.56% of initial]
[Iter 1330/20000] Loss: 0.0039328 (Best: 0.0031390 @iter1321) ([91m↑0.55%[0m) [1.56% of initial]
[Iter 1340/20000] Loss: 0.0037215 (Best: 0.0031390 @iter1321) ([92m↓5.37%[0m) [1.48% of initial]
[Iter 1340/20000] Loss: 0.0037215 (Best: 0.0031390 @iter1321) ([92m↓5.37%[0m) [1.48% of initial]
[Iter 1350/20000] Loss: 0.0037146 (Best: 0.0031390 @iter1321) ([92m↓0.19%[0m) [1.48% of initial]
[Iter 1350/20000] Loss: 0.0037146 (Best: 0.0031390 @iter1321) ([92m↓0.19%[0m) [1.48% of initial]
[Iter 1360/20000] Loss: 0.0038512 (Best: 0.0031390 @iter1321) ([91m↑3.68%[0m) [1.53% of initial]
[Iter 1360/20000] Loss: 0.0038512 (Best: 0.0031390 @iter1321) ([91m↑3.68%[0m) [1.53% of initial]
[Iter 1370/20000] Loss: 0.0036475 (Best: 0.0031390 @iter1321) ([92m↓5.29%[0m) [1.45% of initial]
[Iter 1370/20000] Loss: 0.0036475 (Best: 0.0031390 @iter1321) ([92m↓5.29%[0m) [1.45% of initial]
[Iter 1380/20000] Loss: 0.0039128 (Best: 0.0031082 @iter1375) ([91m↑7.27%[0m) [1.55% of initial]
[Iter 1380/20000] Loss: 0.0039128 (Best: 0.0031082 @iter1375) ([91m↑7.27%[0m) [1.55% of initial]
[Iter 1390/20000] Loss: 0.0037640 (Best: 0.0031082 @iter1375) ([92m↓3.80%[0m) [1.50% of initial]
[Iter 1390/20000] Loss: 0.0037640 (Best: 0.0031082 @iter1375) ([92m↓3.80%[0m) [1.50% of initial]
Iter:1399, L1 loss=0.002275, Total loss=0.003007, Time:11
Iter:1399, L1 loss=0.002275, Total loss=0.003007, Time:11
[Iter 1400/20000] Loss: 0.0034410 (Best: 0.0030020 @iter1394) ([92m↓8.58%[0m) [1.37% of initial]
[Iter 1400/20000] Loss: 0.0034410 (Best: 0.0030020 @iter1394) ([92m↓8.58%[0m) [1.37% of initial]
[Iter 1410/20000] Loss: 0.0088258 (Best: 0.0030020 @iter1394) ([91m↑156.49%[0m) [3.51% of initial]
[Iter 1410/20000] Loss: 0.0088258 (Best: 0.0030020 @iter1394) ([91m↑156.49%[0m) [3.51% of initial]
[Iter 1420/20000] Loss: 0.0057154 (Best: 0.0030020 @iter1394) ([92m↓35.24%[0m) [2.27% of initial]
[Iter 1420/20000] Loss: 0.0057154 (Best: 0.0030020 @iter1394) ([92m↓35.24%[0m) [2.27% of initial]
[Iter 1430/20000] Loss: 0.0048303 (Best: 0.0030020 @iter1394) ([92m↓15.49%[0m) [1.92% of initial]
[Iter 1430/20000] Loss: 0.0048303 (Best: 0.0030020 @iter1394) ([92m↓15.49%[0m) [1.92% of initial]
[Iter 1440/20000] Loss: 0.0043855 (Best: 0.0030020 @iter1394) ([92m↓9.21%[0m) [1.74% of initial]
[Iter 1440/20000] Loss: 0.0043855 (Best: 0.0030020 @iter1394) ([92m↓9.21%[0m) [1.74% of initial]
[Iter 1450/20000] Loss: 0.0035099 (Best: 0.0030020 @iter1394) ([92m↓19.96%[0m) [1.39% of initial]
[Iter 1450/20000] Loss: 0.0035099 (Best: 0.0030020 @iter1394) ([92m↓19.96%[0m) [1.39% of initial]
[Iter 1460/20000] Loss: 0.0035351 (Best: 0.0029076 @iter1459) ([91m↑0.72%[0m) [1.40% of initial]
[Iter 1460/20000] Loss: 0.0035351 (Best: 0.0029076 @iter1459) ([91m↑0.72%[0m) [1.40% of initial]
[Iter 1470/20000] Loss: 0.0034121 (Best: 0.0029076 @iter1459) ([92m↓3.48%[0m) [1.36% of initial]
[Iter 1470/20000] Loss: 0.0034121 (Best: 0.0029076 @iter1459) ([92m↓3.48%[0m) [1.36% of initial]
[Iter 1480/20000] Loss: 0.0032993 (Best: 0.0027681 @iter1480) ([92m↓3.31%[0m) [1.31% of initial]
[Iter 1480/20000] Loss: 0.0032993 (Best: 0.0027681 @iter1480) ([92m↓3.31%[0m) [1.31% of initial]
[Iter 1490/20000] Loss: 0.0032108 (Best: 0.0027681 @iter1480) ([92m↓2.68%[0m) [1.28% of initial]
[Iter 1490/20000] Loss: 0.0032108 (Best: 0.0027681 @iter1480) ([92m↓2.68%[0m) [1.28% of initial]
Iter:1499, L1 loss=0.002628, Total loss=0.003301, Time:13
Iter:1499, L1 loss=0.002628, Total loss=0.003301, Time:13
[Iter 1500/20000] Loss: 0.0031942 (Best: 0.0027681 @iter1480) ([92m↓0.52%[0m) [1.27% of initial]
[Iter 1500/20000] Loss: 0.0031942 (Best: 0.0027681 @iter1480) ([92m↓0.52%[0m) [1.27% of initial]
[Iter 1510/20000] Loss: 0.0030330 (Best: 0.0025855 @iter1504) ([92m↓5.05%[0m) [1.20% of initial]
[Iter 1510/20000] Loss: 0.0030330 (Best: 0.0025855 @iter1504) ([92m↓5.05%[0m) [1.20% of initial]
[Iter 1520/20000] Loss: 0.0030070 (Best: 0.0025770 @iter1520) ([92m↓0.86%[0m) [1.19% of initial]
[Iter 1520/20000] Loss: 0.0030070 (Best: 0.0025770 @iter1520) ([92m↓0.86%[0m) [1.19% of initial]
[Iter 1530/20000] Loss: 0.0031093 (Best: 0.0025667 @iter1526) ([91m↑3.40%[0m) [1.24% of initial]
[Iter 1530/20000] Loss: 0.0031093 (Best: 0.0025667 @iter1526) ([91m↑3.40%[0m) [1.24% of initial]
[Iter 1540/20000] Loss: 0.0030313 (Best: 0.0025667 @iter1526) ([92m↓2.51%[0m) [1.20% of initial]
[Iter 1540/20000] Loss: 0.0030313 (Best: 0.0025667 @iter1526) ([92m↓2.51%[0m) [1.20% of initial]
[Iter 1550/20000] Loss: 0.0030381 (Best: 0.0025667 @iter1526) ([91m↑0.22%[0m) [1.21% of initial]
[Iter 1550/20000] Loss: 0.0030381 (Best: 0.0025667 @iter1526) ([91m↑0.22%[0m) [1.21% of initial]
[Iter 1560/20000] Loss: 0.0032427 (Best: 0.0025116 @iter1558) ([91m↑6.73%[0m) [1.29% of initial]
[Iter 1560/20000] Loss: 0.0032427 (Best: 0.0025116 @iter1558) ([91m↑6.73%[0m) [1.29% of initial]
[Iter 1570/20000] Loss: 0.0028689 (Best: 0.0025116 @iter1558) ([92m↓11.53%[0m) [1.14% of initial]
[Iter 1570/20000] Loss: 0.0028689 (Best: 0.0025116 @iter1558) ([92m↓11.53%[0m) [1.14% of initial]
[Iter 1580/20000] Loss: 0.0028212 (Best: 0.0023468 @iter1573) ([92m↓1.67%[0m) [1.12% of initial]
[Iter 1580/20000] Loss: 0.0028212 (Best: 0.0023468 @iter1573) ([92m↓1.67%[0m) [1.12% of initial]
[Iter 1590/20000] Loss: 0.0027021 (Best: 0.0023468 @iter1573) ([92m↓4.22%[0m) [1.07% of initial]
[Iter 1590/20000] Loss: 0.0027021 (Best: 0.0023468 @iter1573) ([92m↓4.22%[0m) [1.07% of initial]
Iter:1599, L1 loss=0.00271, Total loss=0.003367, Time:13
Iter:1599, L1 loss=0.00271, Total loss=0.003367, Time:13
[Iter 1600/20000] Loss: 0.0030720 (Best: 0.0022941 @iter1591) ([91m↑13.69%[0m) [1.22% of initial]
[Iter 1600/20000] Loss: 0.0030720 (Best: 0.0022941 @iter1591) ([91m↑13.69%[0m) [1.22% of initial]
[Iter 1610/20000] Loss: 0.0083737 (Best: 0.0022941 @iter1591) ([91m↑172.58%[0m) [3.33% of initial]
[Iter 1610/20000] Loss: 0.0083737 (Best: 0.0022941 @iter1591) ([91m↑172.58%[0m) [3.33% of initial]
[Iter 1620/20000] Loss: 0.0053207 (Best: 0.0022941 @iter1591) ([92m↓36.46%[0m) [2.11% of initial]
[Iter 1620/20000] Loss: 0.0053207 (Best: 0.0022941 @iter1591) ([92m↓36.46%[0m) [2.11% of initial]
[Iter 1630/20000] Loss: 0.0040729 (Best: 0.0022941 @iter1591) ([92m↓23.45%[0m) [1.62% of initial]
[Iter 1630/20000] Loss: 0.0040729 (Best: 0.0022941 @iter1591) ([92m↓23.45%[0m) [1.62% of initial]
[Iter 1640/20000] Loss: 0.0038738 (Best: 0.0022941 @iter1591) ([92m↓4.89%[0m) [1.54% of initial]
[Iter 1640/20000] Loss: 0.0038738 (Best: 0.0022941 @iter1591) ([92m↓4.89%[0m) [1.54% of initial]
[Iter 1650/20000] Loss: 0.0033793 (Best: 0.0022941 @iter1591) ([92m↓12.77%[0m) [1.34% of initial]
[Iter 1650/20000] Loss: 0.0033793 (Best: 0.0022941 @iter1591) ([92m↓12.77%[0m) [1.34% of initial]
[Iter 1660/20000] Loss: 0.0029043 (Best: 0.0022941 @iter1591) ([92m↓14.06%[0m) [1.15% of initial]
[Iter 1660/20000] Loss: 0.0029043 (Best: 0.0022941 @iter1591) ([92m↓14.06%[0m) [1.15% of initial]
[Iter 1670/20000] Loss: 0.0026979 (Best: 0.0022461 @iter1669) ([92m↓7.11%[0m) [1.07% of initial]
[Iter 1670/20000] Loss: 0.0026979 (Best: 0.0022461 @iter1669) ([92m↓7.11%[0m) [1.07% of initial]
[Iter 1680/20000] Loss: 0.0028724 (Best: 0.0022461 @iter1669) ([91m↑6.47%[0m) [1.14% of initial]
[Iter 1680/20000] Loss: 0.0028724 (Best: 0.0022461 @iter1669) ([91m↑6.47%[0m) [1.14% of initial]
[Iter 1690/20000] Loss: 0.0030466 (Best: 0.0022461 @iter1669) ([91m↑6.06%[0m) [1.21% of initial]
[Iter 1690/20000] Loss: 0.0030466 (Best: 0.0022461 @iter1669) ([91m↑6.06%[0m) [1.21% of initial]
Iter:1699, L1 loss=0.002568, Total loss=0.003164, Time:14
Iter:1699, L1 loss=0.002568, Total loss=0.003164, Time:14
[Iter 1700/20000] Loss: 0.0027774 (Best: 0.0022461 @iter1669) ([92m↓8.83%[0m) [1.10% of initial]
[Iter 1700/20000] Loss: 0.0027774 (Best: 0.0022461 @iter1669) ([92m↓8.83%[0m) [1.10% of initial]
[Iter 1710/20000] Loss: 0.0029364 (Best: 0.0022461 @iter1669) ([91m↑5.73%[0m) [1.17% of initial]
[Iter 1710/20000] Loss: 0.0029364 (Best: 0.0022461 @iter1669) ([91m↑5.73%[0m) [1.17% of initial]
[Iter 1720/20000] Loss: 0.0025260 (Best: 0.0022289 @iter1715) ([92m↓13.98%[0m) [1.00% of initial]
[Iter 1720/20000] Loss: 0.0025260 (Best: 0.0022289 @iter1715) ([92m↓13.98%[0m) [1.00% of initial]
[Iter 1730/20000] Loss: 0.0026393 (Best: 0.0022289 @iter1715) ([91m↑4.49%[0m) [1.05% of initial]
[Iter 1730/20000] Loss: 0.0026393 (Best: 0.0022289 @iter1715) ([91m↑4.49%[0m) [1.05% of initial]
[Iter 1740/20000] Loss: 0.0026273 (Best: 0.0022271 @iter1738) ([92m↓0.46%[0m) [1.04% of initial]
[Iter 1740/20000] Loss: 0.0026273 (Best: 0.0022271 @iter1738) ([92m↓0.46%[0m) [1.04% of initial]
[Iter 1750/20000] Loss: 0.0023780 (Best: 0.0021381 @iter1742) ([92m↓9.49%[0m) [0.94% of initial]
[Iter 1750/20000] Loss: 0.0023780 (Best: 0.0021381 @iter1742) ([92m↓9.49%[0m) [0.94% of initial]
[Iter 1760/20000] Loss: 0.0026346 (Best: 0.0021381 @iter1742) ([91m↑10.79%[0m) [1.05% of initial]
[Iter 1760/20000] Loss: 0.0026346 (Best: 0.0021381 @iter1742) ([91m↑10.79%[0m) [1.05% of initial]
[Iter 1770/20000] Loss: 0.0024193 (Best: 0.0020959 @iter1762) ([92m↓8.17%[0m) [0.96% of initial]
[Iter 1770/20000] Loss: 0.0024193 (Best: 0.0020959 @iter1762) ([92m↓8.17%[0m) [0.96% of initial]
[Iter 1780/20000] Loss: 0.0024524 (Best: 0.0020845 @iter1771) ([91m↑1.37%[0m) [0.97% of initial]
[Iter 1780/20000] Loss: 0.0024524 (Best: 0.0020845 @iter1771) ([91m↑1.37%[0m) [0.97% of initial]
[Iter 1790/20000] Loss: 0.0021747 (Best: 0.0018077 @iter1789) ([92m↓11.32%[0m) [0.86% of initial]
[Iter 1790/20000] Loss: 0.0021747 (Best: 0.0018077 @iter1789) ([92m↓11.32%[0m) [0.86% of initial]
Iter:1799, L1 loss=0.001693, Total loss=0.001925, Time:14
Iter:1799, L1 loss=0.001693, Total loss=0.001925, Time:14
[Iter 1800/20000] Loss: 0.0021650 (Best: 0.0018077 @iter1789) ([92m↓0.44%[0m) [0.86% of initial]
[Iter 1800/20000] Loss: 0.0021650 (Best: 0.0018077 @iter1789) ([92m↓0.44%[0m) [0.86% of initial]
[Iter 1810/20000] Loss: 0.0076185 (Best: 0.0018077 @iter1789) ([91m↑251.89%[0m) [3.03% of initial]
[Iter 1810/20000] Loss: 0.0076185 (Best: 0.0018077 @iter1789) ([91m↑251.89%[0m) [3.03% of initial]
[Iter 1820/20000] Loss: 0.0044429 (Best: 0.0018077 @iter1789) ([92m↓41.68%[0m) [1.77% of initial]
[Iter 1820/20000] Loss: 0.0044429 (Best: 0.0018077 @iter1789) ([92m↓41.68%[0m) [1.77% of initial]
[Iter 1830/20000] Loss: 0.0039384 (Best: 0.0018077 @iter1789) ([92m↓11.36%[0m) [1.56% of initial]
[Iter 1830/20000] Loss: 0.0039384 (Best: 0.0018077 @iter1789) ([92m↓11.36%[0m) [1.56% of initial]
[Iter 1840/20000] Loss: 0.0027575 (Best: 0.0018077 @iter1789) ([92m↓29.98%[0m) [1.10% of initial]
[Iter 1840/20000] Loss: 0.0027575 (Best: 0.0018077 @iter1789) ([92m↓29.98%[0m) [1.10% of initial]
[Iter 1850/20000] Loss: 0.0026272 (Best: 0.0018077 @iter1789) ([92m↓4.73%[0m) [1.04% of initial]
[Iter 1850/20000] Loss: 0.0026272 (Best: 0.0018077 @iter1789) ([92m↓4.73%[0m) [1.04% of initial]
[Iter 1860/20000] Loss: 0.0023710 (Best: 0.0018077 @iter1789) ([92m↓9.75%[0m) [0.94% of initial]
[Iter 1860/20000] Loss: 0.0023710 (Best: 0.0018077 @iter1789) ([92m↓9.75%[0m) [0.94% of initial]
[Iter 1870/20000] Loss: 0.0022475 (Best: 0.0018077 @iter1789) ([92m↓5.21%[0m) [0.89% of initial]
[Iter 1870/20000] Loss: 0.0022475 (Best: 0.0018077 @iter1789) ([92m↓5.21%[0m) [0.89% of initial]
[Iter 1880/20000] Loss: 0.0021118 (Best: 0.0018077 @iter1789) ([92m↓6.03%[0m) [0.84% of initial]
[Iter 1880/20000] Loss: 0.0021118 (Best: 0.0018077 @iter1789) ([92m↓6.03%[0m) [0.84% of initial]
[Iter 1890/20000] Loss: 0.0018878 (Best: 0.0017169 @iter1890) ([92m↓10.61%[0m) [0.75% of initial]
[Iter 1890/20000] Loss: 0.0018878 (Best: 0.0017169 @iter1890) ([92m↓10.61%[0m) [0.75% of initial]
Iter:1899, L1 loss=0.001756, Total loss=0.001889, Time:13
Iter:1899, L1 loss=0.001756, Total loss=0.001889, Time:13
[Iter 1900/20000] Loss: 0.0019664 (Best: 0.0015890 @iter1891) ([91m↑4.16%[0m) [0.78% of initial]
[Iter 1900/20000] Loss: 0.0019664 (Best: 0.0015890 @iter1891) ([91m↑4.16%[0m) [0.78% of initial]
[Iter 1910/20000] Loss: 0.0020467 (Best: 0.0015890 @iter1891) ([91m↑4.09%[0m) [0.81% of initial]
[Iter 1910/20000] Loss: 0.0020467 (Best: 0.0015890 @iter1891) ([91m↑4.09%[0m) [0.81% of initial]
[Iter 1920/20000] Loss: 0.0020881 (Best: 0.0015890 @iter1891) ([91m↑2.02%[0m) [0.83% of initial]
[Iter 1920/20000] Loss: 0.0020881 (Best: 0.0015890 @iter1891) ([91m↑2.02%[0m) [0.83% of initial]
[Iter 1930/20000] Loss: 0.0017517 (Best: 0.0015675 @iter1930) ([92m↓16.11%[0m) [0.70% of initial]
[Iter 1930/20000] Loss: 0.0017517 (Best: 0.0015675 @iter1930) ([92m↓16.11%[0m) [0.70% of initial]
[Iter 1940/20000] Loss: 0.0019250 (Best: 0.0015619 @iter1939) ([91m↑9.89%[0m) [0.76% of initial]
[Iter 1940/20000] Loss: 0.0019250 (Best: 0.0015619 @iter1939) ([91m↑9.89%[0m) [0.76% of initial]
[Iter 1950/20000] Loss: 0.0020812 (Best: 0.0015619 @iter1939) ([91m↑8.11%[0m) [0.83% of initial]
[Iter 1950/20000] Loss: 0.0020812 (Best: 0.0015619 @iter1939) ([91m↑8.11%[0m) [0.83% of initial]
[Iter 1960/20000] Loss: 0.0018402 (Best: 0.0015619 @iter1939) ([92m↓11.58%[0m) [0.73% of initial]
[Iter 1960/20000] Loss: 0.0018402 (Best: 0.0015619 @iter1939) ([92m↓11.58%[0m) [0.73% of initial]
[Iter 1970/20000] Loss: 0.0016844 (Best: 0.0014912 @iter1963) ([92m↓8.46%[0m) [0.67% of initial]
[Iter 1970/20000] Loss: 0.0016844 (Best: 0.0014912 @iter1963) ([92m↓8.46%[0m) [0.67% of initial]
[Iter 1980/20000] Loss: 0.0020318 (Best: 0.0014912 @iter1963) ([91m↑20.62%[0m) [0.81% of initial]
[Iter 1980/20000] Loss: 0.0020318 (Best: 0.0014912 @iter1963) ([91m↑20.62%[0m) [0.81% of initial]
[Iter 1990/20000] Loss: 0.0017726 (Best: 0.0014912 @iter1963) ([92m↓12.76%[0m) [0.70% of initial]
[Iter 1990/20000] Loss: 0.0017726 (Best: 0.0014912 @iter1963) ([92m↓12.76%[0m) [0.70% of initial]
Iter:1999, L1 loss=0.001496, Total loss=0.001688, Time:14
Iter:1999, L1 loss=0.001496, Total loss=0.001688, Time:14
[Iter 2000/20000] Loss: 0.0019066 (Best: 0.0014440 @iter1996) ([91m↑7.56%[0m) [0.76% of initial]
[Iter 2000/20000] Loss: 0.0019066 (Best: 0.0014440 @iter1996) ([91m↑7.56%[0m) [0.76% of initial]
Testing Speed: 230.08004493750877 fps
Testing Speed: 230.08004493750877 fps
Testing Time: 0.217315673828125 s
Testing Time: 0.217315673828125 s

[ITER 2000] Evaluating test: SSIM = 0.8517311704158783, PSNR = 17.86451017379761

[ITER 2000] Evaluating test: SSIM = 0.8517311704158783, PSNR = 17.86451017379761
Testing Speed: 266.1979733017411 fps
Testing Speed: 266.1979733017411 fps
Testing Time: 0.011269807815551758 s
Testing Time: 0.011269807815551758 s

[ITER 2000] Evaluating train: SSIM = 0.9999508460362752, PSNR = 49.11960983276367

[ITER 2000] Evaluating train: SSIM = 0.9999508460362752, PSNR = 49.11960983276367
Iter:2000, total_points:43188
Iter:2000, total_points:43188
[Iter 2010/20000] Loss: 0.0066659 (Best: 0.0014440 @iter1996) ([91m↑249.61%[0m) [2.65% of initial]
[Iter 2010/20000] Loss: 0.0066659 (Best: 0.0014440 @iter1996) ([91m↑249.61%[0m) [2.65% of initial]
[Iter 2020/20000] Loss: 0.0037287 (Best: 0.0014440 @iter1996) ([92m↓44.06%[0m) [1.48% of initial]
[Iter 2020/20000] Loss: 0.0037287 (Best: 0.0014440 @iter1996) ([92m↓44.06%[0m) [1.48% of initial]
[Iter 2030/20000] Loss: 0.0028463 (Best: 0.0014440 @iter1996) ([92m↓23.67%[0m) [1.13% of initial]
[Iter 2030/20000] Loss: 0.0028463 (Best: 0.0014440 @iter1996) ([92m↓23.67%[0m) [1.13% of initial]
[Iter 2040/20000] Loss: 0.0024982 (Best: 0.0014440 @iter1996) ([92m↓12.23%[0m) [0.99% of initial]
[Iter 2040/20000] Loss: 0.0024982 (Best: 0.0014440 @iter1996) ([92m↓12.23%[0m) [0.99% of initial]
[Iter 2050/20000] Loss: 0.0020582 (Best: 0.0014440 @iter1996) ([92m↓17.61%[0m) [0.82% of initial]
[Iter 2050/20000] Loss: 0.0020582 (Best: 0.0014440 @iter1996) ([92m↓17.61%[0m) [0.82% of initial]
[Iter 2060/20000] Loss: 0.0017213 (Best: 0.0014440 @iter1996) ([92m↓16.37%[0m) [0.68% of initial]
[Iter 2060/20000] Loss: 0.0017213 (Best: 0.0014440 @iter1996) ([92m↓16.37%[0m) [0.68% of initial]
[Iter 2070/20000] Loss: 0.0019735 (Best: 0.0014440 @iter1996) ([91m↑14.65%[0m) [0.78% of initial]
[Iter 2070/20000] Loss: 0.0019735 (Best: 0.0014440 @iter1996) ([91m↑14.65%[0m) [0.78% of initial]
[Iter 2080/20000] Loss: 0.0018752 (Best: 0.0014440 @iter1996) ([92m↓4.98%[0m) [0.74% of initial]
[Iter 2080/20000] Loss: 0.0018752 (Best: 0.0014440 @iter1996) ([92m↓4.98%[0m) [0.74% of initial]
[Iter 2090/20000] Loss: 0.0018079 (Best: 0.0014129 @iter2089) ([92m↓3.59%[0m) [0.72% of initial]
[Iter 2090/20000] Loss: 0.0018079 (Best: 0.0014129 @iter2089) ([92m↓3.59%[0m) [0.72% of initial]
Iter:2099, L1 loss=0.001594, Total loss=0.0017, Time:15
Iter:2099, L1 loss=0.001594, Total loss=0.0017, Time:15
[Iter 2100/20000] Loss: 0.0016948 (Best: 0.0014129 @iter2089) ([92m↓6.26%[0m) [0.67% of initial]
[Iter 2100/20000] Loss: 0.0016948 (Best: 0.0014129 @iter2089) ([92m↓6.26%[0m) [0.67% of initial]
[Iter 2110/20000] Loss: 0.0015839 (Best: 0.0014129 @iter2089) ([92m↓6.54%[0m) [0.63% of initial]
[Iter 2110/20000] Loss: 0.0015839 (Best: 0.0014129 @iter2089) ([92m↓6.54%[0m) [0.63% of initial]
[Iter 2120/20000] Loss: 0.0014286 (Best: 0.0012930 @iter2120) ([92m↓9.80%[0m) [0.57% of initial]
[Iter 2120/20000] Loss: 0.0014286 (Best: 0.0012930 @iter2120) ([92m↓9.80%[0m) [0.57% of initial]
[Iter 2130/20000] Loss: 0.0015920 (Best: 0.0012231 @iter2125) ([91m↑11.44%[0m) [0.63% of initial]
[Iter 2130/20000] Loss: 0.0015920 (Best: 0.0012231 @iter2125) ([91m↑11.44%[0m) [0.63% of initial]
[Iter 2140/20000] Loss: 0.0017049 (Best: 0.0012231 @iter2125) ([91m↑7.09%[0m) [0.68% of initial]
[Iter 2140/20000] Loss: 0.0017049 (Best: 0.0012231 @iter2125) ([91m↑7.09%[0m) [0.68% of initial]
[Iter 2150/20000] Loss: 0.0017383 (Best: 0.0012231 @iter2125) ([91m↑1.96%[0m) [0.69% of initial]
[Iter 2150/20000] Loss: 0.0017383 (Best: 0.0012231 @iter2125) ([91m↑1.96%[0m) [0.69% of initial]
[Iter 2160/20000] Loss: 0.0015923 (Best: 0.0012231 @iter2125) ([92m↓8.40%[0m) [0.63% of initial]
[Iter 2160/20000] Loss: 0.0015923 (Best: 0.0012231 @iter2125) ([92m↓8.40%[0m) [0.63% of initial]
[Iter 2170/20000] Loss: 0.0016146 (Best: 0.0012231 @iter2125) ([91m↑1.40%[0m) [0.64% of initial]
[Iter 2170/20000] Loss: 0.0016146 (Best: 0.0012231 @iter2125) ([91m↑1.40%[0m) [0.64% of initial]
[Iter 2180/20000] Loss: 0.0013476 (Best: 0.0012231 @iter2125) ([92m↓16.54%[0m) [0.54% of initial]
[Iter 2180/20000] Loss: 0.0013476 (Best: 0.0012231 @iter2125) ([92m↓16.54%[0m) [0.54% of initial]
[Iter 2190/20000] Loss: 0.0015892 (Best: 0.0012231 @iter2125) ([91m↑17.93%[0m) [0.63% of initial]
[Iter 2190/20000] Loss: 0.0015892 (Best: 0.0012231 @iter2125) ([91m↑17.93%[0m) [0.63% of initial]
Iter:2199, L1 loss=0.001458, Total loss=0.001526, Time:16
Iter:2199, L1 loss=0.001458, Total loss=0.001526, Time:16
[Iter 2200/20000] Loss: 0.0015879 (Best: 0.0012231 @iter2125) ([92m↓0.08%[0m) [0.63% of initial]
[Iter 2200/20000] Loss: 0.0015879 (Best: 0.0012231 @iter2125) ([92m↓0.08%[0m) [0.63% of initial]
[Iter 2210/20000] Loss: 0.0079680 (Best: 0.0012231 @iter2125) ([91m↑401.80%[0m) [3.17% of initial]
[Iter 2210/20000] Loss: 0.0079680 (Best: 0.0012231 @iter2125) ([91m↑401.80%[0m) [3.17% of initial]
[Iter 2220/20000] Loss: 0.0043428 (Best: 0.0012231 @iter2125) ([92m↓45.50%[0m) [1.73% of initial]
[Iter 2220/20000] Loss: 0.0043428 (Best: 0.0012231 @iter2125) ([92m↓45.50%[0m) [1.73% of initial]
[Iter 2230/20000] Loss: 0.0026793 (Best: 0.0012231 @iter2125) ([92m↓38.31%[0m) [1.06% of initial]
[Iter 2230/20000] Loss: 0.0026793 (Best: 0.0012231 @iter2125) ([92m↓38.31%[0m) [1.06% of initial]
[Iter 2240/20000] Loss: 0.0022858 (Best: 0.0012231 @iter2125) ([92m↓14.69%[0m) [0.91% of initial]
[Iter 2240/20000] Loss: 0.0022858 (Best: 0.0012231 @iter2125) ([92m↓14.69%[0m) [0.91% of initial]
[Iter 2250/20000] Loss: 0.0021165 (Best: 0.0012231 @iter2125) ([92m↓7.41%[0m) [0.84% of initial]
[Iter 2250/20000] Loss: 0.0021165 (Best: 0.0012231 @iter2125) ([92m↓7.41%[0m) [0.84% of initial]
[Iter 2260/20000] Loss: 0.0017172 (Best: 0.0012231 @iter2125) ([92m↓18.86%[0m) [0.68% of initial]
[Iter 2260/20000] Loss: 0.0017172 (Best: 0.0012231 @iter2125) ([92m↓18.86%[0m) [0.68% of initial]
[Iter 2270/20000] Loss: 0.0018301 (Best: 0.0012231 @iter2125) ([91m↑6.57%[0m) [0.73% of initial]
[Iter 2270/20000] Loss: 0.0018301 (Best: 0.0012231 @iter2125) ([91m↑6.57%[0m) [0.73% of initial]
[Iter 2280/20000] Loss: 0.0014707 (Best: 0.0012231 @iter2125) ([92m↓19.64%[0m) [0.58% of initial]
[Iter 2280/20000] Loss: 0.0014707 (Best: 0.0012231 @iter2125) ([92m↓19.64%[0m) [0.58% of initial]
[Iter 2290/20000] Loss: 0.0014241 (Best: 0.0011960 @iter2285) ([92m↓3.17%[0m) [0.57% of initial]
[Iter 2290/20000] Loss: 0.0014241 (Best: 0.0011960 @iter2285) ([92m↓3.17%[0m) [0.57% of initial]
Iter:2299, L1 loss=0.001281, Total loss=0.001478, Time:15
Iter:2299, L1 loss=0.001281, Total loss=0.001478, Time:15
[Iter 2300/20000] Loss: 0.0017630 (Best: 0.0011960 @iter2285) ([91m↑23.80%[0m) [0.70% of initial]
[Iter 2300/20000] Loss: 0.0017630 (Best: 0.0011960 @iter2285) ([91m↑23.80%[0m) [0.70% of initial]
[Iter 2310/20000] Loss: 0.0015605 (Best: 0.0011960 @iter2285) ([92m↓11.49%[0m) [0.62% of initial]
[Iter 2310/20000] Loss: 0.0015605 (Best: 0.0011960 @iter2285) ([92m↓11.49%[0m) [0.62% of initial]
[Iter 2320/20000] Loss: 0.0013107 (Best: 0.0011700 @iter2320) ([92m↓16.01%[0m) [0.52% of initial]
[Iter 2320/20000] Loss: 0.0013107 (Best: 0.0011700 @iter2320) ([92m↓16.01%[0m) [0.52% of initial]
[Iter 2330/20000] Loss: 0.0012913 (Best: 0.0011132 @iter2327) ([92m↓1.47%[0m) [0.51% of initial]
[Iter 2330/20000] Loss: 0.0012913 (Best: 0.0011132 @iter2327) ([92m↓1.47%[0m) [0.51% of initial]
[Iter 2340/20000] Loss: 0.0013510 (Best: 0.0011050 @iter2338) ([91m↑4.62%[0m) [0.54% of initial]
[Iter 2340/20000] Loss: 0.0013510 (Best: 0.0011050 @iter2338) ([91m↑4.62%[0m) [0.54% of initial]
[Iter 2350/20000] Loss: 0.0015044 (Best: 0.0011050 @iter2338) ([91m↑11.36%[0m) [0.60% of initial]
[Iter 2350/20000] Loss: 0.0015044 (Best: 0.0011050 @iter2338) ([91m↑11.36%[0m) [0.60% of initial]
[Iter 2360/20000] Loss: 0.0013122 (Best: 0.0011050 @iter2338) ([92m↓12.78%[0m) [0.52% of initial]
[Iter 2360/20000] Loss: 0.0013122 (Best: 0.0011050 @iter2338) ([92m↓12.78%[0m) [0.52% of initial]
[Iter 2370/20000] Loss: 0.0014097 (Best: 0.0011050 @iter2338) ([91m↑7.43%[0m) [0.56% of initial]
[Iter 2370/20000] Loss: 0.0014097 (Best: 0.0011050 @iter2338) ([91m↑7.43%[0m) [0.56% of initial]
[Iter 2380/20000] Loss: 0.0014880 (Best: 0.0011050 @iter2338) ([91m↑5.56%[0m) [0.59% of initial]
[Iter 2380/20000] Loss: 0.0014880 (Best: 0.0011050 @iter2338) ([91m↑5.56%[0m) [0.59% of initial]
[Iter 2390/20000] Loss: 0.0016085 (Best: 0.0011050 @iter2338) ([91m↑8.10%[0m) [0.64% of initial]
[Iter 2390/20000] Loss: 0.0016085 (Best: 0.0011050 @iter2338) ([91m↑8.10%[0m) [0.64% of initial]
Iter:2399, L1 loss=0.00123, Total loss=0.001306, Time:14
Iter:2399, L1 loss=0.00123, Total loss=0.001306, Time:14
[Iter 2400/20000] Loss: 0.0013480 (Best: 0.0011050 @iter2338) ([92m↓16.20%[0m) [0.54% of initial]
[Iter 2400/20000] Loss: 0.0013480 (Best: 0.0011050 @iter2338) ([92m↓16.20%[0m) [0.54% of initial]
[Iter 2410/20000] Loss: 0.0061113 (Best: 0.0011050 @iter2338) ([91m↑353.36%[0m) [2.43% of initial]
[Iter 2410/20000] Loss: 0.0061113 (Best: 0.0011050 @iter2338) ([91m↑353.36%[0m) [2.43% of initial]
[Iter 2420/20000] Loss: 0.0034878 (Best: 0.0011050 @iter2338) ([92m↓42.93%[0m) [1.39% of initial]
[Iter 2420/20000] Loss: 0.0034878 (Best: 0.0011050 @iter2338) ([92m↓42.93%[0m) [1.39% of initial]
[Iter 2430/20000] Loss: 0.0025936 (Best: 0.0011050 @iter2338) ([92m↓25.64%[0m) [1.03% of initial]
[Iter 2430/20000] Loss: 0.0025936 (Best: 0.0011050 @iter2338) ([92m↓25.64%[0m) [1.03% of initial]
[Iter 2440/20000] Loss: 0.0020421 (Best: 0.0011050 @iter2338) ([92m↓21.26%[0m) [0.81% of initial]
[Iter 2440/20000] Loss: 0.0020421 (Best: 0.0011050 @iter2338) ([92m↓21.26%[0m) [0.81% of initial]
[Iter 2450/20000] Loss: 0.0019604 (Best: 0.0011050 @iter2338) ([92m↓4.00%[0m) [0.78% of initial]
[Iter 2450/20000] Loss: 0.0019604 (Best: 0.0011050 @iter2338) ([92m↓4.00%[0m) [0.78% of initial]
[Iter 2460/20000] Loss: 0.0016919 (Best: 0.0011050 @iter2338) ([92m↓13.69%[0m) [0.67% of initial]
[Iter 2460/20000] Loss: 0.0016919 (Best: 0.0011050 @iter2338) ([92m↓13.69%[0m) [0.67% of initial]
[Iter 2470/20000] Loss: 0.0016763 (Best: 0.0011050 @iter2338) ([92m↓0.92%[0m) [0.67% of initial]
[Iter 2470/20000] Loss: 0.0016763 (Best: 0.0011050 @iter2338) ([92m↓0.92%[0m) [0.67% of initial]
[Iter 2480/20000] Loss: 0.0016986 (Best: 0.0011050 @iter2338) ([91m↑1.33%[0m) [0.67% of initial]
[Iter 2480/20000] Loss: 0.0016986 (Best: 0.0011050 @iter2338) ([91m↑1.33%[0m) [0.67% of initial]
[Iter 2490/20000] Loss: 0.0014873 (Best: 0.0011050 @iter2338) ([92m↓12.44%[0m) [0.59% of initial]
[Iter 2490/20000] Loss: 0.0014873 (Best: 0.0011050 @iter2338) ([92m↓12.44%[0m) [0.59% of initial]
Iter:2499, L1 loss=0.00121, Total loss=0.001238, Time:17
Iter:2499, L1 loss=0.00121, Total loss=0.001238, Time:17
[Iter 2500/20000] Loss: 0.0013036 (Best: 0.0011050 @iter2338) ([92m↓12.35%[0m) [0.52% of initial]
[Iter 2500/20000] Loss: 0.0013036 (Best: 0.0011050 @iter2338) ([92m↓12.35%[0m) [0.52% of initial]
[Iter 2510/20000] Loss: 0.0013616 (Best: 0.0010396 @iter2504) ([91m↑4.46%[0m) [0.54% of initial]
[Iter 2510/20000] Loss: 0.0013616 (Best: 0.0010396 @iter2504) ([91m↑4.46%[0m) [0.54% of initial]
[Iter 2520/20000] Loss: 0.0012312 (Best: 0.0009997 @iter2519) ([92m↓9.58%[0m) [0.49% of initial]
[Iter 2520/20000] Loss: 0.0012312 (Best: 0.0009997 @iter2519) ([92m↓9.58%[0m) [0.49% of initial]
[Iter 2530/20000] Loss: 0.0011107 (Best: 0.0009715 @iter2528) ([92m↓9.79%[0m) [0.44% of initial]
[Iter 2530/20000] Loss: 0.0011107 (Best: 0.0009715 @iter2528) ([92m↓9.79%[0m) [0.44% of initial]
[Iter 2540/20000] Loss: 0.0012009 (Best: 0.0009715 @iter2528) ([91m↑8.12%[0m) [0.48% of initial]
[Iter 2540/20000] Loss: 0.0012009 (Best: 0.0009715 @iter2528) ([91m↑8.12%[0m) [0.48% of initial]
[Iter 2550/20000] Loss: 0.0014044 (Best: 0.0009715 @iter2528) ([91m↑16.95%[0m) [0.56% of initial]
[Iter 2550/20000] Loss: 0.0014044 (Best: 0.0009715 @iter2528) ([91m↑16.95%[0m) [0.56% of initial]
[Iter 2560/20000] Loss: 0.0011707 (Best: 0.0009707 @iter2557) ([92m↓16.64%[0m) [0.47% of initial]
[Iter 2560/20000] Loss: 0.0011707 (Best: 0.0009707 @iter2557) ([92m↓16.64%[0m) [0.47% of initial]
[Iter 2570/20000] Loss: 0.0014235 (Best: 0.0009707 @iter2557) ([91m↑21.59%[0m) [0.57% of initial]
[Iter 2570/20000] Loss: 0.0014235 (Best: 0.0009707 @iter2557) ([91m↑21.59%[0m) [0.57% of initial]
[Iter 2580/20000] Loss: 0.0012661 (Best: 0.0009270 @iter2578) ([92m↓11.06%[0m) [0.50% of initial]
[Iter 2580/20000] Loss: 0.0012661 (Best: 0.0009270 @iter2578) ([92m↓11.06%[0m) [0.50% of initial]
[Iter 2590/20000] Loss: 0.0013089 (Best: 0.0009270 @iter2578) ([91m↑3.38%[0m) [0.52% of initial]
[Iter 2590/20000] Loss: 0.0013089 (Best: 0.0009270 @iter2578) ([91m↑3.38%[0m) [0.52% of initial]
Iter:2599, L1 loss=0.001054, Total loss=0.001008, Time:16
Iter:2599, L1 loss=0.001054, Total loss=0.001008, Time:16
[Iter 2600/20000] Loss: 0.0011908 (Best: 0.0009210 @iter2594) ([92m↓9.02%[0m) [0.47% of initial]
[Iter 2600/20000] Loss: 0.0011908 (Best: 0.0009210 @iter2594) ([92m↓9.02%[0m) [0.47% of initial]
[Iter 2610/20000] Loss: 0.0060264 (Best: 0.0009210 @iter2594) ([91m↑406.07%[0m) [2.39% of initial]
[Iter 2610/20000] Loss: 0.0060264 (Best: 0.0009210 @iter2594) ([91m↑406.07%[0m) [2.39% of initial]
[Iter 2620/20000] Loss: 0.0033234 (Best: 0.0009210 @iter2594) ([92m↓44.85%[0m) [1.32% of initial]
[Iter 2620/20000] Loss: 0.0033234 (Best: 0.0009210 @iter2594) ([92m↓44.85%[0m) [1.32% of initial]
[Iter 2630/20000] Loss: 0.0022542 (Best: 0.0009210 @iter2594) ([92m↓32.17%[0m) [0.90% of initial]
[Iter 2630/20000] Loss: 0.0022542 (Best: 0.0009210 @iter2594) ([92m↓32.17%[0m) [0.90% of initial]
[Iter 2640/20000] Loss: 0.0017778 (Best: 0.0009210 @iter2594) ([92m↓21.14%[0m) [0.71% of initial]
[Iter 2640/20000] Loss: 0.0017778 (Best: 0.0009210 @iter2594) ([92m↓21.14%[0m) [0.71% of initial]
[Iter 2650/20000] Loss: 0.0014611 (Best: 0.0009210 @iter2594) ([92m↓17.81%[0m) [0.58% of initial]
[Iter 2650/20000] Loss: 0.0014611 (Best: 0.0009210 @iter2594) ([92m↓17.81%[0m) [0.58% of initial]
[Iter 2660/20000] Loss: 0.0016189 (Best: 0.0009210 @iter2594) ([91m↑10.80%[0m) [0.64% of initial]
[Iter 2660/20000] Loss: 0.0016189 (Best: 0.0009210 @iter2594) ([91m↑10.80%[0m) [0.64% of initial]
[Iter 2670/20000] Loss: 0.0015562 (Best: 0.0009210 @iter2594) ([92m↓3.87%[0m) [0.62% of initial]
[Iter 2670/20000] Loss: 0.0015562 (Best: 0.0009210 @iter2594) ([92m↓3.87%[0m) [0.62% of initial]
[Iter 2680/20000] Loss: 0.0012256 (Best: 0.0009210 @iter2594) ([92m↓21.24%[0m) [0.49% of initial]
[Iter 2680/20000] Loss: 0.0012256 (Best: 0.0009210 @iter2594) ([92m↓21.24%[0m) [0.49% of initial]
[Iter 2690/20000] Loss: 0.0011885 (Best: 0.0009210 @iter2594) ([92m↓3.03%[0m) [0.47% of initial]
[Iter 2690/20000] Loss: 0.0011885 (Best: 0.0009210 @iter2594) ([92m↓3.03%[0m) [0.47% of initial]
Iter:2699, L1 loss=0.00114, Total loss=0.001157, Time:16
Iter:2699, L1 loss=0.00114, Total loss=0.001157, Time:16
[Iter 2700/20000] Loss: 0.0014467 (Best: 0.0009210 @iter2594) ([91m↑21.73%[0m) [0.57% of initial]
[Iter 2700/20000] Loss: 0.0014467 (Best: 0.0009210 @iter2594) ([91m↑21.73%[0m) [0.57% of initial]
[Iter 2710/20000] Loss: 0.0012612 (Best: 0.0009210 @iter2594) ([92m↓12.82%[0m) [0.50% of initial]
[Iter 2710/20000] Loss: 0.0012612 (Best: 0.0009210 @iter2594) ([92m↓12.82%[0m) [0.50% of initial]
[Iter 2720/20000] Loss: 0.0011277 (Best: 0.0009210 @iter2594) ([92m↓10.59%[0m) [0.45% of initial]
[Iter 2720/20000] Loss: 0.0011277 (Best: 0.0009210 @iter2594) ([92m↓10.59%[0m) [0.45% of initial]
[Iter 2730/20000] Loss: 0.0010149 (Best: 0.0008751 @iter2727) ([92m↓10.00%[0m) [0.40% of initial]
[Iter 2730/20000] Loss: 0.0010149 (Best: 0.0008751 @iter2727) ([92m↓10.00%[0m) [0.40% of initial]
[Iter 2740/20000] Loss: 0.0008781 (Best: 0.0007820 @iter2740) ([92m↓13.48%[0m) [0.35% of initial]
[Iter 2740/20000] Loss: 0.0008781 (Best: 0.0007820 @iter2740) ([92m↓13.48%[0m) [0.35% of initial]
[Iter 2750/20000] Loss: 0.0011353 (Best: 0.0007820 @iter2740) ([91m↑29.29%[0m) [0.45% of initial]
[Iter 2750/20000] Loss: 0.0011353 (Best: 0.0007820 @iter2740) ([91m↑29.29%[0m) [0.45% of initial]
[Iter 2760/20000] Loss: 0.0011704 (Best: 0.0007820 @iter2740) ([91m↑3.09%[0m) [0.46% of initial]
[Iter 2760/20000] Loss: 0.0011704 (Best: 0.0007820 @iter2740) ([91m↑3.09%[0m) [0.46% of initial]
[Iter 2770/20000] Loss: 0.0012931 (Best: 0.0007820 @iter2740) ([91m↑10.49%[0m) [0.51% of initial]
[Iter 2770/20000] Loss: 0.0012931 (Best: 0.0007820 @iter2740) ([91m↑10.49%[0m) [0.51% of initial]
[Iter 2780/20000] Loss: 0.0011058 (Best: 0.0007820 @iter2740) ([92m↓14.48%[0m) [0.44% of initial]
[Iter 2780/20000] Loss: 0.0011058 (Best: 0.0007820 @iter2740) ([92m↓14.48%[0m) [0.44% of initial]
[Iter 2790/20000] Loss: 0.0011605 (Best: 0.0007820 @iter2740) ([91m↑4.95%[0m) [0.46% of initial]
[Iter 2790/20000] Loss: 0.0011605 (Best: 0.0007820 @iter2740) ([91m↑4.95%[0m) [0.46% of initial]
Iter:2799, L1 loss=0.001278, Total loss=0.001304, Time:16
Iter:2799, L1 loss=0.001278, Total loss=0.001304, Time:16
[Iter 2800/20000] Loss: 0.0011854 (Best: 0.0007820 @iter2740) ([91m↑2.15%[0m) [0.47% of initial]
[Iter 2800/20000] Loss: 0.0011854 (Best: 0.0007820 @iter2740) ([91m↑2.15%[0m) [0.47% of initial]
[Iter 2810/20000] Loss: 0.0050917 (Best: 0.0007820 @iter2740) ([91m↑329.52%[0m) [2.02% of initial]
[Iter 2810/20000] Loss: 0.0050917 (Best: 0.0007820 @iter2740) ([91m↑329.52%[0m) [2.02% of initial]
[Iter 2820/20000] Loss: 0.0027653 (Best: 0.0007820 @iter2740) ([92m↓45.69%[0m) [1.10% of initial]
[Iter 2820/20000] Loss: 0.0027653 (Best: 0.0007820 @iter2740) ([92m↓45.69%[0m) [1.10% of initial]
[Iter 2830/20000] Loss: 0.0018263 (Best: 0.0007820 @iter2740) ([92m↓33.95%[0m) [0.73% of initial]
[Iter 2830/20000] Loss: 0.0018263 (Best: 0.0007820 @iter2740) ([92m↓33.95%[0m) [0.73% of initial]
[Iter 2840/20000] Loss: 0.0015724 (Best: 0.0007820 @iter2740) ([92m↓13.90%[0m) [0.62% of initial]
[Iter 2840/20000] Loss: 0.0015724 (Best: 0.0007820 @iter2740) ([92m↓13.90%[0m) [0.62% of initial]
[Iter 2850/20000] Loss: 0.0013431 (Best: 0.0007820 @iter2740) ([92m↓14.58%[0m) [0.53% of initial]
[Iter 2850/20000] Loss: 0.0013431 (Best: 0.0007820 @iter2740) ([92m↓14.58%[0m) [0.53% of initial]
[Iter 2860/20000] Loss: 0.0014726 (Best: 0.0007820 @iter2740) ([91m↑9.64%[0m) [0.59% of initial]
[Iter 2860/20000] Loss: 0.0014726 (Best: 0.0007820 @iter2740) ([91m↑9.64%[0m) [0.59% of initial]
[Iter 2870/20000] Loss: 0.0012416 (Best: 0.0007820 @iter2740) ([92m↓15.69%[0m) [0.49% of initial]
[Iter 2870/20000] Loss: 0.0012416 (Best: 0.0007820 @iter2740) ([92m↓15.69%[0m) [0.49% of initial]
[Iter 2880/20000] Loss: 0.0011669 (Best: 0.0007820 @iter2740) ([92m↓6.02%[0m) [0.46% of initial]
[Iter 2880/20000] Loss: 0.0011669 (Best: 0.0007820 @iter2740) ([92m↓6.02%[0m) [0.46% of initial]
[Iter 2890/20000] Loss: 0.0011449 (Best: 0.0007820 @iter2740) ([92m↓1.88%[0m) [0.45% of initial]
[Iter 2890/20000] Loss: 0.0011449 (Best: 0.0007820 @iter2740) ([92m↓1.88%[0m) [0.45% of initial]
Iter:2899, L1 loss=0.0008878, Total loss=0.0008848, Time:19
Iter:2899, L1 loss=0.0008878, Total loss=0.0008848, Time:19
[Iter 2900/20000] Loss: 0.0010884 (Best: 0.0007820 @iter2740) ([92m↓4.94%[0m) [0.43% of initial]
[Iter 2900/20000] Loss: 0.0010884 (Best: 0.0007820 @iter2740) ([92m↓4.94%[0m) [0.43% of initial]
[Iter 2910/20000] Loss: 0.0011314 (Best: 0.0007820 @iter2740) ([91m↑3.94%[0m) [0.45% of initial]
[Iter 2910/20000] Loss: 0.0011314 (Best: 0.0007820 @iter2740) ([91m↑3.94%[0m) [0.45% of initial]
[Iter 2920/20000] Loss: 0.0011826 (Best: 0.0007820 @iter2740) ([91m↑4.53%[0m) [0.47% of initial]
[Iter 2920/20000] Loss: 0.0011826 (Best: 0.0007820 @iter2740) ([91m↑4.53%[0m) [0.47% of initial]
[Iter 2930/20000] Loss: 0.0011536 (Best: 0.0007820 @iter2740) ([92m↓2.45%[0m) [0.46% of initial]
[Iter 2930/20000] Loss: 0.0011536 (Best: 0.0007820 @iter2740) ([92m↓2.45%[0m) [0.46% of initial]
[Iter 2940/20000] Loss: 0.0009692 (Best: 0.0007820 @iter2740) ([92m↓15.98%[0m) [0.39% of initial]
[Iter 2940/20000] Loss: 0.0009692 (Best: 0.0007820 @iter2740) ([92m↓15.98%[0m) [0.39% of initial]
[Iter 2950/20000] Loss: 0.0009331 (Best: 0.0007767 @iter2950) ([92m↓3.73%[0m) [0.37% of initial]
[Iter 2950/20000] Loss: 0.0009331 (Best: 0.0007767 @iter2950) ([92m↓3.73%[0m) [0.37% of initial]
[Iter 2960/20000] Loss: 0.0010308 (Best: 0.0007767 @iter2950) ([91m↑10.48%[0m) [0.41% of initial]
[Iter 2960/20000] Loss: 0.0010308 (Best: 0.0007767 @iter2950) ([91m↑10.48%[0m) [0.41% of initial]
[Iter 2970/20000] Loss: 0.0009184 (Best: 0.0007197 @iter2969) ([92m↓10.91%[0m) [0.36% of initial]
[Iter 2970/20000] Loss: 0.0009184 (Best: 0.0007197 @iter2969) ([92m↓10.91%[0m) [0.36% of initial]
[Iter 2980/20000] Loss: 0.0008411 (Best: 0.0007197 @iter2969) ([92m↓8.42%[0m) [0.33% of initial]
[Iter 2980/20000] Loss: 0.0008411 (Best: 0.0007197 @iter2969) ([92m↓8.42%[0m) [0.33% of initial]
[Iter 2990/20000] Loss: 0.0009022 (Best: 0.0006887 @iter2983) ([91m↑7.26%[0m) [0.36% of initial]
[Iter 2990/20000] Loss: 0.0009022 (Best: 0.0006887 @iter2983) ([91m↑7.26%[0m) [0.36% of initial]
Iter:2999, L1 loss=0.0007087, Total loss=0.0006713, Time:18
Iter:2999, L1 loss=0.0007087, Total loss=0.0006713, Time:18
[Iter 3000/20000] Loss: 0.0008657 (Best: 0.0006713 @iter2999) ([92m↓4.04%[0m) [0.34% of initial]
[Iter 3000/20000] Loss: 0.0008657 (Best: 0.0006713 @iter2999) ([92m↓4.04%[0m) [0.34% of initial]
[Iter 3010/20000] Loss: 0.0046394 (Best: 0.0006713 @iter2999) ([91m↑435.91%[0m) [1.84% of initial]
[Iter 3010/20000] Loss: 0.0046394 (Best: 0.0006713 @iter2999) ([91m↑435.91%[0m) [1.84% of initial]
[Iter 3020/20000] Loss: 0.0027093 (Best: 0.0006713 @iter2999) ([92m↓41.60%[0m) [1.08% of initial]
[Iter 3020/20000] Loss: 0.0027093 (Best: 0.0006713 @iter2999) ([92m↓41.60%[0m) [1.08% of initial]
[Iter 3030/20000] Loss: 0.0021167 (Best: 0.0006713 @iter2999) ([92m↓21.87%[0m) [0.84% of initial]
[Iter 3030/20000] Loss: 0.0021167 (Best: 0.0006713 @iter2999) ([92m↓21.87%[0m) [0.84% of initial]
[Iter 3040/20000] Loss: 0.0016990 (Best: 0.0006713 @iter2999) ([92m↓19.73%[0m) [0.67% of initial]
[Iter 3040/20000] Loss: 0.0016990 (Best: 0.0006713 @iter2999) ([92m↓19.73%[0m) [0.67% of initial]
[Iter 3050/20000] Loss: 0.0014227 (Best: 0.0006713 @iter2999) ([92m↓16.26%[0m) [0.57% of initial]
[Iter 3050/20000] Loss: 0.0014227 (Best: 0.0006713 @iter2999) ([92m↓16.26%[0m) [0.57% of initial]
[Iter 3060/20000] Loss: 0.0013843 (Best: 0.0006713 @iter2999) ([92m↓2.70%[0m) [0.55% of initial]
[Iter 3060/20000] Loss: 0.0013843 (Best: 0.0006713 @iter2999) ([92m↓2.70%[0m) [0.55% of initial]
[Iter 3070/20000] Loss: 0.0011546 (Best: 0.0006713 @iter2999) ([92m↓16.59%[0m) [0.46% of initial]
[Iter 3070/20000] Loss: 0.0011546 (Best: 0.0006713 @iter2999) ([92m↓16.59%[0m) [0.46% of initial]
[Iter 3080/20000] Loss: 0.0011452 (Best: 0.0006713 @iter2999) ([92m↓0.81%[0m) [0.45% of initial]
[Iter 3080/20000] Loss: 0.0011452 (Best: 0.0006713 @iter2999) ([92m↓0.81%[0m) [0.45% of initial]
[Iter 3090/20000] Loss: 0.0010643 (Best: 0.0006713 @iter2999) ([92m↓7.07%[0m) [0.42% of initial]
[Iter 3090/20000] Loss: 0.0010643 (Best: 0.0006713 @iter2999) ([92m↓7.07%[0m) [0.42% of initial]
Iter:3099, L1 loss=0.0009215, Total loss=0.000855, Time:18
Iter:3099, L1 loss=0.0009215, Total loss=0.000855, Time:18
[Iter 3100/20000] Loss: 0.0010017 (Best: 0.0006713 @iter2999) ([92m↓5.88%[0m) [0.40% of initial]
[Iter 3100/20000] Loss: 0.0010017 (Best: 0.0006713 @iter2999) ([92m↓5.88%[0m) [0.40% of initial]
[Iter 3110/20000] Loss: 0.0010856 (Best: 0.0006713 @iter2999) ([91m↑8.37%[0m) [0.43% of initial]
[Iter 3110/20000] Loss: 0.0010856 (Best: 0.0006713 @iter2999) ([91m↑8.37%[0m) [0.43% of initial]
[Iter 3120/20000] Loss: 0.0010447 (Best: 0.0006713 @iter2999) ([92m↓3.76%[0m) [0.42% of initial]
[Iter 3120/20000] Loss: 0.0010447 (Best: 0.0006713 @iter2999) ([92m↓3.76%[0m) [0.42% of initial]
[Iter 3130/20000] Loss: 0.0008586 (Best: 0.0006713 @iter2999) ([92m↓17.82%[0m) [0.34% of initial]
[Iter 3130/20000] Loss: 0.0008586 (Best: 0.0006713 @iter2999) ([92m↓17.82%[0m) [0.34% of initial]
[Iter 3140/20000] Loss: 0.0008658 (Best: 0.0006713 @iter2999) ([91m↑0.84%[0m) [0.34% of initial]
[Iter 3140/20000] Loss: 0.0008658 (Best: 0.0006713 @iter2999) ([91m↑0.84%[0m) [0.34% of initial]
[Iter 3150/20000] Loss: 0.0008860 (Best: 0.0006713 @iter2999) ([91m↑2.33%[0m) [0.35% of initial]
[Iter 3150/20000] Loss: 0.0008860 (Best: 0.0006713 @iter2999) ([91m↑2.33%[0m) [0.35% of initial]
[Iter 3160/20000] Loss: 0.0008144 (Best: 0.0006713 @iter2999) ([92m↓8.08%[0m) [0.32% of initial]
[Iter 3160/20000] Loss: 0.0008144 (Best: 0.0006713 @iter2999) ([92m↓8.08%[0m) [0.32% of initial]
[Iter 3170/20000] Loss: 0.0008377 (Best: 0.0006713 @iter2999) ([91m↑2.86%[0m) [0.33% of initial]
[Iter 3170/20000] Loss: 0.0008377 (Best: 0.0006713 @iter2999) ([91m↑2.86%[0m) [0.33% of initial]
[Iter 3180/20000] Loss: 0.0008571 (Best: 0.0006713 @iter2999) ([91m↑2.32%[0m) [0.34% of initial]
[Iter 3180/20000] Loss: 0.0008571 (Best: 0.0006713 @iter2999) ([91m↑2.32%[0m) [0.34% of initial]
[Iter 3190/20000] Loss: 0.0008445 (Best: 0.0006713 @iter2999) ([92m↓1.47%[0m) [0.34% of initial]
[Iter 3190/20000] Loss: 0.0008445 (Best: 0.0006713 @iter2999) ([92m↓1.47%[0m) [0.34% of initial]
Iter:3199, L1 loss=0.0007892, Total loss=0.0007318, Time:18
Iter:3199, L1 loss=0.0007892, Total loss=0.0007318, Time:18
[Iter 3200/20000] Loss: 0.0008220 (Best: 0.0006418 @iter3196) ([92m↓2.66%[0m) [0.33% of initial]
[Iter 3200/20000] Loss: 0.0008220 (Best: 0.0006418 @iter3196) ([92m↓2.66%[0m) [0.33% of initial]
[Iter 3210/20000] Loss: 0.0052154 (Best: 0.0006418 @iter3196) ([91m↑534.43%[0m) [2.07% of initial]
[Iter 3210/20000] Loss: 0.0052154 (Best: 0.0006418 @iter3196) ([91m↑534.43%[0m) [2.07% of initial]
[Iter 3220/20000] Loss: 0.0028213 (Best: 0.0006418 @iter3196) ([92m↓45.90%[0m) [1.12% of initial]
[Iter 3220/20000] Loss: 0.0028213 (Best: 0.0006418 @iter3196) ([92m↓45.90%[0m) [1.12% of initial]
[Iter 3230/20000] Loss: 0.0017170 (Best: 0.0006418 @iter3196) ([92m↓39.14%[0m) [0.68% of initial]
[Iter 3230/20000] Loss: 0.0017170 (Best: 0.0006418 @iter3196) ([92m↓39.14%[0m) [0.68% of initial]
[Iter 3240/20000] Loss: 0.0015272 (Best: 0.0006418 @iter3196) ([92m↓11.05%[0m) [0.61% of initial]
[Iter 3240/20000] Loss: 0.0015272 (Best: 0.0006418 @iter3196) ([92m↓11.05%[0m) [0.61% of initial]
[Iter 3250/20000] Loss: 0.0011228 (Best: 0.0006418 @iter3196) ([92m↓26.48%[0m) [0.45% of initial]
[Iter 3250/20000] Loss: 0.0011228 (Best: 0.0006418 @iter3196) ([92m↓26.48%[0m) [0.45% of initial]
[Iter 3260/20000] Loss: 0.0009681 (Best: 0.0006418 @iter3196) ([92m↓13.77%[0m) [0.38% of initial]
[Iter 3260/20000] Loss: 0.0009681 (Best: 0.0006418 @iter3196) ([92m↓13.77%[0m) [0.38% of initial]
[Iter 3270/20000] Loss: 0.0010060 (Best: 0.0006418 @iter3196) ([91m↑3.91%[0m) [0.40% of initial]
[Iter 3270/20000] Loss: 0.0010060 (Best: 0.0006418 @iter3196) ([91m↑3.91%[0m) [0.40% of initial]
[Iter 3280/20000] Loss: 0.0010620 (Best: 0.0006418 @iter3196) ([91m↑5.57%[0m) [0.42% of initial]
[Iter 3280/20000] Loss: 0.0010620 (Best: 0.0006418 @iter3196) ([91m↑5.57%[0m) [0.42% of initial]
[Iter 3290/20000] Loss: 0.0008013 (Best: 0.0006418 @iter3196) ([92m↓24.55%[0m) [0.32% of initial]
[Iter 3290/20000] Loss: 0.0008013 (Best: 0.0006418 @iter3196) ([92m↓24.55%[0m) [0.32% of initial]
Iter:3299, L1 loss=0.001253, Total loss=0.001328, Time:15
Iter:3299, L1 loss=0.001253, Total loss=0.001328, Time:15
[Iter 3300/20000] Loss: 0.0010777 (Best: 0.0006418 @iter3196) ([91m↑34.49%[0m) [0.43% of initial]
[Iter 3300/20000] Loss: 0.0010777 (Best: 0.0006418 @iter3196) ([91m↑34.49%[0m) [0.43% of initial]
[Iter 3310/20000] Loss: 0.0008085 (Best: 0.0006418 @iter3196) ([92m↓24.98%[0m) [0.32% of initial]
[Iter 3310/20000] Loss: 0.0008085 (Best: 0.0006418 @iter3196) ([92m↓24.98%[0m) [0.32% of initial]
[Iter 3320/20000] Loss: 0.0009283 (Best: 0.0006418 @iter3196) ([91m↑14.82%[0m) [0.37% of initial]
[Iter 3320/20000] Loss: 0.0009283 (Best: 0.0006418 @iter3196) ([91m↑14.82%[0m) [0.37% of initial]
[Iter 3330/20000] Loss: 0.0009393 (Best: 0.0006418 @iter3196) ([91m↑1.18%[0m) [0.37% of initial]
[Iter 3330/20000] Loss: 0.0009393 (Best: 0.0006418 @iter3196) ([91m↑1.18%[0m) [0.37% of initial]
[Iter 3340/20000] Loss: 0.0010740 (Best: 0.0006418 @iter3196) ([91m↑14.35%[0m) [0.43% of initial]
[Iter 3340/20000] Loss: 0.0010740 (Best: 0.0006418 @iter3196) ([91m↑14.35%[0m) [0.43% of initial]
[Iter 3350/20000] Loss: 0.0008690 (Best: 0.0006418 @iter3196) ([92m↓19.09%[0m) [0.35% of initial]
[Iter 3350/20000] Loss: 0.0008690 (Best: 0.0006418 @iter3196) ([92m↓19.09%[0m) [0.35% of initial]
[Iter 3360/20000] Loss: 0.0011032 (Best: 0.0006418 @iter3196) ([91m↑26.95%[0m) [0.44% of initial]
[Iter 3360/20000] Loss: 0.0011032 (Best: 0.0006418 @iter3196) ([91m↑26.95%[0m) [0.44% of initial]
[Iter 3370/20000] Loss: 0.0007862 (Best: 0.0006418 @iter3196) ([92m↓28.74%[0m) [0.31% of initial]
[Iter 3370/20000] Loss: 0.0007862 (Best: 0.0006418 @iter3196) ([92m↓28.74%[0m) [0.31% of initial]
[Iter 3380/20000] Loss: 0.0007741 (Best: 0.0006289 @iter3379) ([92m↓1.54%[0m) [0.31% of initial]
[Iter 3380/20000] Loss: 0.0007741 (Best: 0.0006289 @iter3379) ([92m↓1.54%[0m) [0.31% of initial]
[Iter 3390/20000] Loss: 0.0010080 (Best: 0.0006289 @iter3379) ([91m↑30.23%[0m) [0.40% of initial]
[Iter 3390/20000] Loss: 0.0010080 (Best: 0.0006289 @iter3379) ([91m↑30.23%[0m) [0.40% of initial]
Iter:3399, L1 loss=0.001241, Total loss=0.00124, Time:18
Iter:3399, L1 loss=0.001241, Total loss=0.00124, Time:18
[Iter 3400/20000] Loss: 0.0010314 (Best: 0.0006289 @iter3379) ([91m↑2.32%[0m) [0.41% of initial]
[Iter 3400/20000] Loss: 0.0010314 (Best: 0.0006289 @iter3379) ([91m↑2.32%[0m) [0.41% of initial]
[Iter 3410/20000] Loss: 0.0043589 (Best: 0.0006289 @iter3379) ([91m↑322.61%[0m) [1.73% of initial]
[Iter 3410/20000] Loss: 0.0043589 (Best: 0.0006289 @iter3379) ([91m↑322.61%[0m) [1.73% of initial]
[Iter 3420/20000] Loss: 0.0023098 (Best: 0.0006289 @iter3379) ([92m↓47.01%[0m) [0.92% of initial]
[Iter 3420/20000] Loss: 0.0023098 (Best: 0.0006289 @iter3379) ([92m↓47.01%[0m) [0.92% of initial]
[Iter 3430/20000] Loss: 0.0015330 (Best: 0.0006289 @iter3379) ([92m↓33.63%[0m) [0.61% of initial]
[Iter 3430/20000] Loss: 0.0015330 (Best: 0.0006289 @iter3379) ([92m↓33.63%[0m) [0.61% of initial]
[Iter 3440/20000] Loss: 0.0013342 (Best: 0.0006289 @iter3379) ([92m↓12.96%[0m) [0.53% of initial]
[Iter 3440/20000] Loss: 0.0013342 (Best: 0.0006289 @iter3379) ([92m↓12.96%[0m) [0.53% of initial]
[Iter 3450/20000] Loss: 0.0011990 (Best: 0.0006289 @iter3379) ([92m↓10.14%[0m) [0.48% of initial]
[Iter 3450/20000] Loss: 0.0011990 (Best: 0.0006289 @iter3379) ([92m↓10.14%[0m) [0.48% of initial]
[Iter 3460/20000] Loss: 0.0010892 (Best: 0.0006289 @iter3379) ([92m↓9.16%[0m) [0.43% of initial]
[Iter 3460/20000] Loss: 0.0010892 (Best: 0.0006289 @iter3379) ([92m↓9.16%[0m) [0.43% of initial]
[Iter 3470/20000] Loss: 0.0009957 (Best: 0.0006289 @iter3379) ([92m↓8.58%[0m) [0.40% of initial]
[Iter 3470/20000] Loss: 0.0009957 (Best: 0.0006289 @iter3379) ([92m↓8.58%[0m) [0.40% of initial]
[Iter 3480/20000] Loss: 0.0009311 (Best: 0.0006289 @iter3379) ([92m↓6.49%[0m) [0.37% of initial]
[Iter 3480/20000] Loss: 0.0009311 (Best: 0.0006289 @iter3379) ([92m↓6.49%[0m) [0.37% of initial]
Optimizing 
Training parameters: {'iterations': 20000, 'position_lr_init': 0.00019, 'position_lr_final': 1.9e-06, 'position_lr_delay_mult': 0.01, 'position_lr_max_steps': 20000, 'feature_lr': 0.002, 'opacity_lr': 0.008, 'scaling_lr': 0.005, 'rotation_lr': 0.001, 'percent_dense': 0.01, 'lambda_dssim': 0.2, 'densification_interval': 200, 'opacity_reset_interval': 4000, 'densify_from_iter': 500, 'densify_until_iter': 8000, 'densify_grad_threshold': 2.6e-05, 'random_background': False, 'sample_pseudo_interval': 1, 'start_sample_pseudo': 0, 'end_sample_pseudo': 30000}
Create gaussians1
GsDict.keys() is dict_keys(['gs0', 'gs1'])

Starting training...
Total iterations: 20000
==================================================
[Iter 10/20000] Loss: 0.2179127 (Best: 0.2126908 @iter10) [100.00% of initial]
[Iter 20/20000] Loss: 0.1746701 (Best: 0.1693031 @iter20) ([92m↓19.84%[0m) [69.39% of initial]
[Iter 30/20000] Loss: 0.1374906 (Best: 0.1327878 @iter30) ([92m↓21.29%[0m) [54.62% of initial]
[Iter 40/20000] Loss: 0.1123922 (Best: 0.1098378 @iter40) ([92m↓18.25%[0m) [44.65% of initial]
[Iter 50/20000] Loss: 0.0993461 (Best: 0.0965453 @iter49) ([92m↓11.61%[0m) [39.47% of initial]
[Iter 60/20000] Loss: 0.0936759 (Best: 0.0908450 @iter59) ([92m↓5.71%[0m) [37.22% of initial]
[Iter 70/20000] Loss: 0.0884556 (Best: 0.0869402 @iter70) ([92m↓5.57%[0m) [35.14% of initial]
[Iter 80/20000] Loss: 0.0851898 (Best: 0.0831016 @iter80) ([92m↓3.69%[0m) [33.85% of initial]
[Iter 90/20000] Loss: 0.0824184 (Best: 0.0801595 @iter88) ([92m↓3.25%[0m) [32.74% of initial]
Iter:99, L1 loss=0.05723, Total loss=0.07877, Time:13
[Iter 100/20000] Loss: 0.0786774 (Best: 0.0766314 @iter97) ([92m↓4.54%[0m) [31.26% of initial]
[Iter 110/20000] Loss: 0.0753295 (Best: 0.0731336 @iter106) ([92m↓4.26%[0m) [29.93% of initial]
[Iter 120/20000] Loss: 0.0714518 (Best: 0.0685731 @iter118) ([92m↓5.15%[0m) [28.39% of initial]
[Iter 130/20000] Loss: 0.0667159 (Best: 0.0642162 @iter130) ([92m↓6.63%[0m) [26.51% of initial]
[Iter 140/20000] Loss: 0.0635590 (Best: 0.0613103 @iter140) ([92m↓4.73%[0m) [25.25% of initial]
[Iter 150/20000] Loss: 0.0613015 (Best: 0.0584095 @iter148) ([92m↓3.55%[0m) [24.35% of initial]
[Iter 160/20000] Loss: 0.0590986 (Best: 0.0559598 @iter157) ([92m↓3.59%[0m) [23.48% of initial]
[Iter 170/20000] Loss: 0.0564072 (Best: 0.0535446 @iter167) ([92m↓4.55%[0m) [22.41% of initial]
[Iter 180/20000] Loss: 0.0523819 (Best: 0.0500636 @iter179) ([92m↓7.14%[0m) [20.81% of initial]
[Iter 190/20000] Loss: 0.0495916 (Best: 0.0478368 @iter188) ([92m↓5.33%[0m) [19.70% of initial]
Iter:199, L1 loss=0.03439, Total loss=0.04979, Time:13
[Iter 200/20000] Loss: 0.0478299 (Best: 0.0457557 @iter198) ([92m↓3.55%[0m) [19.00% of initial]
[Iter 210/20000] Loss: 0.0450539 (Best: 0.0428284 @iter209) ([92m↓5.80%[0m) [17.90% of initial]
[Iter 220/20000] Loss: 0.0440717 (Best: 0.0412574 @iter219) ([92m↓2.18%[0m) [17.51% of initial]
[Iter 230/20000] Loss: 0.0422844 (Best: 0.0398593 @iter227) ([92m↓4.06%[0m) [16.80% of initial]
[Iter 240/20000] Loss: 0.0402716 (Best: 0.0377602 @iter238) ([92m↓4.76%[0m) [16.00% of initial]
[Iter 250/20000] Loss: 0.0379821 (Best: 0.0362288 @iter248) ([92m↓5.68%[0m) [15.09% of initial]
[Iter 260/20000] Loss: 0.0359069 (Best: 0.0343433 @iter260) ([92m↓5.46%[0m) [14.27% of initial]
[Iter 270/20000] Loss: 0.0349451 (Best: 0.0328457 @iter269) ([92m↓2.68%[0m) [13.88% of initial]
[Iter 280/20000] Loss: 0.0346833 (Best: 0.0316945 @iter277) ([92m↓0.75%[0m) [13.78% of initial]
[Iter 290/20000] Loss: 0.0331149 (Best: 0.0304131 @iter287) ([92m↓4.52%[0m) [13.16% of initial]
Iter:299, L1 loss=0.02212, Total loss=0.03347, Time:13
[Iter 300/20000] Loss: 0.0308962 (Best: 0.0289975 @iter300) ([92m↓6.70%[0m) [12.27% of initial]
[Iter 310/20000] Loss: 0.0294044 (Best: 0.0274145 @iter310) ([92m↓4.83%[0m) [11.68% of initial]
[Iter 320/20000] Loss: 0.0279572 (Best: 0.0266253 @iter320) ([92m↓4.92%[0m) [11.11% of initial]
[Iter 330/20000] Loss: 0.0275480 (Best: 0.0256602 @iter330) ([92m↓1.46%[0m) [10.94% of initial]
[Iter 340/20000] Loss: 0.0255335 (Best: 0.0244979 @iter340) ([92m↓7.31%[0m) [10.14% of initial]
[Iter 350/20000] Loss: 0.0261390 (Best: 0.0235734 @iter349) ([91m↑2.37%[0m) [10.38% of initial]
[Iter 360/20000] Loss: 0.0248085 (Best: 0.0227957 @iter358) ([92m↓5.09%[0m) [9.86% of initial]
[Iter 370/20000] Loss: 0.0245300 (Best: 0.0222807 @iter368) ([92m↓1.12%[0m) [9.75% of initial]
[Iter 380/20000] Loss: 0.0221990 (Best: 0.0210888 @iter379) ([92m↓9.50%[0m) [8.82% of initial]
[Iter 390/20000] Loss: 0.0216734 (Best: 0.0202980 @iter385) ([92m↓2.37%[0m) [8.61% of initial]
Optimizing 
Training parameters: {'iterations': 20000, 'position_lr_init': 0.00019, 'position_lr_final': 1.9e-06, 'position_lr_delay_mult': 0.01, 'position_lr_max_steps': 20000, 'feature_lr': 0.002, 'opacity_lr': 0.008, 'scaling_lr': 0.005, 'rotation_lr': 0.001, 'percent_dense': 0.01, 'lambda_dssim': 0.2, 'densification_interval': 200, 'opacity_reset_interval': 4000, 'densify_from_iter': 500, 'densify_until_iter': 8000, 'densify_grad_threshold': 2.6e-05, 'random_background': False, 'sample_pseudo_interval': 1, 'start_sample_pseudo': 0, 'end_sample_pseudo': 30000}
Create gaussians1
GsDict.keys() is dict_keys(['gs0', 'gs1'])

Starting training...
Total iterations: 20000
==================================================
[Iter 10/20000] Loss: 0.2179126 (Best: 0.2126908 @iter10) [100.00% of initial]
[Iter 20/20000] Loss: 0.1746701 (Best: 0.1693029 @iter20) ([92m↓19.84%[0m) [69.39% of initial]
[Iter 30/20000] Loss: 0.1374910 (Best: 0.1327884 @iter30) ([92m↓21.29%[0m) [54.62% of initial]
[Iter 40/20000] Loss: 0.1123923 (Best: 0.1098377 @iter40) ([92m↓18.25%[0m) [44.65% of initial]
[Iter 50/20000] Loss: 0.0993442 (Best: 0.0965458 @iter49) ([92m↓11.61%[0m) [39.47% of initial]
[Iter 60/20000] Loss: 0.0936773 (Best: 0.0908518 @iter59) ([92m↓5.70%[0m) [37.22% of initial]
[Iter 70/20000] Loss: 0.0884541 (Best: 0.0869403 @iter70) ([92m↓5.58%[0m) [35.14% of initial]
[Iter 80/20000] Loss: 0.0851913 (Best: 0.0831070 @iter80) ([92m↓3.69%[0m) [33.85% of initial]
[Iter 90/20000] Loss: 0.0824093 (Best: 0.0801457 @iter88) ([92m↓3.27%[0m) [32.74% of initial]
Iter:99, L1 loss=0.05723, Total loss=0.07877, Time:15
[Iter 100/20000] Loss: 0.0786736 (Best: 0.0766262 @iter97) ([92m↓4.53%[0m) [31.26% of initial]
[Iter 110/20000] Loss: 0.0753214 (Best: 0.0731315 @iter106) ([92m↓4.26%[0m) [29.92% of initial]
[Iter 120/20000] Loss: 0.0714356 (Best: 0.0685524 @iter118) ([92m↓5.16%[0m) [28.38% of initial]
[Iter 130/20000] Loss: 0.0667052 (Best: 0.0641990 @iter130) ([92m↓6.62%[0m) [26.50% of initial]
[Iter 140/20000] Loss: 0.0635378 (Best: 0.0612905 @iter140) ([92m↓4.75%[0m) [25.24% of initial]
[Iter 150/20000] Loss: 0.0612754 (Best: 0.0583912 @iter148) ([92m↓3.56%[0m) [24.34% of initial]
[Iter 160/20000] Loss: 0.0590545 (Best: 0.0559516 @iter157) ([92m↓3.62%[0m) [23.46% of initial]
[Iter 170/20000] Loss: 0.0563547 (Best: 0.0535233 @iter167) ([92m↓4.57%[0m) [22.39% of initial]
[Iter 180/20000] Loss: 0.0523423 (Best: 0.0500117 @iter179) ([92m↓7.12%[0m) [20.80% of initial]
[Iter 190/20000] Loss: 0.0495550 (Best: 0.0478360 @iter188) ([92m↓5.33%[0m) [19.69% of initial]
Iter:199, L1 loss=0.0345, Total loss=0.04972, Time:15
[Iter 200/20000] Loss: 0.0478118 (Best: 0.0456420 @iter198) ([92m↓3.52%[0m) [19.00% of initial]
[Iter 210/20000] Loss: 0.0450592 (Best: 0.0428094 @iter209) ([92m↓5.76%[0m) [17.90% of initial]
[Iter 220/20000] Loss: 0.0440960 (Best: 0.0412433 @iter219) ([92m↓2.14%[0m) [17.52% of initial]
[Iter 230/20000] Loss: 0.0423347 (Best: 0.0399214 @iter227) ([92m↓3.99%[0m) [16.82% of initial]
[Iter 240/20000] Loss: 0.0402544 (Best: 0.0377645 @iter238) ([92m↓4.91%[0m) [15.99% of initial]
[Iter 250/20000] Loss: 0.0378963 (Best: 0.0362103 @iter248) ([92m↓5.86%[0m) [15.06% of initial]
[Iter 260/20000] Loss: 0.0358194 (Best: 0.0343163 @iter260) ([92m↓5.48%[0m) [14.23% of initial]
[Iter 270/20000] Loss: 0.0349925 (Best: 0.0328234 @iter269) ([92m↓2.31%[0m) [13.90% of initial]
[Iter 280/20000] Loss: 0.0346987 (Best: 0.0318360 @iter277) ([92m↓0.84%[0m) [13.79% of initial]
[Iter 290/20000] Loss: 0.0329342 (Best: 0.0303293 @iter287) ([92m↓5.09%[0m) [13.08% of initial]
Iter:299, L1 loss=0.02213, Total loss=0.03328, Time:14
[Iter 300/20000] Loss: 0.0307077 (Best: 0.0288353 @iter300) ([92m↓6.76%[0m) [12.20% of initial]
[Iter 310/20000] Loss: 0.0293528 (Best: 0.0274127 @iter310) ([92m↓4.41%[0m) [11.66% of initial]
[Iter 320/20000] Loss: 0.0278957 (Best: 0.0264356 @iter320) ([92m↓4.96%[0m) [11.08% of initial]
[Iter 330/20000] Loss: 0.0274280 (Best: 0.0255223 @iter330) ([92m↓1.68%[0m) [10.90% of initial]
[Iter 340/20000] Loss: 0.0254151 (Best: 0.0242836 @iter340) ([92m↓7.34%[0m) [10.10% of initial]
[Iter 350/20000] Loss: 0.0262118 (Best: 0.0236180 @iter349) ([91m↑3.13%[0m) [10.41% of initial]
[Iter 360/20000] Loss: 0.0247579 (Best: 0.0228129 @iter358) ([92m↓5.55%[0m) [9.84% of initial]
[Iter 370/20000] Loss: 0.0245303 (Best: 0.0220905 @iter368) ([92m↓0.92%[0m) [9.75% of initial]
[Iter 380/20000] Loss: 0.0222081 (Best: 0.0208801 @iter379) ([92m↓9.47%[0m) [8.82% of initial]
[Iter 390/20000] Loss: 0.0215881 (Best: 0.0201034 @iter385) ([92m↓2.79%[0m) [8.58% of initial]
Iter:399, L1 loss=0.0135, Total loss=0.02074, Time:14
[Iter 400/20000] Loss: 0.0201813 (Best: 0.0188199 @iter400) ([92m↓6.52%[0m) [8.02% of initial]
[Iter 410/20000] Loss: 0.0194163 (Best: 0.0184918 @iter410) ([92m↓3.79%[0m) [7.71% of initial]
[Iter 420/20000] Loss: 0.0196879 (Best: 0.0176556 @iter418) ([91m↑1.40%[0m) [7.82% of initial]
[Iter 430/20000] Loss: 0.0176803 (Best: 0.0168277 @iter430) ([92m↓10.20%[0m) [7.02% of initial]
[Iter 440/20000] Loss: 0.0179882 (Best: 0.0163067 @iter438) ([91m↑1.74%[0m) [7.15% of initial]
[Iter 450/20000] Loss: 0.0169663 (Best: 0.0150741 @iter449) ([92m↓5.68%[0m) [6.74% of initial]
[Iter 460/20000] Loss: 0.0163023 (Best: 0.0145428 @iter458) ([92m↓3.91%[0m) [6.48% of initial]
[Iter 470/20000] Loss: 0.0150412 (Best: 0.0141654 @iter470) ([92m↓7.74%[0m) [5.98% of initial]
[Iter 480/20000] Loss: 0.0145188 (Best: 0.0132563 @iter479) ([92m↓3.47%[0m) [5.77% of initial]
[Iter 490/20000] Loss: 0.0133655 (Best: 0.0123795 @iter490) ([92m↓7.94%[0m) [5.31% of initial]
Iter:499, L1 loss=0.009606, Total loss=0.01424, Time:14
[Iter 500/20000] Loss: 0.0133361 (Best: 0.0123666 @iter498) ([92m↓0.22%[0m) [5.30% of initial]
[Iter 510/20000] Loss: 0.0129957 (Best: 0.0116650 @iter508) ([92m↓2.55%[0m) [5.16% of initial]
[Iter 520/20000] Loss: 0.0125337 (Best: 0.0112668 @iter514) ([92m↓3.55%[0m) [4.98% of initial]
[Iter 530/20000] Loss: 0.0124352 (Best: 0.0112668 @iter514) ([92m↓0.79%[0m) [4.94% of initial]
[Iter 540/20000] Loss: 0.0123632 (Best: 0.0110053 @iter538) ([92m↓0.58%[0m) [4.91% of initial]
[Iter 550/20000] Loss: 0.0120421 (Best: 0.0108774 @iter548) ([92m↓2.60%[0m) [4.78% of initial]
[Iter 560/20000] Loss: 0.0121320 (Best: 0.0105929 @iter556) ([91m↑0.75%[0m) [4.82% of initial]
[Iter 570/20000] Loss: 0.0115544 (Best: 0.0103515 @iter569) ([92m↓4.76%[0m) [4.59% of initial]
[Iter 580/20000] Loss: 0.0112463 (Best: 0.0101078 @iter578) ([92m↓2.67%[0m) [4.47% of initial]
[Iter 590/20000] Loss: 0.0111812 (Best: 0.0098618 @iter583) ([92m↓0.58%[0m) [4.44% of initial]
Iter:599, L1 loss=0.006972, Total loss=0.01181, Time:14
[Iter 600/20000] Loss: 0.0108481 (Best: 0.0098265 @iter598) ([92m↓2.98%[0m) [4.31% of initial]
[Iter 610/20000] Loss: 0.0219235 (Best: 0.0098265 @iter598) ([91m↑102.10%[0m) [8.71% of initial]
[Iter 620/20000] Loss: 0.0139308 (Best: 0.0098265 @iter598) ([92m↓36.46%[0m) [5.53% of initial]
[Iter 630/20000] Loss: 0.0113820 (Best: 0.0098265 @iter598) ([92m↓18.30%[0m) [4.52% of initial]
[Iter 640/20000] Loss: 0.0097134 (Best: 0.0087348 @iter640) ([92m↓14.66%[0m) [3.86% of initial]
[Iter 650/20000] Loss: 0.0101924 (Best: 0.0087348 @iter640) ([91m↑4.93%[0m) [4.05% of initial]
[Iter 660/20000] Loss: 0.0098577 (Best: 0.0086701 @iter655) ([92m↓3.28%[0m) [3.92% of initial]
[Iter 670/20000] Loss: 0.0095230 (Best: 0.0085171 @iter662) ([92m↓3.40%[0m) [3.78% of initial]
[Iter 680/20000] Loss: 0.0087994 (Best: 0.0081828 @iter680) ([92m↓7.60%[0m) [3.50% of initial]
[Iter 690/20000] Loss: 0.0090191 (Best: 0.0078638 @iter685) ([91m↑2.50%[0m) [3.58% of initial]
Iter:699, L1 loss=0.005709, Total loss=0.009483, Time:14
[Iter 700/20000] Loss: 0.0086682 (Best: 0.0077984 @iter695) ([92m↓3.89%[0m) [3.44% of initial]
[Iter 710/20000] Loss: 0.0081832 (Best: 0.0075088 @iter703) ([92m↓5.60%[0m) [3.25% of initial]
[Iter 720/20000] Loss: 0.0083543 (Best: 0.0075088 @iter703) ([91m↑2.09%[0m) [3.32% of initial]
[Iter 730/20000] Loss: 0.0084563 (Best: 0.0072739 @iter727) ([91m↑1.22%[0m) [3.36% of initial]
[Iter 740/20000] Loss: 0.0085234 (Best: 0.0072609 @iter736) ([91m↑0.79%[0m) [3.39% of initial]
[Iter 750/20000] Original Loss: 0.0079809 Pseudo Loss: 0.0006230 (7.81% of original) (Best: 0.0070075 @iter748) ([92m↓6.36%[0m) [3.17% of initial]
[Iter 760/20000] Original Loss: 0.0075642 Pseudo Loss: 0.0006230 (8.24% of original) (Best: 0.0069672 @iter751) ([92m↓5.22%[0m) [3.01% of initial]
[Iter 770/20000] Original Loss: 0.0075694 Pseudo Loss: 0.0006230 (8.23% of original) (Best: 0.0069672 @iter751) ([91m↑0.07%[0m) [3.01% of initial]
[Iter 780/20000] Original Loss: 0.0077240 Pseudo Loss: 0.0006230 (8.07% of original) (Best: 0.0066586 @iter778) ([91m↑2.04%[0m) [3.07% of initial]
[Iter 790/20000] Original Loss: 0.0075748 Pseudo Loss: 0.0006230 (8.22% of original) (Best: 0.0065663 @iter787) ([92m↓1.93%[0m) [3.01% of initial]
Iter:799, L1 loss=0.005173, Total loss=0.008172, Time:13
[Iter 800/20000] Original Loss: 0.0073677 Pseudo Loss: 0.0009475 (12.86% of original) (Best: 0.0065663 @iter787) ([92m↓2.73%[0m) [2.93% of initial]
[Iter 810/20000] Original Loss: 0.0176260 Pseudo Loss: 0.0009475 (5.38% of original) (Best: 0.0065663 @iter787) ([91m↑139.24%[0m) [7.00% of initial]
[Iter 820/20000] Original Loss: 0.0110942 Pseudo Loss: 0.0009475 (8.54% of original) (Best: 0.0065663 @iter787) ([92m↓37.06%[0m) [4.41% of initial]
[Iter 830/20000] Original Loss: 0.0089393 Pseudo Loss: 0.0009475 (10.60% of original) (Best: 0.0065663 @iter787) ([92m↓19.42%[0m) [3.55% of initial]
[Iter 840/20000] Original Loss: 0.0080431 Pseudo Loss: 0.0009475 (11.78% of original) (Best: 0.0065663 @iter787) ([92m↓10.03%[0m) [3.20% of initial]
[Iter 850/20000] Original Loss: 0.0074344 Pseudo Loss: 0.0009475 (12.74% of original) (Best: 0.0065663 @iter787) ([92m↓7.57%[0m) [2.95% of initial]
[Iter 860/20000] Original Loss: 0.0069914 Pseudo Loss: 0.0009475 (13.55% of original) (Best: 0.0061829 @iter856) ([92m↓5.96%[0m) [2.78% of initial]
[Iter 870/20000] Original Loss: 0.0066168 Pseudo Loss: 0.0009475 (14.32% of original) (Best: 0.0060683 @iter862) ([92m↓5.36%[0m) [2.63% of initial]
[Iter 880/20000] Original Loss: 0.0066689 Pseudo Loss: 0.0009475 (14.21% of original) (Best: 0.0058796 @iter875) ([91m↑0.79%[0m) [2.65% of initial]
[Iter 890/20000] Original Loss: 0.0063531 Pseudo Loss: 0.0009475 (14.91% of original) (Best: 0.0057174 @iter884) ([92m↓4.73%[0m) [2.52% of initial]
Iter:899, L1 loss=0.003843, Total loss=0.005669, Time:13
[Iter 900/20000] Original Loss: 0.0064935 Pseudo Loss: 0.0009475 (14.59% of original) (Best: 0.0056691 @iter899) ([91m↑2.21%[0m) [2.58% of initial]
[Iter 910/20000] Original Loss: 0.0064959 Pseudo Loss: 0.0009475 (14.59% of original) (Best: 0.0054179 @iter907) ([91m↑0.04%[0m) [2.58% of initial]
[Iter 920/20000] Original Loss: 0.0059476 Pseudo Loss: 0.0009475 (15.93% of original) (Best: 0.0053485 @iter919) ([92m↓8.44%[0m) [2.36% of initial]
[Iter 930/20000] Original Loss: 0.0062029 Pseudo Loss: 0.0009475 (15.27% of original) (Best: 0.0052557 @iter925) ([91m↑4.29%[0m) [2.46% of initial]
[Iter 940/20000] Original Loss: 0.0062181 Pseudo Loss: 0.0009475 (15.24% of original) (Best: 0.0051307 @iter938) ([91m↑0.24%[0m) [2.47% of initial]
[Iter 950/20000] Original Loss: 0.0057619 Pseudo Loss: 0.0009475 (16.44% of original) (Best: 0.0051307 @iter938) ([92m↓7.34%[0m) [2.29% of initial]
[Iter 960/20000] Original Loss: 0.0059027 Pseudo Loss: 0.0009475 (16.05% of original) (Best: 0.0051307 @iter938) ([91m↑2.44%[0m) [2.35% of initial]
[Iter 970/20000] Original Loss: 0.0058900 Pseudo Loss: 0.0009475 (16.09% of original) (Best: 0.0051088 @iter964) ([92m↓0.21%[0m) [2.34% of initial]
[Iter 980/20000] Original Loss: 0.0059316 Pseudo Loss: 0.0009475 (15.97% of original) (Best: 0.0051088 @iter964) ([91m↑0.71%[0m) [2.36% of initial]
[Iter 990/20000] Original Loss: 0.0060478 Pseudo Loss: 0.0009475 (15.67% of original) (Best: 0.0051088 @iter964) ([91m↑1.96%[0m) [2.40% of initial]
Iter:999, L1 loss=0.004458, Total loss=0.006571, Time:13
[Iter 1000/20000] Original Loss: 0.0062016 Pseudo Loss: 0.0009475 (15.28% of original) (Best: 0.0051088 @iter964) ([91m↑2.54%[0m) [2.46% of initial]
[Iter 1010/20000] Original Loss: 0.0119825 Pseudo Loss: 0.0009475 (7.91% of original) (Best: 0.0051088 @iter964) ([91m↑93.22%[0m) [4.76% of initial]
[Iter 1020/20000] Original Loss: 0.0082529 Pseudo Loss: 0.0009475 (11.48% of original) (Best: 0.0051088 @iter964) ([92m↓31.13%[0m) [3.28% of initial]
[Iter 1030/20000] Original Loss: 0.0066951 Pseudo Loss: 0.0009475 (14.15% of original) (Best: 0.0051088 @iter964) ([92m↓18.87%[0m) [2.66% of initial]
[Iter 1040/20000] Original Loss: 0.0058962 Pseudo Loss: 0.0009475 (16.07% of original) (Best: 0.0051088 @iter964) ([92m↓11.93%[0m) [2.34% of initial]
[Iter 1050/20000] Original Loss: 0.0057196 Pseudo Loss: 0.0009475 (16.57% of original) (Best: 0.0049610 @iter1049) ([92m↓2.99%[0m) [2.27% of initial]
[Iter 1060/20000] Original Loss: 0.0056073 Pseudo Loss: 0.0009475 (16.90% of original) (Best: 0.0047969 @iter1051) ([92m↓1.96%[0m) [2.23% of initial]
[Iter 1070/20000] Original Loss: 0.0053432 Pseudo Loss: 0.0009475 (17.73% of original) (Best: 0.0044006 @iter1066) ([92m↓4.71%[0m) [2.12% of initial]
[Iter 1080/20000] Original Loss: 0.0051923 Pseudo Loss: 0.0011391 (21.94% of original) (Best: 0.0044006 @iter1066) ([92m↓2.82%[0m) [2.06% of initial]
[Iter 1090/20000] Original Loss: 0.0049910 Pseudo Loss: 0.0011391 (22.82% of original) (Best: 0.0044006 @iter1066) ([92m↓3.88%[0m) [1.98% of initial]
Iter:1099, L1 loss=0.003416, Total loss=0.004976, Time:14
[Iter 1100/20000] Original Loss: 0.0048783 Pseudo Loss: 0.0011391 (23.35% of original) (Best: 0.0041829 @iter1093) ([92m↓2.26%[0m) [1.94% of initial]
[Iter 1110/20000] Original Loss: 0.0049788 Pseudo Loss: 0.0011391 (22.88% of original) (Best: 0.0041829 @iter1093) ([91m↑2.06%[0m) [1.98% of initial]
[Iter 1120/20000] Original Loss: 0.0049953 Pseudo Loss: 0.0011391 (22.80% of original) (Best: 0.0040554 @iter1117) ([91m↑0.33%[0m) [1.98% of initial]
[Iter 1130/20000] Original Loss: 0.0051960 Pseudo Loss: 0.0011391 (21.92% of original) (Best: 0.0040554 @iter1117) ([91m↑4.02%[0m) [2.06% of initial]
[Iter 1140/20000] Original Loss: 0.0047054 Pseudo Loss: 0.0011391 (24.21% of original) (Best: 0.0040135 @iter1135) ([92m↓9.44%[0m) [1.87% of initial]
[Iter 1150/20000] Original Loss: 0.0043500 Pseudo Loss: 0.0011391 (26.19% of original) (Best: 0.0039109 @iter1145) ([92m↓7.55%[0m) [1.73% of initial]
[Iter 1160/20000] Original Loss: 0.0049763 Pseudo Loss: 0.0011391 (22.89% of original) (Best: 0.0039109 @iter1145) ([91m↑14.40%[0m) [1.98% of initial]
[Iter 1170/20000] Original Loss: 0.0045984 Pseudo Loss: 0.0011391 (24.77% of original) (Best: 0.0039109 @iter1145) ([92m↓7.59%[0m) [1.83% of initial]
[Iter 1180/20000] Original Loss: 0.0042943 Pseudo Loss: 0.0011391 (26.53% of original) (Best: 0.0038771 @iter1180) ([92m↓6.61%[0m) [1.71% of initial]
[Iter 1190/20000] Original Loss: 0.0046151 Pseudo Loss: 0.0011391 (24.68% of original) (Best: 0.0038657 @iter1186) ([91m↑7.47%[0m) [1.83% of initial]
Iter:1199, L1 loss=0.003455, Total loss=0.004822, Time:12
[Iter 1200/20000] Original Loss: 0.0045219 Pseudo Loss: 0.0011391 (25.19% of original) (Best: 0.0037503 @iter1195) ([92m↓2.02%[0m) [1.80% of initial]
[Iter 1210/20000] Original Loss: 0.0104866 Pseudo Loss: 0.0011391 (10.86% of original) (Best: 0.0037503 @iter1195) ([91m↑131.91%[0m) [4.17% of initial]
[Iter 1220/20000] Original Loss: 0.0070417 Pseudo Loss: 0.0011391 (16.18% of original) (Best: 0.0037503 @iter1195) ([92m↓32.85%[0m) [2.80% of initial]
[Iter 1230/20000] Original Loss: 0.0059679 Pseudo Loss: 0.0011391 (19.09% of original) (Best: 0.0037503 @iter1195) ([92m↓15.25%[0m) [2.37% of initial]
[Iter 1240/20000] Original Loss: 0.0053647 Pseudo Loss: 0.0011391 (21.23% of original) (Best: 0.0037503 @iter1195) ([92m↓10.11%[0m) [2.13% of initial]
[Iter 1250/20000] Original Loss: 0.0047608 Pseudo Loss: 0.0011391 (23.93% of original) (Best: 0.0037503 @iter1195) ([92m↓11.26%[0m) [1.89% of initial]
[Iter 1260/20000] Original Loss: 0.0045834 Pseudo Loss: 0.0011391 (24.85% of original) (Best: 0.0036406 @iter1258) ([92m↓3.73%[0m) [1.82% of initial]
[Iter 1270/20000] Original Loss: 0.0040941 Pseudo Loss: 0.0011391 (27.82% of original) (Best: 0.0036406 @iter1258) ([92m↓10.67%[0m) [1.63% of initial]
[Iter 1280/20000] Original Loss: 0.0042822 Pseudo Loss: 0.0011391 (26.60% of original) (Best: 0.0034541 @iter1273) ([91m↑4.59%[0m) [1.70% of initial]
[Iter 1290/20000] Original Loss: 0.0041553 Pseudo Loss: 0.0011391 (27.41% of original) (Best: 0.0033003 @iter1285) ([92m↓2.96%[0m) [1.65% of initial]
Iter:1299, L1 loss=0.002749, Total loss=0.003566, Time:15
[Iter 1300/20000] Original Loss: 0.0039019 Pseudo Loss: 0.0011391 (29.19% of original) (Best: 0.0033003 @iter1285) ([92m↓6.10%[0m) [1.55% of initial]
[Iter 1310/20000] Original Loss: 0.0039406 Pseudo Loss: 0.0011391 (28.91% of original) (Best: 0.0033003 @iter1285) ([91m↑0.99%[0m) [1.57% of initial]
[Iter 1320/20000] Original Loss: 0.0038208 Pseudo Loss: 0.0011391 (29.81% of original) (Best: 0.0030676 @iter1319) ([92m↓3.04%[0m) [1.52% of initial]
[Iter 1330/20000] Original Loss: 0.0038777 Pseudo Loss: 0.0011391 (29.37% of original) (Best: 0.0030050 @iter1321) ([91m↑1.49%[0m) [1.54% of initial]
[Iter 1340/20000] Original Loss: 0.0036198 Pseudo Loss: 0.0011391 (31.47% of original) (Best: 0.0030050 @iter1321) ([92m↓6.65%[0m) [1.44% of initial]
[Iter 1350/20000] Original Loss: 0.0036390 Pseudo Loss: 0.0011391 (31.30% of original) (Best: 0.0030050 @iter1321) ([91m↑0.53%[0m) [1.45% of initial]
[Iter 1360/20000] Original Loss: 0.0037657 Pseudo Loss: 0.0011391 (30.25% of original) (Best: 0.0030050 @iter1321) ([91m↑3.48%[0m) [1.50% of initial]
[Iter 1370/20000] Original Loss: 0.0036094 Pseudo Loss: 0.0011391 (31.56% of original) (Best: 0.0030050 @iter1321) ([92m↓4.15%[0m) [1.43% of initial]
[Iter 1380/20000] Original Loss: 0.0039169 Pseudo Loss: 0.0011391 (29.08% of original) (Best: 0.0030050 @iter1321) ([91m↑8.52%[0m) [1.56% of initial]
[Iter 1390/20000] Original Loss: 0.0036564 Pseudo Loss: 0.0011391 (31.15% of original) (Best: 0.0030050 @iter1321) ([92m↓6.65%[0m) [1.45% of initial]
Iter:1399, L1 loss=0.002306, Total loss=0.002896, Time:13
[Iter 1400/20000] Original Loss: 0.0033707 Pseudo Loss: 0.0011391 (33.79% of original) (Best: 0.0028955 @iter1399) ([92m↓7.81%[0m) [1.34% of initial]
[Iter 1410/20000] Original Loss: 0.0086703 Pseudo Loss: 0.0011391 (13.14% of original) (Best: 0.0028955 @iter1399) ([91m↑157.22%[0m) [3.44% of initial]
[Iter 1420/20000] Original Loss: 0.0057129 Pseudo Loss: 0.0011391 (19.94% of original) (Best: 0.0028955 @iter1399) ([92m↓34.11%[0m) [2.27% of initial]
[Iter 1430/20000] Original Loss: 0.0048587 Pseudo Loss: 0.0011391 (23.44% of original) (Best: 0.0028955 @iter1399) ([92m↓14.95%[0m) [1.93% of initial]
[Iter 1440/20000] Original Loss: 0.0043162 Pseudo Loss: 0.0011391 (26.39% of original) (Best: 0.0028955 @iter1399) ([92m↓11.16%[0m) [1.71% of initial]
[Iter 1450/20000] Original Loss: 0.0035330 Pseudo Loss: 0.0011391 (32.24% of original) (Best: 0.0028955 @iter1399) ([92m↓18.14%[0m) [1.40% of initial]
[Iter 1460/20000] Original Loss: 0.0035091 Pseudo Loss: 0.0011391 (32.46% of original) (Best: 0.0028955 @iter1399) ([92m↓0.68%[0m) [1.39% of initial]
[Iter 1470/20000] Original Loss: 0.0033763 Pseudo Loss: 0.0011391 (33.74% of original) (Best: 0.0028955 @iter1399) ([92m↓3.78%[0m) [1.34% of initial]
[Iter 1480/20000] Original Loss: 0.0032763 Pseudo Loss: 0.0011391 (34.77% of original) (Best: 0.0027453 @iter1480) ([92m↓2.96%[0m) [1.30% of initial]
[Iter 1490/20000] Original Loss: 0.0031771 Pseudo Loss: 0.0011391 (35.85% of original) (Best: 0.0027453 @iter1480) ([92m↓3.03%[0m) [1.26% of initial]
Iter:1499, L1 loss=0.002584, Total loss=0.003304, Time:14
[Iter 1500/20000] Original Loss: 0.0031552 Pseudo Loss: 0.0011391 (36.10% of original) (Best: 0.0027453 @iter1480) ([92m↓0.69%[0m) [1.25% of initial]
[Iter 1510/20000] Original Loss: 0.0030085 Pseudo Loss: 0.0011391 (37.86% of original) (Best: 0.0025772 @iter1504) ([92m↓4.65%[0m) [1.20% of initial]
[Iter 1520/20000] Original Loss: 0.0030063 Pseudo Loss: 0.0011391 (37.89% of original) (Best: 0.0025659 @iter1520) ([92m↓0.07%[0m) [1.19% of initial]
[Iter 1530/20000] Original Loss: 0.0031075 Pseudo Loss: 0.0011391 (36.66% of original) (Best: 0.0025559 @iter1526) ([91m↑3.37%[0m) [1.23% of initial]
[Iter 1540/20000] Original Loss: 0.0030107 Pseudo Loss: 0.0011391 (37.83% of original) (Best: 0.0025559 @iter1526) ([92m↓3.12%[0m) [1.20% of initial]
[Iter 1550/20000] Original Loss: 0.0029342 Pseudo Loss: 0.0011391 (38.82% of original) (Best: 0.0025559 @iter1526) ([92m↓2.54%[0m) [1.17% of initial]
[Iter 1560/20000] Original Loss: 0.0031681 Pseudo Loss: 0.0011391 (35.95% of original) (Best: 0.0024920 @iter1558) ([91m↑7.97%[0m) [1.26% of initial]
[Iter 1570/20000] Original Loss: 0.0027489 Pseudo Loss: 0.0011391 (41.44% of original) (Best: 0.0024230 @iter1569) ([92m↓13.23%[0m) [1.09% of initial]
[Iter 1580/20000] Original Loss: 0.0027600 Pseudo Loss: 0.0011391 (41.27% of original) (Best: 0.0022971 @iter1573) ([91m↑0.40%[0m) [1.10% of initial]
[Iter 1590/20000] Original Loss: 0.0026558 Pseudo Loss: 0.0011391 (42.89% of original) (Best: 0.0022971 @iter1573) ([92m↓3.78%[0m) [1.06% of initial]
Iter:1599, L1 loss=0.002683, Total loss=0.003327, Time:13
[Iter 1600/20000] Original Loss: 0.0030132 Pseudo Loss: 0.0011391 (37.80% of original) (Best: 0.0022443 @iter1591) ([91m↑13.46%[0m) [1.20% of initial]
[Iter 1610/20000] Original Loss: 0.0082661 Pseudo Loss: 0.0011391 (13.78% of original) (Best: 0.0022443 @iter1591) ([91m↑174.33%[0m) [3.28% of initial]
[Iter 1620/20000] Original Loss: 0.0052745 Pseudo Loss: 0.0011391 (21.60% of original) (Best: 0.0022443 @iter1591) ([92m↓36.19%[0m) [2.10% of initial]
[Iter 1630/20000] Original Loss: 0.0039519 Pseudo Loss: 0.0021582 (54.61% of original) (Best: 0.0022443 @iter1591) ([92m↓25.07%[0m) [1.57% of initial]
[Iter 1640/20000] Original Loss: 0.0037665 Pseudo Loss: 0.0021582 (57.30% of original) (Best: 0.0022443 @iter1591) ([92m↓4.69%[0m) [1.50% of initial]
[Iter 1650/20000] Original Loss: 0.0032976 Pseudo Loss: 0.0021582 (65.45% of original) (Best: 0.0022443 @iter1591) ([92m↓12.45%[0m) [1.31% of initial]
[Iter 1660/20000] Original Loss: 0.0028874 Pseudo Loss: 0.0021582 (74.75% of original) (Best: 0.0022443 @iter1591) ([92m↓12.44%[0m) [1.15% of initial]
[Iter 1670/20000] Original Loss: 0.0027063 Pseudo Loss: 0.0021582 (79.75% of original) (Best: 0.0022443 @iter1591) ([92m↓6.27%[0m) [1.08% of initial]
[Iter 1680/20000] Original Loss: 0.0029185 Pseudo Loss: 0.0021582 (73.95% of original) (Best: 0.0022443 @iter1591) ([91m↑7.84%[0m) [1.16% of initial]
[Iter 1690/20000] Original Loss: 0.0030863 Pseudo Loss: 0.0021582 (69.93% of original) (Best: 0.0022443 @iter1591) ([91m↑5.75%[0m) [1.23% of initial]
Iter:1699, L1 loss=0.002653, Total loss=0.00314, Time:14
[Iter 1700/20000] Original Loss: 0.0027895 Pseudo Loss: 0.0021582 (77.37% of original) (Best: 0.0022443 @iter1591) ([92m↓9.62%[0m) [1.11% of initial]
[Iter 1710/20000] Original Loss: 0.0031226 Pseudo Loss: 0.0021582 (69.12% of original) (Best: 0.0022443 @iter1591) ([91m↑11.94%[0m) [1.24% of initial]
[Iter 1720/20000] Original Loss: 0.0025835 Pseudo Loss: 0.0021582 (83.54% of original) (Best: 0.0022443 @iter1591) ([92m↓17.26%[0m) [1.03% of initial]
[Iter 1730/20000] Original Loss: 0.0025578 Pseudo Loss: 0.0021582 (84.38% of original) (Best: 0.0022443 @iter1591) ([92m↓1.00%[0m) [1.02% of initial]
[Iter 1740/20000] Original Loss: 0.0025782 Pseudo Loss: 0.0021582 (83.71% of original) (Best: 0.0021903 @iter1738) ([91m↑0.80%[0m) [1.02% of initial]
[Iter 1750/20000] Original Loss: 0.0023107 Pseudo Loss: 0.0021582 (93.40% of original) (Best: 0.0020655 @iter1750) ([92m↓10.38%[0m) [0.92% of initial]
[Iter 1760/20000] Original Loss: 0.0025915 Pseudo Loss: 0.0021582 (83.28% of original) (Best: 0.0020655 @iter1750) ([91m↑12.15%[0m) [1.03% of initial]
[Iter 1770/20000] Original Loss: 0.0023933 Pseudo Loss: 0.0021582 (90.18% of original) (Best: 0.0020548 @iter1762) ([92m↓7.65%[0m) [0.95% of initial]
[Iter 1780/20000] Original Loss: 0.0024263 Pseudo Loss: 0.0021582 (88.95% of original) (Best: 0.0020548 @iter1762) ([91m↑1.38%[0m) [0.96% of initial]
[Iter 1790/20000] Original Loss: 0.0021545 Pseudo Loss: 0.0021582 (100.17% of original) (Best: 0.0018186 @iter1789) ([92m↓11.20%[0m) [0.86% of initial]
Iter:1799, L1 loss=0.001697, Total loss=0.001901, Time:13
[Iter 1800/20000] Original Loss: 0.0021872 Pseudo Loss: 0.0021582 (98.67% of original) (Best: 0.0018186 @iter1789) ([91m↑1.52%[0m) [0.87% of initial]
[Iter 1810/20000] Original Loss: 0.0076745 Pseudo Loss: 0.0021582 (28.12% of original) (Best: 0.0018186 @iter1789) ([91m↑250.88%[0m) [3.05% of initial]
[Iter 1820/20000] Original Loss: 0.0046133 Pseudo Loss: 0.0021582 (46.78% of original) (Best: 0.0018186 @iter1789) ([92m↓39.89%[0m) [1.83% of initial]
[Iter 1830/20000] Original Loss: 0.0040230 Pseudo Loss: 0.0021582 (53.65% of original) (Best: 0.0018186 @iter1789) ([92m↓12.80%[0m) [1.60% of initial]
[Iter 1840/20000] Original Loss: 0.0027763 Pseudo Loss: 0.0021582 (77.74% of original) (Best: 0.0018186 @iter1789) ([92m↓30.99%[0m) [1.10% of initial]
[Iter 1850/20000] Original Loss: 0.0025899 Pseudo Loss: 0.0021582 (83.33% of original) (Best: 0.0018186 @iter1789) ([92m↓6.71%[0m) [1.03% of initial]
[Iter 1860/20000] Original Loss: 0.0023493 Pseudo Loss: 0.0021582 (91.87% of original) (Best: 0.0018186 @iter1789) ([92m↓9.29%[0m) [0.93% of initial]
[Iter 1870/20000] Original Loss: 0.0021850 Pseudo Loss: 0.0021582 (98.78% of original) (Best: 0.0018151 @iter1867) ([92m↓6.99%[0m) [0.87% of initial]
[Iter 1880/20000] Original Loss: 0.0020894 Pseudo Loss: 0.0021582 (103.30% of original) (Best: 0.0018151 @iter1867) ([92m↓4.38%[0m) [0.83% of initial]
[Iter 1890/20000] Original Loss: 0.0019199 Pseudo Loss: 0.0021582 (112.41% of original) (Best: 0.0017793 @iter1887) ([92m↓8.11%[0m) [0.76% of initial]
Iter:1899, L1 loss=0.001769, Total loss=0.002015, Time:14
[Iter 1900/20000] Original Loss: 0.0020387 Pseudo Loss: 0.0021582 (105.86% of original) (Best: 0.0016294 @iter1891) ([91m↑6.19%[0m) [0.81% of initial]
[Iter 1910/20000] Original Loss: 0.0020801 Pseudo Loss: 0.0021582 (103.76% of original) (Best: 0.0016294 @iter1891) ([91m↑2.03%[0m) [0.83% of initial]
[Iter 1920/20000] Original Loss: 0.0020956 Pseudo Loss: 0.0021582 (102.99% of original) (Best: 0.0016294 @iter1891) ([91m↑0.74%[0m) [0.83% of initial]
[Iter 1930/20000] Original Loss: 0.0017826 Pseudo Loss: 0.0021582 (121.07% of original) (Best: 0.0015834 @iter1930) ([92m↓14.93%[0m) [0.71% of initial]
[Iter 1940/20000] Original Loss: 0.0018688 Pseudo Loss: 0.0021582 (115.49% of original) (Best: 0.0015336 @iter1939) ([91m↑4.84%[0m) [0.74% of initial]
[Iter 1950/20000] Original Loss: 0.0020481 Pseudo Loss: 0.0021582 (105.38% of original) (Best: 0.0015336 @iter1939) ([91m↑9.59%[0m) [0.81% of initial]
[Iter 1960/20000] Original Loss: 0.0018784 Pseudo Loss: 0.0021582 (114.90% of original) (Best: 0.0015336 @iter1939) ([92m↓8.29%[0m) [0.75% of initial]
[Iter 1970/20000] Original Loss: 0.0017125 Pseudo Loss: 0.0021582 (126.03% of original) (Best: 0.0015336 @iter1939) ([92m↓8.83%[0m) [0.68% of initial]
[Iter 1980/20000] Original Loss: 0.0020425 Pseudo Loss: 0.0021582 (105.67% of original) (Best: 0.0015336 @iter1939) ([91m↑19.27%[0m) [0.81% of initial]
[Iter 1990/20000] Original Loss: 0.0017953 Pseudo Loss: 0.0021582 (120.22% of original) (Best: 0.0015336 @iter1939) ([92m↓12.10%[0m) [0.71% of initial]
Iter:1999, L1 loss=0.001605, Total loss=0.001973, Time:14
[Iter 2000/20000] Original Loss: 0.0020271 Pseudo Loss: 0.0021582 (106.47% of original) (Best: 0.0015336 @iter1939) ([91m↑12.91%[0m) [0.81% of initial]
Testing Speed: 197.1400316604436 fps
Testing Time: 0.25362682342529297 s

[ITER 2000] Evaluating test: SSIM = 0.8478951263427734, PSNR = 17.5193744468689
Testing Speed: 180.88772605733016 fps
Testing Time: 0.01658487319946289 s

[ITER 2000] Evaluating train: SSIM = 0.9999496738115946, PSNR = 48.31907272338867
Iter:2000, total_points:43029
[Iter 2010/20000] Original Loss: 0.0066683 Pseudo Loss: 0.0021582 (32.37% of original) (Best: 0.0015336 @iter1939) ([91m↑228.96%[0m) [2.65% of initial]
[Iter 2020/20000] Original Loss: 0.0037955 Pseudo Loss: 0.0021582 (56.86% of original) (Best: 0.0015336 @iter1939) ([92m↓43.08%[0m) [1.51% of initial]
[Iter 2030/20000] Original Loss: 0.0028327 Pseudo Loss: 0.0021582 (76.19% of original) (Best: 0.0015336 @iter1939) ([92m↓25.37%[0m) [1.13% of initial]
[Iter 2040/20000] Original Loss: 0.0024956 Pseudo Loss: 0.0021582 (86.48% of original) (Best: 0.0015336 @iter1939) ([92m↓11.90%[0m) [0.99% of initial]
[Iter 2050/20000] Original Loss: 0.0021104 Pseudo Loss: 0.0021582 (102.27% of original) (Best: 0.0015336 @iter1939) ([92m↓15.44%[0m) [0.84% of initial]
[Iter 2060/20000] Original Loss: 0.0018543 Pseudo Loss: 0.0021582 (116.39% of original) (Best: 0.0015336 @iter1939) ([92m↓12.14%[0m) [0.74% of initial]
[Iter 2070/20000] Original Loss: 0.0020073 Pseudo Loss: 0.0021582 (107.52% of original) (Best: 0.0015336 @iter1939) ([91m↑8.25%[0m) [0.80% of initial]
[Iter 2080/20000] Original Loss: 0.0019970 Pseudo Loss: 0.0021582 (108.08% of original) (Best: 0.0015336 @iter1939) ([92m↓0.51%[0m) [0.79% of initial]
[Iter 2090/20000] Original Loss: 0.0018941 Pseudo Loss: 0.0021582 (113.95% of original) (Best: 0.0015223 @iter2087) ([92m↓5.15%[0m) [0.75% of initial]
Iter:2099, L1 loss=0.001603, Total loss=0.001741, Time:14
[Iter 2100/20000] Original Loss: 0.0017347 Pseudo Loss: 0.0021582 (124.41% of original) (Best: 0.0015223 @iter2087) ([92m↓8.41%[0m) [0.69% of initial]
[Iter 2110/20000] Original Loss: 0.0017356 Pseudo Loss: 0.0021582 (124.35% of original) (Best: 0.0014791 @iter2101) ([91m↑0.05%[0m) [0.69% of initial]
[Iter 2120/20000] Original Loss: 0.0014621 Pseudo Loss: 0.0021582 (147.62% of original) (Best: 0.0013332 @iter2120) ([92m↓15.76%[0m) [0.58% of initial]
[Iter 2130/20000] Original Loss: 0.0016182 Pseudo Loss: 0.0021582 (133.37% of original) (Best: 0.0013010 @iter2125) ([91m↑10.68%[0m) [0.64% of initial]
[Iter 2140/20000] Original Loss: 0.0017596 Pseudo Loss: 0.0021582 (122.66% of original) (Best: 0.0013010 @iter2125) ([91m↑8.74%[0m) [0.70% of initial]
[Iter 2150/20000] Original Loss: 0.0018168 Pseudo Loss: 0.0021582 (118.79% of original) (Best: 0.0013010 @iter2125) ([91m↑3.25%[0m) [0.72% of initial]
[Iter 2160/20000] Original Loss: 0.0016130 Pseudo Loss: 0.0021582 (133.80% of original) (Best: 0.0013010 @iter2125) ([92m↓11.22%[0m) [0.64% of initial]
[Iter 2170/20000] Original Loss: 0.0016499 Pseudo Loss: 0.0021582 (130.81% of original) (Best: 0.0013010 @iter2125) ([91m↑2.29%[0m) [0.66% of initial]
[Iter 2180/20000] Original Loss: 0.0013512 Pseudo Loss: 0.0021582 (159.73% of original) (Best: 0.0012242 @iter2180) ([92m↓18.10%[0m) [0.54% of initial]
[Iter 2190/20000] Original Loss: 0.0016139 Pseudo Loss: 0.0021582 (133.73% of original) (Best: 0.0012242 @iter2180) ([91m↑19.44%[0m) [0.64% of initial]
Iter:2199, L1 loss=0.001371, Total loss=0.00153, Time:14
[Iter 2200/20000] Original Loss: 0.0016833 Pseudo Loss: 0.0021582 (128.22% of original) (Best: 0.0012242 @iter2180) ([91m↑4.30%[0m) [0.67% of initial]
[Iter 2210/20000] Original Loss: 0.0075845 Pseudo Loss: 0.0021582 (28.46% of original) (Best: 0.0012242 @iter2180) ([91m↑350.58%[0m) [3.01% of initial]
[Iter 2220/20000] Original Loss: 0.0042555 Pseudo Loss: 0.0021582 (50.72% of original) (Best: 0.0012242 @iter2180) ([92m↓43.89%[0m) [1.69% of initial]
[Iter 2230/20000] Original Loss: 0.0027029 Pseudo Loss: 0.0021582 (79.85% of original) (Best: 0.0012242 @iter2180) ([92m↓36.48%[0m) [1.07% of initial]
[Iter 2240/20000] Original Loss: 0.0023134 Pseudo Loss: 0.0021582 (93.29% of original) (Best: 0.0012242 @iter2180) ([92m↓14.41%[0m) [0.92% of initial]
[Iter 2250/20000] Original Loss: 0.0021240 Pseudo Loss: 0.0021582 (101.61% of original) (Best: 0.0012242 @iter2180) ([92m↓8.19%[0m) [0.84% of initial]
[Iter 2260/20000] Original Loss: 0.0017317 Pseudo Loss: 0.0021582 (124.63% of original) (Best: 0.0012242 @iter2180) ([92m↓18.47%[0m) [0.69% of initial]
[Iter 2270/20000] Original Loss: 0.0018084 Pseudo Loss: 0.0021582 (119.35% of original) (Best: 0.0012242 @iter2180) ([91m↑4.43%[0m) [0.72% of initial]
[Iter 2280/20000] Original Loss: 0.0014682 Pseudo Loss: 0.0021582 (146.99% of original) (Best: 0.0012242 @iter2180) ([92m↓18.81%[0m) [0.58% of initial]
[Iter 2290/20000] Original Loss: 0.0014167 Pseudo Loss: 0.0021582 (152.34% of original) (Best: 0.0012177 @iter2285) ([92m↓3.51%[0m) [0.56% of initial]
Iter:2299, L1 loss=0.001299, Total loss=0.001447, Time:15
[Iter 2300/20000] Original Loss: 0.0017147 Pseudo Loss: 0.0021582 (125.87% of original) (Best: 0.0012177 @iter2285) ([91m↑21.03%[0m) [0.68% of initial]
[Iter 2310/20000] Original Loss: 0.0015584 Pseudo Loss: 0.0021582 (138.49% of original) (Best: 0.0012177 @iter2285) ([92m↓9.11%[0m) [0.62% of initial]
[Iter 2320/20000] Original Loss: 0.0013179 Pseudo Loss: 0.0021582 (163.76% of original) (Best: 0.0011661 @iter2320) ([92m↓15.43%[0m) [0.52% of initial]
[Iter 2330/20000] Original Loss: 0.0013347 Pseudo Loss: 0.0021582 (161.71% of original) (Best: 0.0011577 @iter2329) ([91m↑1.27%[0m) [0.53% of initial]
[Iter 2340/20000] Original Loss: 0.0013609 Pseudo Loss: 0.0021582 (158.59% of original) (Best: 0.0011378 @iter2338) ([91m↑1.96%[0m) [0.54% of initial]
[Iter 2350/20000] Original Loss: 0.0015297 Pseudo Loss: 0.0021582 (141.09% of original) (Best: 0.0011378 @iter2338) ([91m↑12.40%[0m) [0.61% of initial]
[Iter 2360/20000] Original Loss: 0.0013211 Pseudo Loss: 0.0021582 (163.36% of original) (Best: 0.0011006 @iter2359) ([92m↓13.63%[0m) [0.52% of initial]
[Iter 2370/20000] Original Loss: 0.0014354 Pseudo Loss: 0.0021582 (150.36% of original) (Best: 0.0011006 @iter2359) ([91m↑8.65%[0m) [0.57% of initial]
[Iter 2380/20000] Original Loss: 0.0014842 Pseudo Loss: 0.0019614 (132.16% of original) (Best: 0.0011006 @iter2359) ([91m↑3.40%[0m) [0.59% of initial]
[Iter 2390/20000] Original Loss: 0.0016105 Pseudo Loss: 0.0019614 (121.79% of original) (Best: 0.0011006 @iter2359) ([91m↑8.51%[0m) [0.64% of initial]
Iter:2399, L1 loss=0.001229, Total loss=0.001309, Time:15
[Iter 2400/20000] Original Loss: 0.0013510 Pseudo Loss: 0.0019614 (145.18% of original) (Best: 0.0011006 @iter2359) ([92m↓16.11%[0m) [0.54% of initial]
[Iter 2410/20000] Original Loss: 0.0060932 Pseudo Loss: 0.0019614 (32.19% of original) (Best: 0.0011006 @iter2359) ([91m↑351.01%[0m) [2.42% of initial]
[Iter 2420/20000] Original Loss: 0.0034902 Pseudo Loss: 0.0019614 (56.20% of original) (Best: 0.0011006 @iter2359) ([92m↓42.72%[0m) [1.39% of initial]
[Iter 2430/20000] Original Loss: 0.0025237 Pseudo Loss: 0.0019614 (77.72% of original) (Best: 0.0011006 @iter2359) ([92m↓27.69%[0m) [1.00% of initial]
[Iter 2440/20000] Original Loss: 0.0020381 Pseudo Loss: 0.0019614 (96.24% of original) (Best: 0.0011006 @iter2359) ([92m↓19.24%[0m) [0.81% of initial]
[Iter 2450/20000] Original Loss: 0.0019578 Pseudo Loss: 0.0019614 (100.19% of original) (Best: 0.0011006 @iter2359) ([92m↓3.94%[0m) [0.78% of initial]
[Iter 2460/20000] Original Loss: 0.0016858 Pseudo Loss: 0.0019614 (116.35% of original) (Best: 0.0011006 @iter2359) ([92m↓13.89%[0m) [0.67% of initial]
[Iter 2470/20000] Original Loss: 0.0016333 Pseudo Loss: 0.0019614 (120.09% of original) (Best: 0.0011006 @iter2359) ([92m↓3.12%[0m) [0.65% of initial]
[Iter 2480/20000] Original Loss: 0.0016483 Pseudo Loss: 0.0019614 (119.00% of original) (Best: 0.0011006 @iter2359) ([91m↑0.92%[0m) [0.65% of initial]
[Iter 2490/20000] Original Loss: 0.0014734 Pseudo Loss: 0.0019614 (133.13% of original) (Best: 0.0011006 @iter2359) ([92m↓10.62%[0m) [0.59% of initial]
Iter:2499, L1 loss=0.001205, Total loss=0.001214, Time:16
[Iter 2500/20000] Original Loss: 0.0013296 Pseudo Loss: 0.0019614 (147.52% of original) (Best: 0.0011006 @iter2359) ([92m↓9.76%[0m) [0.53% of initial]
[Iter 2510/20000] Original Loss: 0.0013639 Pseudo Loss: 0.0019614 (143.81% of original) (Best: 0.0010516 @iter2504) ([91m↑2.58%[0m) [0.54% of initial]
[Iter 2520/20000] Original Loss: 0.0012573 Pseudo Loss: 0.0019614 (156.01% of original) (Best: 0.0010516 @iter2504) ([92m↓7.82%[0m) [0.50% of initial]
[Iter 2530/20000] Original Loss: 0.0011183 Pseudo Loss: 0.0019614 (175.40% of original) (Best: 0.0009786 @iter2528) ([92m↓11.06%[0m) [0.44% of initial]
[Iter 2540/20000] Original Loss: 0.0012267 Pseudo Loss: 0.0019614 (159.90% of original) (Best: 0.0009786 @iter2528) ([91m↑9.69%[0m) [0.49% of initial]
[Iter 2550/20000] Original Loss: 0.0014369 Pseudo Loss: 0.0019614 (136.50% of original) (Best: 0.0009786 @iter2528) ([91m↑17.14%[0m) [0.57% of initial]
[Iter 2560/20000] Original Loss: 0.0011931 Pseudo Loss: 0.0019614 (164.39% of original) (Best: 0.0009786 @iter2528) ([92m↓16.97%[0m) [0.47% of initial]
[Iter 2570/20000] Original Loss: 0.0014066 Pseudo Loss: 0.0019614 (139.45% of original) (Best: 0.0009786 @iter2528) ([91m↑17.89%[0m) [0.56% of initial]
[Iter 2580/20000] Original Loss: 0.0012611 Pseudo Loss: 0.0019614 (155.54% of original) (Best: 0.0009781 @iter2578) ([92m↓10.34%[0m) [0.50% of initial]
[Iter 2590/20000] Original Loss: 0.0013450 Pseudo Loss: 0.0019614 (145.84% of original) (Best: 0.0009654 @iter2584) ([91m↑6.65%[0m) [0.53% of initial]
Iter:2599, L1 loss=0.00103, Total loss=0.001084, Time:16
[Iter 2600/20000] Original Loss: 0.0012225 Pseudo Loss: 0.0019614 (160.45% of original) (Best: 0.0009654 @iter2584) ([92m↓9.11%[0m) [0.49% of initial]
[Iter 2610/20000] Original Loss: 0.0062556 Pseudo Loss: 0.0019614 (31.35% of original) (Best: 0.0009654 @iter2584) ([91m↑411.72%[0m) [2.49% of initial]
[Iter 2620/20000] Original Loss: 0.0033023 Pseudo Loss: 0.0019614 (59.40% of original) (Best: 0.0009654 @iter2584) ([92m↓47.21%[0m) [1.31% of initial]
[Iter 2630/20000] Original Loss: 0.0022505 Pseudo Loss: 0.0019614 (87.16% of original) (Best: 0.0009654 @iter2584) ([92m↓31.85%[0m) [0.89% of initial]
[Iter 2640/20000] Original Loss: 0.0017598 Pseudo Loss: 0.0019614 (111.46% of original) (Best: 0.0009654 @iter2584) ([92m↓21.80%[0m) [0.70% of initial]
[Iter 2650/20000] Original Loss: 0.0014602 Pseudo Loss: 0.0019614 (134.32% of original) (Best: 0.0009654 @iter2584) ([92m↓17.02%[0m) [0.58% of initial]
[Iter 2660/20000] Original Loss: 0.0016229 Pseudo Loss: 0.0019614 (120.86% of original) (Best: 0.0009654 @iter2584) ([91m↑11.14%[0m) [0.64% of initial]
[Iter 2670/20000] Original Loss: 0.0015679 Pseudo Loss: 0.0027651 (176.36% of original) (Best: 0.0009654 @iter2584) ([92m↓3.39%[0m) [0.62% of initial]
[Iter 2680/20000] Original Loss: 0.0012234 Pseudo Loss: 0.0027651 (226.01% of original) (Best: 0.0009654 @iter2584) ([92m↓21.97%[0m) [0.49% of initial]
[Iter 2690/20000] Original Loss: 0.0011785 Pseudo Loss: 0.0027651 (234.62% of original) (Best: 0.0009654 @iter2584) ([92m↓3.67%[0m) [0.47% of initial]
Iter:2699, L1 loss=0.001128, Total loss=0.001149, Time:16
[Iter 2700/20000] Original Loss: 0.0014444 Pseudo Loss: 0.0027651 (191.43% of original) (Best: 0.0009654 @iter2584) ([91m↑22.56%[0m) [0.57% of initial]
[Iter 2710/20000] Original Loss: 0.0012418 Pseudo Loss: 0.0027651 (222.66% of original) (Best: 0.0009654 @iter2584) ([92m↓14.03%[0m) [0.49% of initial]
[Iter 2720/20000] Original Loss: 0.0011099 Pseudo Loss: 0.0027651 (249.12% of original) (Best: 0.0009654 @iter2584) ([92m↓10.62%[0m) [0.44% of initial]
[Iter 2730/20000] Original Loss: 0.0010123 Pseudo Loss: 0.0027565 (272.31% of original) (Best: 0.0008650 @iter2725) ([92m↓8.80%[0m) [0.40% of initial]
[Iter 2740/20000] Original Loss: 0.0008836 Pseudo Loss: 0.0027565 (311.97% of original) (Best: 0.0007981 @iter2740) ([92m↓12.71%[0m) [0.35% of initial]
[Iter 2750/20000] Original Loss: 0.0011270 Pseudo Loss: 0.0027565 (244.60% of original) (Best: 0.0007981 @iter2740) ([91m↑27.54%[0m) [0.45% of initial]
[Iter 2760/20000] Original Loss: 0.0012449 Pseudo Loss: 0.0027565 (221.42% of original) (Best: 0.0007981 @iter2740) ([91m↑10.47%[0m) [0.49% of initial]
[Iter 2770/20000] Original Loss: 0.0013686 Pseudo Loss: 0.0027565 (201.42% of original) (Best: 0.0007981 @iter2740) ([91m↑9.93%[0m) [0.54% of initial]
[Iter 2780/20000] Original Loss: 0.0011094 Pseudo Loss: 0.0027565 (248.47% of original) (Best: 0.0007981 @iter2740) ([92m↓18.94%[0m) [0.44% of initial]
[Iter 2790/20000] Original Loss: 0.0011580 Pseudo Loss: 0.0027565 (238.05% of original) (Best: 0.0007981 @iter2740) ([91m↑4.38%[0m) [0.46% of initial]
Iter:2799, L1 loss=0.001308, Total loss=0.001288, Time:16
[Iter 2800/20000] Original Loss: 0.0011408 Pseudo Loss: 0.0027565 (241.64% of original) (Best: 0.0007981 @iter2740) ([92m↓1.48%[0m) [0.45% of initial]
[Iter 2810/20000] Original Loss: 0.0052028 Pseudo Loss: 0.0027565 (52.98% of original) (Best: 0.0007981 @iter2740) ([91m↑356.08%[0m) [2.07% of initial]
[Iter 2820/20000] Original Loss: 0.0027497 Pseudo Loss: 0.0027565 (100.25% of original) (Best: 0.0007981 @iter2740) ([92m↓47.15%[0m) [1.09% of initial]
[Iter 2830/20000] Original Loss: 0.0017900 Pseudo Loss: 0.0027565 (153.99% of original) (Best: 0.0007981 @iter2740) ([92m↓34.90%[0m) [0.71% of initial]
[Iter 2840/20000] Original Loss: 0.0015124 Pseudo Loss: 0.0027565 (182.26% of original) (Best: 0.0007981 @iter2740) ([92m↓15.51%[0m) [0.60% of initial]
[Iter 2850/20000] Original Loss: 0.0012920 Pseudo Loss: 0.0027565 (213.35% of original) (Best: 0.0007981 @iter2740) ([92m↓14.57%[0m) [0.51% of initial]
[Iter 2860/20000] Original Loss: 0.0014040 Pseudo Loss: 0.0027565 (196.33% of original) (Best: 0.0007981 @iter2740) ([91m↑8.67%[0m) [0.56% of initial]
[Iter 2870/20000] Original Loss: 0.0011937 Pseudo Loss: 0.0027565 (230.93% of original) (Best: 0.0007981 @iter2740) ([92m↓14.98%[0m) [0.47% of initial]
[Iter 2880/20000] Original Loss: 0.0011784 Pseudo Loss: 0.0027565 (233.91% of original) (Best: 0.0007981 @iter2740) ([92m↓1.27%[0m) [0.47% of initial]
[Iter 2890/20000] Original Loss: 0.0011178 Pseudo Loss: 0.0027565 (246.59% of original) (Best: 0.0007981 @iter2740) ([92m↓5.14%[0m) [0.44% of initial]
Iter:2899, L1 loss=0.0008918, Total loss=0.0008703, Time:17
[Iter 2900/20000] Original Loss: 0.0010531 Pseudo Loss: 0.0027565 (261.75% of original) (Best: 0.0007981 @iter2740) ([92m↓5.79%[0m) [0.42% of initial]
[Iter 2910/20000] Original Loss: 0.0011174 Pseudo Loss: 0.0027565 (246.68% of original) (Best: 0.0007981 @iter2740) ([91m↑6.11%[0m) [0.44% of initial]
[Iter 2920/20000] Original Loss: 0.0011767 Pseudo Loss: 0.0027565 (234.25% of original) (Best: 0.0007981 @iter2740) ([91m↑5.31%[0m) [0.47% of initial]
[Iter 2930/20000] Original Loss: 0.0011121 Pseudo Loss: 0.0027565 (247.87% of original) (Best: 0.0007981 @iter2740) ([92m↓5.49%[0m) [0.44% of initial]
[Iter 2940/20000] Original Loss: 0.0009580 Pseudo Loss: 0.0027565 (287.74% of original) (Best: 0.0007981 @iter2740) ([92m↓13.86%[0m) [0.38% of initial]
[Iter 2950/20000] Original Loss: 0.0009534 Pseudo Loss: 0.0027565 (289.13% of original) (Best: 0.0007981 @iter2740) ([92m↓0.48%[0m) [0.38% of initial]
[Iter 2960/20000] Original Loss: 0.0010173 Pseudo Loss: 0.0027565 (270.96% of original) (Best: 0.0007981 @iter2740) ([91m↑6.71%[0m) [0.40% of initial]
[Iter 2970/20000] Original Loss: 0.0009059 Pseudo Loss: 0.0027565 (304.27% of original) (Best: 0.0007457 @iter2969) ([92m↓10.95%[0m) [0.36% of initial]
[Iter 2980/20000] Original Loss: 0.0008273 Pseudo Loss: 0.0027565 (333.19% of original) (Best: 0.0007412 @iter2977) ([92m↓8.68%[0m) [0.33% of initial]
[Iter 2990/20000] Original Loss: 0.0008748 Pseudo Loss: 0.0027565 (315.10% of original) (Best: 0.0007015 @iter2983) ([91m↑5.74%[0m) [0.35% of initial]
Iter:2999, L1 loss=0.0007107, Total loss=0.000675, Time:16
[Iter 3000/20000] Original Loss: 0.0008374 Pseudo Loss: 0.0027565 (329.16% of original) (Best: 0.0006750 @iter2999) ([92m↓4.27%[0m) [0.33% of initial]
[Iter 3010/20000] Original Loss: 0.0050904 Pseudo Loss: 0.0027565 (54.15% of original) (Best: 0.0006750 @iter2999) ([91m↑507.86%[0m) [2.02% of initial]
[Iter 3020/20000] Original Loss: 0.0028085 Pseudo Loss: 0.0027565 (98.15% of original) (Best: 0.0006750 @iter2999) ([92m↓44.83%[0m) [1.12% of initial]
[Iter 3030/20000] Original Loss: 0.0022045 Pseudo Loss: 0.0027565 (125.04% of original) (Best: 0.0006750 @iter2999) ([92m↓21.51%[0m) [0.88% of initial]
[Iter 3040/20000] Original Loss: 0.0017386 Pseudo Loss: 0.0027565 (158.55% of original) (Best: 0.0006750 @iter2999) ([92m↓21.14%[0m) [0.69% of initial]
[Iter 3050/20000] Original Loss: 0.0014633 Pseudo Loss: 0.0027565 (188.38% of original) (Best: 0.0006750 @iter2999) ([92m↓15.84%[0m) [0.58% of initial]
[Iter 3060/20000] Original Loss: 0.0013949 Pseudo Loss: 0.0027565 (197.62% of original) (Best: 0.0006750 @iter2999) ([92m↓4.68%[0m) [0.55% of initial]
[Iter 3070/20000] Original Loss: 0.0011309 Pseudo Loss: 0.0023083 (204.12% of original) (Best: 0.0006750 @iter2999) ([92m↓18.92%[0m) [0.45% of initial]
[Iter 3080/20000] Original Loss: 0.0011547 Pseudo Loss: 0.0023083 (199.90% of original) (Best: 0.0006750 @iter2999) ([91m↑2.11%[0m) [0.46% of initial]
[Iter 3090/20000] Original Loss: 0.0010851 Pseudo Loss: 0.0023083 (212.73% of original) (Best: 0.0006750 @iter2999) ([92m↓6.03%[0m) [0.43% of initial]
Iter:3099, L1 loss=0.0008991, Total loss=0.000898, Time:17
[Iter 3100/20000] Original Loss: 0.0010021 Pseudo Loss: 0.0023083 (230.36% of original) (Best: 0.0006750 @iter2999) ([92m↓7.65%[0m) [0.40% of initial]
[Iter 3110/20000] Original Loss: 0.0011305 Pseudo Loss: 0.0023083 (204.20% of original) (Best: 0.0006750 @iter2999) ([91m↑12.81%[0m) [0.45% of initial]
[Iter 3120/20000] Original Loss: 0.0010797 Pseudo Loss: 0.0023083 (213.79% of original) (Best: 0.0006750 @iter2999) ([92m↓4.49%[0m) [0.43% of initial]
[Iter 3130/20000] Original Loss: 0.0008631 Pseudo Loss: 0.0023083 (267.46% of original) (Best: 0.0006750 @iter2999) ([92m↓20.07%[0m) [0.34% of initial]
[Iter 3140/20000] Original Loss: 0.0008223 Pseudo Loss: 0.0023083 (280.72% of original) (Best: 0.0006733 @iter3139) ([92m↓4.72%[0m) [0.33% of initial]
[Iter 3150/20000] Original Loss: 0.0008538 Pseudo Loss: 0.0023083 (270.36% of original) (Best: 0.0006733 @iter3139) ([91m↑3.83%[0m) [0.34% of initial]
[Iter 3160/20000] Original Loss: 0.0008223 Pseudo Loss: 0.0023083 (280.71% of original) (Best: 0.0006733 @iter3139) ([92m↓3.69%[0m) [0.33% of initial]
[Iter 3170/20000] Original Loss: 0.0008330 Pseudo Loss: 0.0023083 (277.11% of original) (Best: 0.0006733 @iter3139) ([91m↑1.30%[0m) [0.33% of initial]
[Iter 3180/20000] Original Loss: 0.0009086 Pseudo Loss: 0.0023083 (254.05% of original) (Best: 0.0006733 @iter3139) ([91m↑9.08%[0m) [0.36% of initial]
Optimizing 
Training parameters: {'iterations': 20000, 'position_lr_init': 0.00019, 'position_lr_final': 1.9e-06, 'position_lr_delay_mult': 0.01, 'position_lr_max_steps': 20000, 'feature_lr': 0.002, 'opacity_lr': 0.008, 'scaling_lr': 0.005, 'rotation_lr': 0.001, 'percent_dense': 0.01, 'lambda_dssim': 0.2, 'densification_interval': 200, 'opacity_reset_interval': 4000, 'densify_from_iter': 500, 'densify_until_iter': 8000, 'densify_grad_threshold': 2.6e-05, 'random_background': False, 'sample_pseudo_interval': 1, 'start_sample_pseudo': 0, 'end_sample_pseudo': 30000}
Create gaussians1
GsDict.keys() is dict_keys(['gs0', 'gs1'])

Starting training...
Total iterations: 20000
==================================================
[Iter 10/20000] Loss: 0.2179127 (Best: 0.2126908 @iter10) [100.00% of initial]
[Iter 10] Gaussian 0 vs 1:
  Original Loss: 0.2126908
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.2126908 (Pseudo: 0.00%)
[Iter 10] Gaussian 1 vs 0:
  Original Loss: 0.2126908
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.2126908 (Pseudo: 0.00%)
[Iter 20/20000] Loss: 0.1746701 (Best: 0.1693032 @iter20) ([92m↓19.84%[0m) [69.39% of initial]
[Iter 20] Gaussian 0 vs 1:
  Original Loss: 0.1693032
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.1693032 (Pseudo: 0.00%)
[Iter 20] Gaussian 1 vs 0:
  Original Loss: 0.1693031
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.1693031 (Pseudo: 0.00%)
[Iter 30/20000] Loss: 0.1374904 (Best: 0.1327874 @iter30) ([92m↓21.29%[0m) [54.62% of initial]
[Iter 30] Gaussian 0 vs 1:
  Original Loss: 0.1327874
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.1327874 (Pseudo: 0.00%)
[Iter 30] Gaussian 1 vs 0:
  Original Loss: 0.1327881
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.1327881 (Pseudo: 0.00%)
[Iter 40/20000] Loss: 0.1123929 (Best: 0.1098391 @iter40) ([92m↓18.25%[0m) [44.65% of initial]
[Iter 40] Gaussian 0 vs 1:
  Original Loss: 0.1098391
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.1098391 (Pseudo: 0.00%)
[Iter 40] Gaussian 1 vs 0:
  Original Loss: 0.1098367
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.1098367 (Pseudo: 0.00%)
[Iter 50/20000] Loss: 0.0993467 (Best: 0.0965446 @iter49) ([92m↓11.61%[0m) [39.47% of initial]
[Iter 50] Gaussian 0 vs 1:
  Original Loss: 0.0994881
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0994881 (Pseudo: 0.00%)
[Iter 50] Gaussian 1 vs 0:
  Original Loss: 0.0994867
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0994867 (Pseudo: 0.00%)
[Iter 60/20000] Loss: 0.0936726 (Best: 0.0908473 @iter59) ([92m↓5.71%[0m) [37.22% of initial]
[Iter 60] Gaussian 0 vs 1:
  Original Loss: 0.0940218
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0940218 (Pseudo: 0.00%)
[Iter 60] Gaussian 1 vs 0:
  Original Loss: 0.0940245
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0940245 (Pseudo: 0.00%)
[Iter 70/20000] Loss: 0.0884486 (Best: 0.0869353 @iter70) ([92m↓5.58%[0m) [35.14% of initial]
[Iter 70] Gaussian 0 vs 1:
  Original Loss: 0.0869353
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0869353 (Pseudo: 0.00%)
[Iter 70] Gaussian 1 vs 0:
  Original Loss: 0.0869347
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0869347 (Pseudo: 0.00%)
[Iter 80/20000] Loss: 0.0851862 (Best: 0.0831028 @iter80) ([92m↓3.69%[0m) [33.84% of initial]
[Iter 80] Gaussian 0 vs 1:
  Original Loss: 0.0831028
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0831028 (Pseudo: 0.00%)
[Iter 80] Gaussian 1 vs 0:
  Original Loss: 0.0831017
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0831017 (Pseudo: 0.00%)
[Iter 90/20000] Loss: 0.0824125 (Best: 0.0801463 @iter88) ([92m↓3.26%[0m) [32.74% of initial]
[Iter 90] Gaussian 0 vs 1:
  Original Loss: 0.0823522
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0823522 (Pseudo: 0.00%)
[Iter 90] Gaussian 1 vs 0:
  Original Loss: 0.0823472
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0823472 (Pseudo: 0.00%)
Iter:99, L1 loss=0.05723, Total loss=0.07875, Time:14
[Iter 100/20000] Loss: 0.0786595 (Best: 0.0766176 @iter97) ([92m↓4.55%[0m) [31.25% of initial]
[Iter 100] Gaussian 0 vs 1:
  Original Loss: 0.0782890
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0782890 (Pseudo: 0.00%)
[Iter 100] Gaussian 1 vs 0:
  Original Loss: 0.0782995
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0782995 (Pseudo: 0.00%)
[Iter 110/20000] Loss: 0.0753305 (Best: 0.0731322 @iter106) ([92m↓4.23%[0m) [29.93% of initial]
[Iter 110] Gaussian 0 vs 1:
  Original Loss: 0.0744890
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0744890 (Pseudo: 0.00%)
[Iter 110] Gaussian 1 vs 0:
  Original Loss: 0.0744969
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0744969 (Pseudo: 0.00%)
[Iter 120/20000] Loss: 0.0714365 (Best: 0.0685671 @iter118) ([92m↓5.17%[0m) [28.38% of initial]
[Iter 120] Gaussian 0 vs 1:
  Original Loss: 0.0723054
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0723054 (Pseudo: 0.00%)
[Iter 120] Gaussian 1 vs 0:
  Original Loss: 0.0723218
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0723218 (Pseudo: 0.00%)
[Iter 130/20000] Loss: 0.0667060 (Best: 0.0642167 @iter130) ([92m↓6.62%[0m) [26.50% of initial]
[Iter 130] Gaussian 0 vs 1:
  Original Loss: 0.0642167
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0642167 (Pseudo: 0.00%)
[Iter 130] Gaussian 1 vs 0:
  Original Loss: 0.0642049
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0642049 (Pseudo: 0.00%)
[Iter 140/20000] Loss: 0.0635588 (Best: 0.0613106 @iter140) ([92m↓4.72%[0m) [25.25% of initial]
[Iter 140] Gaussian 0 vs 1:
  Original Loss: 0.0613106
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0613106 (Pseudo: 0.00%)
[Iter 140] Gaussian 1 vs 0:
  Original Loss: 0.0612862
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0612862 (Pseudo: 0.00%)
[Iter 150/20000] Loss: 0.0612936 (Best: 0.0584034 @iter148) ([92m↓3.56%[0m) [24.35% of initial]
[Iter 150] Gaussian 0 vs 1:
  Original Loss: 0.0605803
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0605803 (Pseudo: 0.00%)
[Iter 150] Gaussian 1 vs 0:
  Original Loss: 0.0605246
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0605246 (Pseudo: 0.00%)
[Iter 160/20000] Loss: 0.0590792 (Best: 0.0559436 @iter157) ([92m↓3.61%[0m) [23.47% of initial]
[Iter 160] Gaussian 0 vs 1:
  Original Loss: 0.0600219
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0600219 (Pseudo: 0.00%)
[Iter 160] Gaussian 1 vs 0:
  Original Loss: 0.0599866
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0599866 (Pseudo: 0.00%)
[Iter 170/20000] Loss: 0.0563707 (Best: 0.0535459 @iter167) ([92m↓4.58%[0m) [22.40% of initial]
[Iter 170] Gaussian 0 vs 1:
  Original Loss: 0.0574144
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0574144 (Pseudo: 0.00%)
[Iter 170] Gaussian 1 vs 0:
  Original Loss: 0.0573768
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0573768 (Pseudo: 0.00%)
[Iter 180/20000] Loss: 0.0523170 (Best: 0.0499551 @iter179) ([92m↓7.19%[0m) [20.79% of initial]
[Iter 180] Gaussian 0 vs 1:
  Original Loss: 0.0518869
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0518869 (Pseudo: 0.00%)
[Iter 180] Gaussian 1 vs 0:
  Original Loss: 0.0518797
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0518797 (Pseudo: 0.00%)
[Iter 190/20000] Loss: 0.0495569 (Best: 0.0478060 @iter188) ([92m↓5.28%[0m) [19.69% of initial]
[Iter 190] Gaussian 0 vs 1:
  Original Loss: 0.0490611
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0490611 (Pseudo: 0.00%)
[Iter 190] Gaussian 1 vs 0:
  Original Loss: 0.0490043
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0490043 (Pseudo: 0.00%)
Iter:199, L1 loss=0.03439, Total loss=0.04972, Time:13
[Iter 200/20000] Loss: 0.0478035 (Best: 0.0456395 @iter198) ([92m↓3.54%[0m) [18.99% of initial]
[Iter 200] Gaussian 0 vs 1:
  Original Loss: 0.0467061
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0467061 (Pseudo: 0.00%)
[Iter 200] Gaussian 1 vs 0:
  Original Loss: 0.0466626
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0466626 (Pseudo: 0.00%)
[Iter 210/20000] Loss: 0.0451214 (Best: 0.0429170 @iter209) ([92m↓5.61%[0m) [17.93% of initial]
[Iter 210] Gaussian 0 vs 1:
  Original Loss: 0.0449412
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0449412 (Pseudo: 0.00%)
[Iter 210] Gaussian 1 vs 0:
  Original Loss: 0.0448964
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0448964 (Pseudo: 0.00%)
[Iter 220/20000] Loss: 0.0441093 (Best: 0.0412987 @iter219) ([92m↓2.24%[0m) [17.52% of initial]
[Iter 220] Gaussian 0 vs 1:
  Original Loss: 0.0454830
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0454830 (Pseudo: 0.00%)
[Iter 220] Gaussian 1 vs 0:
  Original Loss: 0.0454687
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0454687 (Pseudo: 0.00%)
[Iter 230/20000] Loss: 0.0423580 (Best: 0.0399695 @iter227) ([92m↓3.97%[0m) [16.83% of initial]
[Iter 230] Gaussian 0 vs 1:
  Original Loss: 0.0410731
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0410731 (Pseudo: 0.00%)
[Iter 230] Gaussian 1 vs 0:
  Original Loss: 0.0410271
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0410271 (Pseudo: 0.00%)
[Iter 240/20000] Loss: 0.0403109 (Best: 0.0378554 @iter238) ([92m↓4.83%[0m) [16.02% of initial]
[Iter 240] Gaussian 0 vs 1:
  Original Loss: 0.0394540
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0394540 (Pseudo: 0.00%)
[Iter 240] Gaussian 1 vs 0:
  Original Loss: 0.0393807
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0393807 (Pseudo: 0.00%)
[Iter 250/20000] Loss: 0.0380480 (Best: 0.0364345 @iter248) ([92m↓5.61%[0m) [15.12% of initial]
[Iter 250] Gaussian 0 vs 1:
  Original Loss: 0.0376637
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0376637 (Pseudo: 0.00%)
[Iter 250] Gaussian 1 vs 0:
  Original Loss: 0.0375876
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0375876 (Pseudo: 0.00%)
[Iter 260/20000] Loss: 0.0359541 (Best: 0.0344145 @iter260) ([92m↓5.50%[0m) [14.28% of initial]
[Iter 260] Gaussian 0 vs 1:
  Original Loss: 0.0344145
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0344145 (Pseudo: 0.00%)
[Iter 260] Gaussian 1 vs 0:
  Original Loss: 0.0343649
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0343649 (Pseudo: 0.00%)
[Iter 270/20000] Loss: 0.0351133 (Best: 0.0329082 @iter269) ([92m↓2.34%[0m) [13.95% of initial]
[Iter 270] Gaussian 0 vs 1:
  Original Loss: 0.0349457
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0349457 (Pseudo: 0.00%)
[Iter 270] Gaussian 1 vs 0:
  Original Loss: 0.0346965
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0346965 (Pseudo: 0.00%)
[Iter 280/20000] Loss: 0.0348929 (Best: 0.0320895 @iter277) ([92m↓0.63%[0m) [13.86% of initial]
[Iter 280] Gaussian 0 vs 1:
  Original Loss: 0.0360350
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0360350 (Pseudo: 0.00%)
[Iter 280] Gaussian 1 vs 0:
  Original Loss: 0.0358975
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0358975 (Pseudo: 0.00%)
[Iter 290/20000] Loss: 0.0332877 (Best: 0.0307509 @iter287) ([92m↓4.60%[0m) [13.22% of initial]
[Iter 290] Gaussian 0 vs 1:
  Original Loss: 0.0321821
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0321821 (Pseudo: 0.00%)
[Iter 290] Gaussian 1 vs 0:
  Original Loss: 0.0320641
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0320641 (Pseudo: 0.00%)
Iter:299, L1 loss=0.02223, Total loss=0.03345, Time:13
[Iter 300/20000] Loss: 0.0308694 (Best: 0.0289517 @iter300) ([92m↓7.26%[0m) [12.26% of initial]
[Iter 300] Gaussian 0 vs 1:
  Original Loss: 0.0289517
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0289517 (Pseudo: 0.00%)
[Iter 300] Gaussian 1 vs 0:
  Original Loss: 0.0288728
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0288728 (Pseudo: 0.00%)
[Iter 310/20000] Loss: 0.0294541 (Best: 0.0275258 @iter310) ([92m↓4.58%[0m) [11.70% of initial]
[Iter 310] Gaussian 0 vs 1:
  Original Loss: 0.0275258
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0275258 (Pseudo: 0.00%)
[Iter 310] Gaussian 1 vs 0:
  Original Loss: 0.0274410
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0274410 (Pseudo: 0.00%)
[Iter 320/20000] Loss: 0.0281077 (Best: 0.0268069 @iter320) ([92m↓4.57%[0m) [11.17% of initial]
[Iter 320] Gaussian 0 vs 1:
  Original Loss: 0.0268069
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0268069 (Pseudo: 0.00%)
[Iter 320] Gaussian 1 vs 0:
  Original Loss: 0.0264823
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0264823 (Pseudo: 0.00%)
[Iter 330/20000] Loss: 0.0273802 (Best: 0.0254291 @iter330) ([92m↓2.59%[0m) [10.88% of initial]
[Iter 330] Gaussian 0 vs 1:
  Original Loss: 0.0254291
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0254291 (Pseudo: 0.00%)
[Iter 330] Gaussian 1 vs 0:
  Original Loss: 0.0255578
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0255578 (Pseudo: 0.00%)
[Iter 340/20000] Loss: 0.0252536 (Best: 0.0241521 @iter340) ([92m↓7.77%[0m) [10.03% of initial]
[Iter 340] Gaussian 0 vs 1:
  Original Loss: 0.0241521
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0241521 (Pseudo: 0.00%)
[Iter 340] Gaussian 1 vs 0:
  Original Loss: 0.0240770
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0240770 (Pseudo: 0.00%)
[Iter 350/20000] Loss: 0.0257944 (Best: 0.0230991 @iter349) ([91m↑2.14%[0m) [10.25% of initial]
[Iter 350] Gaussian 0 vs 1:
  Original Loss: 0.0271488
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0271488 (Pseudo: 0.00%)
[Iter 350] Gaussian 1 vs 0:
  Original Loss: 0.0272299
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0272299 (Pseudo: 0.00%)
[Iter 360/20000] Loss: 0.0242940 (Best: 0.0222765 @iter358) ([92m↓5.82%[0m) [9.65% of initial]
[Iter 360] Gaussian 0 vs 1:
  Original Loss: 0.0238383
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0238383 (Pseudo: 0.00%)
[Iter 360] Gaussian 1 vs 0:
  Original Loss: 0.0239671
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0239671 (Pseudo: 0.00%)
[Iter 370/20000] Loss: 0.0241734 (Best: 0.0218732 @iter361) ([92m↓0.50%[0m) [9.60% of initial]
[Iter 370] Gaussian 0 vs 1:
  Original Loss: 0.0253446
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0253446 (Pseudo: 0.00%)
[Iter 370] Gaussian 1 vs 0:
  Original Loss: 0.0250924
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0250924 (Pseudo: 0.00%)
[Iter 380/20000] Loss: 0.0217383 (Best: 0.0206792 @iter379) ([92m↓10.07%[0m) [8.64% of initial]
[Iter 380] Gaussian 0 vs 1:
  Original Loss: 0.0220522
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0220522 (Pseudo: 0.00%)
[Iter 380] Gaussian 1 vs 0:
  Original Loss: 0.0221860
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0221860 (Pseudo: 0.00%)
[Iter 390/20000] Loss: 0.0213690 (Best: 0.0199424 @iter390) ([92m↓1.70%[0m) [8.49% of initial]
[Iter 390] Gaussian 0 vs 1:
  Original Loss: 0.0199424
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0199424 (Pseudo: 0.00%)
[Iter 390] Gaussian 1 vs 0:
  Original Loss: 0.0200245
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0200245 (Pseudo: 0.00%)
Iter:399, L1 loss=0.01349, Total loss=0.02106, Time:13
[Iter 400/20000] Loss: 0.0203755 (Best: 0.0189369 @iter400) ([92m↓4.65%[0m) [8.09% of initial]
[Iter 400] Gaussian 0 vs 1:
  Original Loss: 0.0189369
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0189369 (Pseudo: 0.00%)
[Iter 400] Gaussian 1 vs 0:
  Original Loss: 0.0189504
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0189504 (Pseudo: 0.00%)
[Iter 410/20000] Loss: 0.0192299 (Best: 0.0182271 @iter410) ([92m↓5.62%[0m) [7.64% of initial]
[Iter 410] Gaussian 0 vs 1:
  Original Loss: 0.0182271
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0182271 (Pseudo: 0.00%)
[Iter 410] Gaussian 1 vs 0:
  Original Loss: 0.0184672
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0184672 (Pseudo: 0.00%)
[Iter 420/20000] Loss: 0.0196615 (Best: 0.0175988 @iter418) ([91m↑2.24%[0m) [7.81% of initial]
[Iter 420] Gaussian 0 vs 1:
  Original Loss: 0.0208778
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0208778 (Pseudo: 0.00%)
[Iter 420] Gaussian 1 vs 0:
  Original Loss: 0.0213819
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0213819 (Pseudo: 0.00%)
[Iter 430/20000] Loss: 0.0175741 (Best: 0.0166156 @iter430) ([92m↓10.62%[0m) [6.98% of initial]
[Iter 430] Gaussian 0 vs 1:
  Original Loss: 0.0166156
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0166156 (Pseudo: 0.00%)
[Iter 430] Gaussian 1 vs 0:
  Original Loss: 0.0168131
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0168131 (Pseudo: 0.00%)
[Iter 440/20000] Loss: 0.0178229 (Best: 0.0162050 @iter438) ([91m↑1.42%[0m) [7.08% of initial]
[Iter 440] Gaussian 0 vs 1:
  Original Loss: 0.0185417
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0185417 (Pseudo: 0.00%)
[Iter 440] Gaussian 1 vs 0:
  Original Loss: 0.0190324
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0190324 (Pseudo: 0.00%)
[Iter 450/20000] Loss: 0.0168776 (Best: 0.0151192 @iter449) ([92m↓5.30%[0m) [6.71% of initial]
[Iter 450] Gaussian 0 vs 1:
  Original Loss: 0.0179629
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0179629 (Pseudo: 0.00%)
[Iter 450] Gaussian 1 vs 0:
  Original Loss: 0.0183206
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0183206 (Pseudo: 0.00%)
[Iter 460/20000] Loss: 0.0169664 (Best: 0.0149961 @iter458) ([91m↑0.53%[0m) [6.74% of initial]
[Iter 460] Gaussian 0 vs 1:
  Original Loss: 0.0176798
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0176798 (Pseudo: 0.00%)
[Iter 460] Gaussian 1 vs 0:
  Original Loss: 0.0169604
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0169604 (Pseudo: 0.00%)
[Iter 470/20000] Loss: 0.0151849 (Best: 0.0141054 @iter470) ([92m↓10.50%[0m) [6.03% of initial]
[Iter 470] Gaussian 0 vs 1:
  Original Loss: 0.0141054
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0141054 (Pseudo: 0.00%)
[Iter 470] Gaussian 1 vs 0:
  Original Loss: 0.0141022
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0141022 (Pseudo: 0.00%)
[Iter 480/20000] Loss: 0.0147504 (Best: 0.0131768 @iter479) ([92m↓2.86%[0m) [5.86% of initial]
[Iter 480] Gaussian 0 vs 1:
  Original Loss: 0.0149577
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0149577 (Pseudo: 0.00%)
[Iter 480] Gaussian 1 vs 0:
  Original Loss: 0.0156600
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0156600 (Pseudo: 0.00%)
[Iter 490/20000] Loss: 0.0136229 (Best: 0.0125717 @iter490) ([92m↓7.64%[0m) [5.41% of initial]
[Iter 490] Gaussian 0 vs 1:
  Original Loss: 0.0125717
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0125717 (Pseudo: 0.00%)
[Iter 490] Gaussian 1 vs 0:
  Original Loss: 0.0129172
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0129172 (Pseudo: 0.00%)
Iter:499, L1 loss=0.008572, Total loss=0.01473, Time:13
[Iter 500/20000] Loss: 0.0138864 (Best: 0.0125717 @iter490) ([91m↑1.93%[0m) [5.52% of initial]
[Iter 500] Gaussian 0 vs 1:
  Original Loss: 0.0135728
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0135728 (Pseudo: 0.00%)
[Iter 500] Gaussian 1 vs 0:
  Original Loss: 0.0138634
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0138634 (Pseudo: 0.00%)
[Iter 510/20000] Loss: 0.0137117 (Best: 0.0122981 @iter508) ([92m↓1.26%[0m) [5.45% of initial]
[Iter 510] Gaussian 0 vs 1:
  Original Loss: 0.0138867
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0138867 (Pseudo: 0.00%)
[Iter 510] Gaussian 1 vs 0:
  Original Loss: 0.0140105
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0140105 (Pseudo: 0.00%)
[Iter 520/20000] Loss: 0.0130142 (Best: 0.0118542 @iter514) ([92m↓5.09%[0m) [5.17% of initial]
[Iter 520] Gaussian 0 vs 1:
  Original Loss: 0.0130147
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0130147 (Pseudo: 0.00%)
[Iter 520] Gaussian 1 vs 0:
  Original Loss: 0.0137921
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0137921 (Pseudo: 0.00%)
[Iter 530/20000] Loss: 0.0125349 (Best: 0.0113220 @iter529) ([92m↓3.68%[0m) [4.98% of initial]
[Iter 530] Gaussian 0 vs 1:
  Original Loss: 0.0127368
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0127368 (Pseudo: 0.00%)
[Iter 530] Gaussian 1 vs 0:
  Original Loss: 0.0128324
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0128324 (Pseudo: 0.00%)
[Iter 540/20000] Loss: 0.0123509 (Best: 0.0109954 @iter538) ([92m↓1.47%[0m) [4.91% of initial]
[Iter 540] Gaussian 0 vs 1:
  Original Loss: 0.0123127
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0123127 (Pseudo: 0.00%)
[Iter 540] Gaussian 1 vs 0:
  Original Loss: 0.0124902
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0124902 (Pseudo: 0.00%)
[Iter 550/20000] Loss: 0.0120223 (Best: 0.0108873 @iter548) ([92m↓2.66%[0m) [4.78% of initial]
[Iter 550] Gaussian 0 vs 1:
  Original Loss: 0.0120178
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0120178 (Pseudo: 0.00%)
[Iter 550] Gaussian 1 vs 0:
  Original Loss: 0.0120192
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0120192 (Pseudo: 0.00%)
[Iter 560/20000] Loss: 0.0120279 (Best: 0.0107718 @iter554) ([91m↑0.05%[0m) [4.78% of initial]
[Iter 560] Gaussian 0 vs 1:
  Original Loss: 0.0117566
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0117566 (Pseudo: 0.00%)
[Iter 560] Gaussian 1 vs 0:
  Original Loss: 0.0120415
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0120415 (Pseudo: 0.00%)
[Iter 570/20000] Loss: 0.0116612 (Best: 0.0105740 @iter569) ([92m↓3.05%[0m) [4.63% of initial]
[Iter 570] Gaussian 0 vs 1:
  Original Loss: 0.0125414
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0125414 (Pseudo: 0.00%)
[Iter 570] Gaussian 1 vs 0:
  Original Loss: 0.0124742
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0124742 (Pseudo: 0.00%)
[Iter 580/20000] Loss: 0.0111474 (Best: 0.0102315 @iter578) ([92m↓4.41%[0m) [4.43% of initial]
[Iter 580] Gaussian 0 vs 1:
  Original Loss: 0.0111191
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0111191 (Pseudo: 0.00%)
[Iter 580] Gaussian 1 vs 0:
  Original Loss: 0.0112327
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0112327 (Pseudo: 0.00%)
[Iter 590/20000] Loss: 0.0112633 (Best: 0.0101339 @iter581) ([91m↑1.04%[0m) [4.47% of initial]
[Iter 590] Gaussian 0 vs 1:
  Original Loss: 0.0110161
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0110161 (Pseudo: 0.00%)
[Iter 590] Gaussian 1 vs 0:
  Original Loss: 0.0115692
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0115692 (Pseudo: 0.00%)
Iter:599, L1 loss=0.007102, Total loss=0.01171, Time:12
[Iter 600/20000] Loss: 0.0109169 (Best: 0.0099806 @iter597) ([92m↓3.08%[0m) [4.34% of initial]
[Iter 600] Gaussian 0 vs 1:
  Original Loss: 0.0109069
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0109069 (Pseudo: 0.00%)
[Iter 600] Gaussian 1 vs 0:
  Original Loss: 0.0112734
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0112734 (Pseudo: 0.00%)
[Iter 610/20000] Loss: 0.0226870 (Best: 0.0099806 @iter597) ([91m↑107.82%[0m) [9.01% of initial]
[Iter 610] Gaussian 0 vs 1:
  Original Loss: 0.0201165
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0201165 (Pseudo: 0.00%)
[Iter 610] Gaussian 1 vs 0:
  Original Loss: 0.0185503
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0185503 (Pseudo: 0.00%)
[Iter 620/20000] Loss: 0.0142463 (Best: 0.0099806 @iter597) ([92m↓37.20%[0m) [5.66% of initial]
[Iter 620] Gaussian 0 vs 1:
  Original Loss: 0.0128440
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0128440 (Pseudo: 0.00%)
[Iter 620] Gaussian 1 vs 0:
  Original Loss: 0.0127735
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0127735 (Pseudo: 0.00%)
[Iter 630/20000] Loss: 0.0121296 (Best: 0.0099806 @iter597) ([92m↓14.86%[0m) [4.82% of initial]
[Iter 630] Gaussian 0 vs 1:
  Original Loss: 0.0111938
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0111938 (Pseudo: 0.00%)
[Iter 630] Gaussian 1 vs 0:
  Original Loss: 0.0115964
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0115964 (Pseudo: 0.00%)
[Iter 640/20000] Loss: 0.0103583 (Best: 0.0093827 @iter640) ([92m↓14.60%[0m) [4.12% of initial]
[Iter 640] Gaussian 0 vs 1:
  Original Loss: 0.0093827
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0093827 (Pseudo: 0.00%)
[Iter 640] Gaussian 1 vs 0:
  Original Loss: 0.0096929
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0096929 (Pseudo: 0.00%)
[Iter 650/20000] Loss: 0.0106687 (Best: 0.0090992 @iter646) ([91m↑3.00%[0m) [4.24% of initial]
[Iter 650] Gaussian 0 vs 1:
  Original Loss: 0.0106487
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0106487 (Pseudo: 0.00%)
[Iter 650] Gaussian 1 vs 0:
  Original Loss: 0.0104906
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0104906 (Pseudo: 0.00%)
[Iter 660/20000] Loss: 0.0100758 (Best: 0.0088308 @iter655) ([92m↓5.56%[0m) [4.00% of initial]
[Iter 660] Gaussian 0 vs 1:
  Original Loss: 0.0107269
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0107269 (Pseudo: 0.00%)
[Iter 660] Gaussian 1 vs 0:
  Original Loss: 0.0105760
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0105760 (Pseudo: 0.00%)
[Iter 670/20000] Loss: 0.0096225 (Best: 0.0085632 @iter667) ([92m↓4.50%[0m) [3.82% of initial]
[Iter 670] Gaussian 0 vs 1:
  Original Loss: 0.0095038
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0095038 (Pseudo: 0.00%)
[Iter 670] Gaussian 1 vs 0:
  Original Loss: 0.0095573
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0095573 (Pseudo: 0.00%)
[Iter 680/20000] Loss: 0.0089444 (Best: 0.0083295 @iter680) ([92m↓7.05%[0m) [3.55% of initial]
[Iter 680] Gaussian 0 vs 1:
  Original Loss: 0.0083295
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0083295 (Pseudo: 0.00%)
[Iter 680] Gaussian 1 vs 0:
  Original Loss: 0.0084241
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0084241 (Pseudo: 0.00%)
[Iter 690/20000] Loss: 0.0091424 (Best: 0.0079674 @iter682) ([91m↑2.21%[0m) [3.63% of initial]
[Iter 690] Gaussian 0 vs 1:
  Original Loss: 0.0086427
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0086427 (Pseudo: 0.00%)
[Iter 690] Gaussian 1 vs 0:
  Original Loss: 0.0091220
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0091220 (Pseudo: 0.00%)
Iter:699, L1 loss=0.005858, Total loss=0.009582, Time:12
[Iter 700/20000] Loss: 0.0090638 (Best: 0.0079674 @iter682) ([92m↓0.86%[0m) [3.60% of initial]
[Iter 700] Gaussian 0 vs 1:
  Original Loss: 0.0090543
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0090543 (Pseudo: 0.00%)
[Iter 700] Gaussian 1 vs 0:
  Original Loss: 0.0089196
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0089196 (Pseudo: 0.00%)
[Iter 710/20000] Loss: 0.0084859 (Best: 0.0079323 @iter710) ([92m↓6.38%[0m) [3.37% of initial]
[Iter 710] Gaussian 0 vs 1:
  Original Loss: 0.0079323
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0079323 (Pseudo: 0.00%)
[Iter 710] Gaussian 1 vs 0:
  Original Loss: 0.0078393
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0078393 (Pseudo: 0.00%)
[Iter 720/20000] Loss: 0.0084705 (Best: 0.0077359 @iter715) ([92m↓0.18%[0m) [3.37% of initial]
[Iter 720] Gaussian 0 vs 1:
  Original Loss: 0.0079169
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0079169 (Pseudo: 0.00%)
[Iter 720] Gaussian 1 vs 0:
  Original Loss: 0.0078790
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0078790 (Pseudo: 0.00%)
[Iter 730/20000] Loss: 0.0085133 (Best: 0.0074094 @iter727) ([91m↑0.50%[0m) [3.38% of initial]
[Iter 730] Gaussian 0 vs 1:
  Original Loss: 0.0084818
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0084818 (Pseudo: 0.00%)
[Iter 730] Gaussian 1 vs 0:
  Original Loss: 0.0085123
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0085123 (Pseudo: 0.00%)
[Iter 740/20000] Loss: 0.0084903 (Best: 0.0073121 @iter733) ([92m↓0.27%[0m) [3.37% of initial]
[Iter 740] Gaussian 0 vs 1:
  Original Loss: 0.0085625
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0085625 (Pseudo: 0.00%)
[Iter 740] Gaussian 1 vs 0:
  Original Loss: 0.0087329
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0087329 (Pseudo: 0.00%)
[Iter 750/20000] Loss: 0.0080259 (Best: 0.0069940 @iter748) ([92m↓5.47%[0m) [3.19% of initial]
[Iter 750] Gaussian 0 vs 1:
  Original Loss: 0.0080502
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0080502 (Pseudo: 0.00%)
[Iter 750] Gaussian 1 vs 0:
  Original Loss: 0.0081641
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0081641 (Pseudo: 0.00%)
[Iter 760/20000] Loss: 0.0073984 (Best: 0.0069245 @iter751) ([92m↓7.82%[0m) [2.94% of initial]
[Iter 760] Gaussian 0 vs 1:
  Original Loss: 0.0070047
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0070047 (Pseudo: 0.00%)
[Iter 760] Gaussian 1 vs 0:
  Original Loss: 0.0070974
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0070974 (Pseudo: 0.00%)
[Iter 770/20000] Loss: 0.0077021 (Best: 0.0069245 @iter751) ([91m↑4.10%[0m) [3.06% of initial]
[Iter 770] Gaussian 0 vs 1:
  Original Loss: 0.0080962
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0080962 (Pseudo: 0.00%)
[Iter 770] Gaussian 1 vs 0:
  Original Loss: 0.0079177
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0079177 (Pseudo: 0.00%)
[Iter 780/20000] Loss: 0.0078750 (Best: 0.0068429 @iter778) ([91m↑2.24%[0m) [3.13% of initial]
[Iter 780] Gaussian 0 vs 1:
  Original Loss: 0.0084408
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0084408 (Pseudo: 0.00%)
[Iter 780] Gaussian 1 vs 0:
  Original Loss: 0.0083337
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0083337 (Pseudo: 0.00%)
[Iter 790/20000] Loss: 0.0077143 (Best: 0.0066965 @iter787) ([92m↓2.04%[0m) [3.06% of initial]
[Iter 790] Gaussian 0 vs 1:
  Original Loss: 0.0077066
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0077066 (Pseudo: 0.00%)
[Iter 790] Gaussian 1 vs 0:
  Original Loss: 0.0075599
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0075599 (Pseudo: 0.00%)
Iter:799, L1 loss=0.005002, Total loss=0.008045, Time:12
[Iter 800/20000] Loss: 0.0073670 (Best: 0.0066757 @iter800) ([92m↓4.50%[0m) [2.93% of initial]
[Iter 800] Gaussian 0 vs 1:
  Original Loss: 0.0066757
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0066757 (Pseudo: 0.00%)
[Iter 800] Gaussian 1 vs 0:
  Original Loss: 0.0067306
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0067306 (Pseudo: 0.00%)
[Iter 810/20000] Loss: 0.0155931 (Best: 0.0066757 @iter800) ([91m↑111.66%[0m) [6.19% of initial]
[Iter 810] Gaussian 0 vs 1:
  Original Loss: 0.0143672
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0143672 (Pseudo: 0.00%)
[Iter 810] Gaussian 1 vs 0:
  Original Loss: 0.0140984
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0140984 (Pseudo: 0.00%)
[Iter 820/20000] Loss: 0.0108039 (Best: 0.0066757 @iter800) ([92m↓30.71%[0m) [4.29% of initial]
[Iter 820] Gaussian 0 vs 1:
  Original Loss: 0.0106544
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0106544 (Pseudo: 0.00%)
[Iter 820] Gaussian 1 vs 0:
  Original Loss: 0.0108468
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0108468 (Pseudo: 0.00%)
[Iter 830/20000] Loss: 0.0088772 (Best: 0.0066757 @iter800) ([92m↓17.83%[0m) [3.53% of initial]
[Iter 830] Gaussian 0 vs 1:
  Original Loss: 0.0093897
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0093897 (Pseudo: 0.00%)
[Iter 830] Gaussian 1 vs 0:
  Original Loss: 0.0090073
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0090073 (Pseudo: 0.00%)
[Iter 840/20000] Loss: 0.0080064 (Best: 0.0066757 @iter800) ([92m↓9.81%[0m) [3.18% of initial]
[Iter 840] Gaussian 0 vs 1:
  Original Loss: 0.0087939
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0087939 (Pseudo: 0.00%)
[Iter 840] Gaussian 1 vs 0:
  Original Loss: 0.0086543
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0086543 (Pseudo: 0.00%)
[Iter 850/20000] Loss: 0.0074728 (Best: 0.0066110 @iter841) ([92m↓6.66%[0m) [2.97% of initial]
[Iter 850] Gaussian 0 vs 1:
  Original Loss: 0.0072854
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0072854 (Pseudo: 0.00%)
[Iter 850] Gaussian 1 vs 0:
  Original Loss: 0.0073029
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0073029 (Pseudo: 0.00%)
[Iter 860/20000] Loss: 0.0069943 (Best: 0.0062077 @iter856) ([92m↓6.40%[0m) [2.78% of initial]
[Iter 860] Gaussian 0 vs 1:
  Original Loss: 0.0070418
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0070418 (Pseudo: 0.00%)
[Iter 860] Gaussian 1 vs 0:
  Original Loss: 0.0071603
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0071603 (Pseudo: 0.00%)
[Iter 870/20000] Loss: 0.0067155 (Best: 0.0060387 @iter862) ([92m↓3.99%[0m) [2.67% of initial]
[Iter 870] Gaussian 0 vs 1:
  Original Loss: 0.0062320
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0062320 (Pseudo: 0.00%)
[Iter 870] Gaussian 1 vs 0:
  Original Loss: 0.0063596
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0063596 (Pseudo: 0.00%)
[Iter 880/20000] Loss: 0.0067303 (Best: 0.0059206 @iter872) ([91m↑0.22%[0m) [2.67% of initial]
[Iter 880] Gaussian 0 vs 1:
  Original Loss: 0.0066641
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0066641 (Pseudo: 0.00%)
[Iter 880] Gaussian 1 vs 0:
  Original Loss: 0.0067342
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0067342 (Pseudo: 0.00%)
[Iter 890/20000] Loss: 0.0062558 (Best: 0.0056485 @iter884) ([92m↓7.05%[0m) [2.49% of initial]
[Iter 890] Gaussian 0 vs 1:
  Original Loss: 0.0057310
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0057310 (Pseudo: 0.00%)
[Iter 890] Gaussian 1 vs 0:
  Original Loss: 0.0059526
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0059526 (Pseudo: 0.00%)
Iter:899, L1 loss=0.003776, Total loss=0.005591, Time:13
[Iter 900/20000] Loss: 0.0064417 (Best: 0.0055795 @iter896) ([91m↑2.97%[0m) [2.56% of initial]
[Iter 900] Gaussian 0 vs 1:
  Original Loss: 0.0065312
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0065312 (Pseudo: 0.00%)
[Iter 900] Gaussian 1 vs 0:
  Original Loss: 0.0066473
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0066473 (Pseudo: 0.00%)
[Iter 910/20000] Loss: 0.0065861 (Best: 0.0053523 @iter907) ([91m↑2.24%[0m) [2.62% of initial]
[Iter 910] Gaussian 0 vs 1:
  Original Loss: 0.0069971
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0069971 (Pseudo: 0.00%)
[Iter 910] Gaussian 1 vs 0:
  Original Loss: 0.0068304
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0068304 (Pseudo: 0.00%)
[Iter 920/20000] Loss: 0.0059679 (Best: 0.0052495 @iter916) ([92m↓9.39%[0m) [2.37% of initial]
[Iter 920] Gaussian 0 vs 1:
  Original Loss: 0.0060867
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0060867 (Pseudo: 0.00%)
[Iter 920] Gaussian 1 vs 0:
  Original Loss: 0.0062539
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0062539 (Pseudo: 0.00%)
[Iter 930/20000] Loss: 0.0063876 (Best: 0.0052495 @iter916) ([91m↑7.03%[0m) [2.54% of initial]
[Iter 930] Gaussian 0 vs 1:
  Original Loss: 0.0068599
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0068599 (Pseudo: 0.00%)
[Iter 930] Gaussian 1 vs 0:
  Original Loss: 0.0067122
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0067122 (Pseudo: 0.00%)
[Iter 940/20000] Loss: 0.0064095 (Best: 0.0051416 @iter938) ([91m↑0.34%[0m) [2.55% of initial]
[Iter 940] Gaussian 0 vs 1:
  Original Loss: 0.0067858
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0067858 (Pseudo: 0.00%)
[Iter 940] Gaussian 1 vs 0:
  Original Loss: 0.0066155
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0066155 (Pseudo: 0.00%)
[Iter 950/20000] Loss: 0.0059223 (Best: 0.0051416 @iter938) ([92m↓7.60%[0m) [2.35% of initial]
[Iter 950] Gaussian 0 vs 1:
  Original Loss: 0.0055614
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0055614 (Pseudo: 0.00%)
[Iter 950] Gaussian 1 vs 0:
  Original Loss: 0.0058549
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0058549 (Pseudo: 0.00%)
[Iter 960/20000] Loss: 0.0060871 (Best: 0.0051416 @iter938) ([91m↑2.78%[0m) [2.42% of initial]
[Iter 960] Gaussian 0 vs 1:
  Original Loss: 0.0062680
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0062680 (Pseudo: 0.00%)
[Iter 960] Gaussian 1 vs 0:
  Original Loss: 0.0063805
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0063805 (Pseudo: 0.00%)
[Iter 970/20000] Loss: 0.0059262 (Best: 0.0051416 @iter938) ([92m↓2.64%[0m) [2.35% of initial]
[Iter 970] Gaussian 0 vs 1:
  Original Loss: 0.0057799
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0057799 (Pseudo: 0.00%)
[Iter 970] Gaussian 1 vs 0:
  Original Loss: 0.0058966
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0058966 (Pseudo: 0.00%)
[Iter 980/20000] Loss: 0.0061291 (Best: 0.0050736 @iter979) ([91m↑3.42%[0m) [2.44% of initial]
[Iter 980] Gaussian 0 vs 1:
  Original Loss: 0.0069057
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0069057 (Pseudo: 0.00%)
[Iter 980] Gaussian 1 vs 0:
  Original Loss: 0.0066209
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0066209 (Pseudo: 0.00%)
[Iter 990/20000] Loss: 0.0061184 (Best: 0.0050736 @iter979) ([92m↓0.17%[0m) [2.43% of initial]
[Iter 990] Gaussian 0 vs 1:
  Original Loss: 0.0061681
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0061681 (Pseudo: 0.00%)
[Iter 990] Gaussian 1 vs 0:
  Original Loss: 0.0064471
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0064471 (Pseudo: 0.00%)
Iter:999, L1 loss=0.004395, Total loss=0.006974, Time:13
[Iter 1000/20000] Loss: 0.0064544 (Best: 0.0050736 @iter979) ([91m↑5.49%[0m) [2.56% of initial]
[Iter 1000] Gaussian 0 vs 1:
  Original Loss: 0.0067566
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0067566 (Pseudo: 0.00%)
[Iter 1000] Gaussian 1 vs 0:
  Original Loss: 0.0063786
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0063786 (Pseudo: 0.00%)
[Iter 1010/20000] Loss: 0.0114101 (Best: 0.0050736 @iter979) ([91m↑76.78%[0m) [4.53% of initial]
[Iter 1010] Gaussian 0 vs 1:
  Original Loss: 0.0110085
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0110085 (Pseudo: 0.00%)
[Iter 1010] Gaussian 1 vs 0:
  Original Loss: 0.0111303
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0111303 (Pseudo: 0.00%)
[Iter 1020/20000] Loss: 0.0084174 (Best: 0.0050736 @iter979) ([92m↓26.23%[0m) [3.34% of initial]
[Iter 1020] Gaussian 0 vs 1:
  Original Loss: 0.0080219
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0080219 (Pseudo: 0.00%)
[Iter 1020] Gaussian 1 vs 0:
  Original Loss: 0.0078563
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0078563 (Pseudo: 0.00%)
[Iter 1030/20000] Loss: 0.0069297 (Best: 0.0050736 @iter979) ([92m↓17.67%[0m) [2.75% of initial]
[Iter 1030] Gaussian 0 vs 1:
  Original Loss: 0.0060526
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0060526 (Pseudo: 0.00%)
[Iter 1030] Gaussian 1 vs 0:
  Original Loss: 0.0062755
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0062755 (Pseudo: 0.00%)
[Iter 1040/20000] Loss: 0.0061052 (Best: 0.0050736 @iter979) ([92m↓11.90%[0m) [2.43% of initial]
[Iter 1040] Gaussian 0 vs 1:
  Original Loss: 0.0060792
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0060792 (Pseudo: 0.00%)
[Iter 1040] Gaussian 1 vs 0:
  Original Loss: 0.0063612
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0063612 (Pseudo: 0.00%)
[Iter 1050/20000] Loss: 0.0059193 (Best: 0.0050736 @iter979) ([92m↓3.04%[0m) [2.35% of initial]
[Iter 1050] Gaussian 0 vs 1:
  Original Loss: 0.0060871
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0060871 (Pseudo: 0.00%)
[Iter 1050] Gaussian 1 vs 0:
  Original Loss: 0.0062946
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0062946 (Pseudo: 0.00%)
[Iter 1060/20000] Loss: 0.0058083 (Best: 0.0048987 @iter1055) ([92m↓1.88%[0m) [2.31% of initial]
[Iter 1060] Gaussian 0 vs 1:
  Original Loss: 0.0060189
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0060189 (Pseudo: 0.00%)
[Iter 1060] Gaussian 1 vs 0:
  Original Loss: 0.0060832
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0060832 (Pseudo: 0.00%)
[Iter 1070/20000] Loss: 0.0055100 (Best: 0.0045490 @iter1066) ([92m↓5.13%[0m) [2.19% of initial]
[Iter 1070] Gaussian 0 vs 1:
  Original Loss: 0.0061190
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0061190 (Pseudo: 0.00%)
[Iter 1070] Gaussian 1 vs 0:
  Original Loss: 0.0061778
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0061778 (Pseudo: 0.00%)
[Iter 1080/20000] Loss: 0.0052826 (Best: 0.0045490 @iter1066) ([92m↓4.13%[0m) [2.10% of initial]
[Iter 1080] Gaussian 0 vs 1:
  Original Loss: 0.0049614
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0049614 (Pseudo: 0.00%)
[Iter 1080] Gaussian 1 vs 0:
  Original Loss: 0.0051746
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0051746 (Pseudo: 0.00%)
[Iter 1090/20000] Loss: 0.0051041 (Best: 0.0045490 @iter1066) ([92m↓3.38%[0m) [2.03% of initial]
[Iter 1090] Gaussian 0 vs 1:
  Original Loss: 0.0049920
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0049920 (Pseudo: 0.00%)
[Iter 1090] Gaussian 1 vs 0:
  Original Loss: 0.0053022
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0053022 (Pseudo: 0.00%)
Iter:1099, L1 loss=0.003567, Total loss=0.005011, Time:13
[Iter 1100/20000] Loss: 0.0050463 (Best: 0.0042903 @iter1093) ([92m↓1.13%[0m) [2.00% of initial]
[Iter 1100] Gaussian 0 vs 1:
  Original Loss: 0.0048681
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0048681 (Pseudo: 0.00%)
[Iter 1100] Gaussian 1 vs 0:
  Original Loss: 0.0047489
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0047489 (Pseudo: 0.00%)
[Iter 1110/20000] Loss: 0.0050618 (Best: 0.0042903 @iter1093) ([91m↑0.31%[0m) [2.01% of initial]
[Iter 1110] Gaussian 0 vs 1:
  Original Loss: 0.0047674
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0047674 (Pseudo: 0.00%)
[Iter 1110] Gaussian 1 vs 0:
  Original Loss: 0.0047834
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0047834 (Pseudo: 0.00%)
[Iter 1120/20000] Loss: 0.0050564 (Best: 0.0042158 @iter1117) ([92m↓0.11%[0m) [2.01% of initial]
[Iter 1120] Gaussian 0 vs 1:
  Original Loss: 0.0049531
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0049531 (Pseudo: 0.00%)
[Iter 1120] Gaussian 1 vs 0:
  Original Loss: 0.0050849
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0050849 (Pseudo: 0.00%)
[Iter 1130/20000] Loss: 0.0053555 (Best: 0.0042158 @iter1117) ([91m↑5.91%[0m) [2.13% of initial]
[Iter 1130] Gaussian 0 vs 1:
  Original Loss: 0.0057057
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0057057 (Pseudo: 0.00%)
[Iter 1130] Gaussian 1 vs 0:
  Original Loss: 0.0056902
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0056902 (Pseudo: 0.00%)
[Iter 1140/20000] Loss: 0.0048608 (Best: 0.0041515 @iter1135) ([92m↓9.24%[0m) [1.93% of initial]
[Iter 1140] Gaussian 0 vs 1:
  Original Loss: 0.0048785
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0048785 (Pseudo: 0.00%)
[Iter 1140] Gaussian 1 vs 0:
  Original Loss: 0.0052467
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0052467 (Pseudo: 0.00%)
[Iter 1150/20000] Loss: 0.0045524 (Best: 0.0040602 @iter1145) ([92m↓6.35%[0m) [1.81% of initial]
[Iter 1150] Gaussian 0 vs 1:
  Original Loss: 0.0041553
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0041553 (Pseudo: 0.00%)
[Iter 1150] Gaussian 1 vs 0:
  Original Loss: 0.0041444
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0041444 (Pseudo: 0.00%)
[Iter 1160/20000] Loss: 0.0053142 (Best: 0.0040602 @iter1145) ([91m↑16.73%[0m) [2.11% of initial]
[Iter 1160] Gaussian 0 vs 1:
  Original Loss: 0.0052046
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0052046 (Pseudo: 0.00%)
[Iter 1160] Gaussian 1 vs 0:
  Original Loss: 0.0051235
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0051235 (Pseudo: 0.00%)
[Iter 1170/20000] Loss: 0.0048004 (Best: 0.0040602 @iter1145) ([92m↓9.67%[0m) [1.91% of initial]
[Iter 1170] Gaussian 0 vs 1:
  Original Loss: 0.0044588
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0044588 (Pseudo: 0.00%)
[Iter 1170] Gaussian 1 vs 0:
  Original Loss: 0.0044537
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0044537 (Pseudo: 0.00%)
[Iter 1180/20000] Loss: 0.0044208 (Best: 0.0040134 @iter1180) ([92m↓7.91%[0m) [1.76% of initial]
[Iter 1180] Gaussian 0 vs 1:
  Original Loss: 0.0040134
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0040134 (Pseudo: 0.00%)
[Iter 1180] Gaussian 1 vs 0:
  Original Loss: 0.0039972
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0039972 (Pseudo: 0.00%)
[Iter 1190/20000] Loss: 0.0047045 (Best: 0.0039785 @iter1183) ([91m↑6.42%[0m) [1.87% of initial]
[Iter 1190] Gaussian 0 vs 1:
  Original Loss: 0.0044426
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0044426 (Pseudo: 0.00%)
[Iter 1190] Gaussian 1 vs 0:
  Original Loss: 0.0045378
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0045378 (Pseudo: 0.00%)
Iter:1199, L1 loss=0.003503, Total loss=0.005189, Time:12
[Iter 1200/20000] Loss: 0.0047256 (Best: 0.0038068 @iter1195) ([91m↑0.45%[0m) [1.88% of initial]
[Iter 1200] Gaussian 0 vs 1:
  Original Loss: 0.0046813
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0046813 (Pseudo: 0.00%)
[Iter 1200] Gaussian 1 vs 0:
  Original Loss: 0.0047296
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0047296 (Pseudo: 0.00%)
[Iter 1210/20000] Loss: 0.0109549 (Best: 0.0038068 @iter1195) ([91m↑131.82%[0m) [4.35% of initial]
[Iter 1210] Gaussian 0 vs 1:
  Original Loss: 0.0099916
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0099916 (Pseudo: 0.00%)
[Iter 1210] Gaussian 1 vs 0:
  Original Loss: 0.0096570
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0096570 (Pseudo: 0.00%)
[Iter 1220/20000] Loss: 0.0068332 (Best: 0.0038068 @iter1195) ([92m↓37.62%[0m) [2.71% of initial]
[Iter 1220] Gaussian 0 vs 1:
  Original Loss: 0.0060282
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0060282 (Pseudo: 0.00%)
[Iter 1220] Gaussian 1 vs 0:
  Original Loss: 0.0063220
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0063220 (Pseudo: 0.00%)
[Iter 1230/20000] Loss: 0.0060294 (Best: 0.0038068 @iter1195) ([92m↓11.76%[0m) [2.40% of initial]
[Iter 1230] Gaussian 0 vs 1:
  Original Loss: 0.0063008
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0063008 (Pseudo: 0.00%)
[Iter 1230] Gaussian 1 vs 0:
  Original Loss: 0.0064186
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0064186 (Pseudo: 0.00%)
[Iter 1240/20000] Loss: 0.0054698 (Best: 0.0038068 @iter1195) ([92m↓9.28%[0m) [2.17% of initial]
[Iter 1240] Gaussian 0 vs 1:
  Original Loss: 0.0055870
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0055870 (Pseudo: 0.00%)
[Iter 1240] Gaussian 1 vs 0:
  Original Loss: 0.0056606
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0056606 (Pseudo: 0.00%)
[Iter 1250/20000] Loss: 0.0048943 (Best: 0.0038068 @iter1195) ([92m↓10.52%[0m) [1.94% of initial]
[Iter 1250] Gaussian 0 vs 1:
  Original Loss: 0.0048327
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0048327 (Pseudo: 0.00%)
[Iter 1250] Gaussian 1 vs 0:
  Original Loss: 0.0047515
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0047515 (Pseudo: 0.00%)
[Iter 1260/20000] Loss: 0.0046114 (Best: 0.0037392 @iter1258) ([92m↓5.78%[0m) [1.83% of initial]
[Iter 1260] Gaussian 0 vs 1:
  Original Loss: 0.0051861
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0051861 (Pseudo: 0.00%)
[Iter 1260] Gaussian 1 vs 0:
  Original Loss: 0.0052220
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0052220 (Pseudo: 0.00%)
[Iter 1270/20000] Loss: 0.0040766 (Best: 0.0037022 @iter1269) ([92m↓11.60%[0m) [1.62% of initial]
[Iter 1270] Gaussian 0 vs 1:
  Original Loss: 0.0040252
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0040252 (Pseudo: 0.00%)
[Iter 1270] Gaussian 1 vs 0:
  Original Loss: 0.0042140
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0042140 (Pseudo: 0.00%)
[Iter 1280/20000] Loss: 0.0043350 (Best: 0.0033428 @iter1273) ([91m↑6.34%[0m) [1.72% of initial]
[Iter 1280] Gaussian 0 vs 1:
  Original Loss: 0.0041768
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0041768 (Pseudo: 0.00%)
[Iter 1280] Gaussian 1 vs 0:
  Original Loss: 0.0043471
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0043471 (Pseudo: 0.00%)
[Iter 1290/20000] Loss: 0.0042305 (Best: 0.0033112 @iter1285) ([92m↓2.41%[0m) [1.68% of initial]
[Iter 1290] Gaussian 0 vs 1:
  Original Loss: 0.0047379
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0047379 (Pseudo: 0.00%)
[Iter 1290] Gaussian 1 vs 0:
  Original Loss: 0.0048158
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0048158 (Pseudo: 0.00%)
Iter:1299, L1 loss=0.002804, Total loss=0.003782, Time:13
[Iter 1300/20000] Loss: 0.0040906 (Best: 0.0033112 @iter1285) ([92m↓3.31%[0m) [1.63% of initial]
[Iter 1300] Gaussian 0 vs 1:
  Original Loss: 0.0041029
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0041029 (Pseudo: 0.00%)
[Iter 1300] Gaussian 1 vs 0:
  Original Loss: 0.0039666
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0039666 (Pseudo: 0.00%)
[Iter 1310/20000] Loss: 0.0041188 (Best: 0.0033112 @iter1285) ([91m↑0.69%[0m) [1.64% of initial]
[Iter 1310] Gaussian 0 vs 1:
  Original Loss: 0.0038565
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0038565 (Pseudo: 0.00%)
[Iter 1310] Gaussian 1 vs 0:
  Original Loss: 0.0038989
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0038989 (Pseudo: 0.00%)
[Iter 1320/20000] Loss: 0.0039904 (Best: 0.0031883 @iter1319) ([92m↓3.12%[0m) [1.59% of initial]
[Iter 1320] Gaussian 0 vs 1:
  Original Loss: 0.0046733
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0046733 (Pseudo: 0.00%)
[Iter 1320] Gaussian 1 vs 0:
  Original Loss: 0.0045786
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0045786 (Pseudo: 0.00%)
[Iter 1330/20000] Loss: 0.0039792 (Best: 0.0031157 @iter1321) ([92m↓0.28%[0m) [1.58% of initial]
[Iter 1330] Gaussian 0 vs 1:
  Original Loss: 0.0041166
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0041166 (Pseudo: 0.00%)
[Iter 1330] Gaussian 1 vs 0:
  Original Loss: 0.0041100
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0041100 (Pseudo: 0.00%)
[Iter 1340/20000] Loss: 0.0037062 (Best: 0.0031157 @iter1321) ([92m↓6.86%[0m) [1.47% of initial]
[Iter 1340] Gaussian 0 vs 1:
  Original Loss: 0.0033376
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0033376 (Pseudo: 0.00%)
[Iter 1340] Gaussian 1 vs 0:
  Original Loss: 0.0033620
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0033620 (Pseudo: 0.00%)
[Iter 1350/20000] Loss: 0.0037873 (Best: 0.0031157 @iter1321) ([91m↑2.19%[0m) [1.50% of initial]
[Iter 1350] Gaussian 0 vs 1:
  Original Loss: 0.0033853
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0033853 (Pseudo: 0.00%)
[Iter 1350] Gaussian 1 vs 0:
  Original Loss: 0.0033520
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0033520 (Pseudo: 0.00%)
[Iter 1360/20000] Loss: 0.0038166 (Best: 0.0031157 @iter1321) ([91m↑0.77%[0m) [1.52% of initial]
[Iter 1360] Gaussian 0 vs 1:
  Original Loss: 0.0035835
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0035835 (Pseudo: 0.00%)
[Iter 1360] Gaussian 1 vs 0:
  Original Loss: 0.0037782
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0037782 (Pseudo: 0.00%)
[Iter 1370/20000] Loss: 0.0037579 (Best: 0.0031157 @iter1321) ([92m↓1.54%[0m) [1.49% of initial]
[Iter 1370] Gaussian 0 vs 1:
  Original Loss: 0.0033161
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0033161 (Pseudo: 0.00%)
[Iter 1370] Gaussian 1 vs 0:
  Original Loss: 0.0033149
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0033149 (Pseudo: 0.00%)
[Iter 1380/20000] Loss: 0.0039391 (Best: 0.0031157 @iter1321) ([91m↑4.82%[0m) [1.56% of initial]
[Iter 1380] Gaussian 0 vs 1:
  Original Loss: 0.0039523
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0039523 (Pseudo: 0.00%)
[Iter 1380] Gaussian 1 vs 0:
  Original Loss: 0.0039351
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0039351 (Pseudo: 0.00%)
[Iter 1390/20000] Loss: 0.0037141 (Best: 0.0031157 @iter1321) ([92m↓5.71%[0m) [1.48% of initial]
[Iter 1390] Gaussian 0 vs 1:
  Original Loss: 0.0035930
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0035930 (Pseudo: 0.00%)
[Iter 1390] Gaussian 1 vs 0:
  Original Loss: 0.0035677
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0035677 (Pseudo: 0.00%)
Iter:1399, L1 loss=0.002301, Total loss=0.002911, Time:13
[Iter 1400/20000] Loss: 0.0034058 (Best: 0.0029115 @iter1399) ([92m↓8.30%[0m) [1.35% of initial]
[Iter 1400] Gaussian 0 vs 1:
  Original Loss: 0.0034453
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0034453 (Pseudo: 0.00%)
[Iter 1400] Gaussian 1 vs 0:
  Original Loss: 0.0035088
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0035088 (Pseudo: 0.00%)
[Iter 1410/20000] Loss: 0.0090438 (Best: 0.0029115 @iter1399) ([91m↑165.54%[0m) [3.59% of initial]
[Iter 1410] Gaussian 0 vs 1:
  Original Loss: 0.0085932
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0085932 (Pseudo: 0.00%)
[Iter 1410] Gaussian 1 vs 0:
  Original Loss: 0.0081792
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0081792 (Pseudo: 0.00%)
[Iter 1420/20000] Loss: 0.0061622 (Best: 0.0029115 @iter1399) ([92m↓31.86%[0m) [2.45% of initial]
[Iter 1420] Gaussian 0 vs 1:
  Original Loss: 0.0058311
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0058311 (Pseudo: 0.00%)
[Iter 1420] Gaussian 1 vs 0:
  Original Loss: 0.0055593
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0055593 (Pseudo: 0.00%)
[Iter 1430/20000] Loss: 0.0050902 (Best: 0.0029115 @iter1399) ([92m↓17.40%[0m) [2.02% of initial]
[Iter 1430] Gaussian 0 vs 1:
  Original Loss: 0.0043874
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0043874 (Pseudo: 0.00%)
[Iter 1430] Gaussian 1 vs 0:
  Original Loss: 0.0043331
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0043331 (Pseudo: 0.00%)
[Iter 1440/20000] Loss: 0.0045893 (Best: 0.0029115 @iter1399) ([92m↓9.84%[0m) [1.82% of initial]
[Iter 1440] Gaussian 0 vs 1:
  Original Loss: 0.0046832
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0046832 (Pseudo: 0.00%)
[Iter 1440] Gaussian 1 vs 0:
  Original Loss: 0.0044418
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0044418 (Pseudo: 0.00%)
[Iter 1450/20000] Loss: 0.0035705 (Best: 0.0029115 @iter1399) ([92m↓22.20%[0m) [1.42% of initial]
[Iter 1450] Gaussian 0 vs 1:
  Original Loss: 0.0031571
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0031571 (Pseudo: 0.00%)
[Iter 1450] Gaussian 1 vs 0:
  Original Loss: 0.0031602
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0031602 (Pseudo: 0.00%)
[Iter 1460/20000] Loss: 0.0035775 (Best: 0.0029115 @iter1399) ([91m↑0.20%[0m) [1.42% of initial]
[Iter 1460] Gaussian 0 vs 1:
  Original Loss: 0.0035905
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0035905 (Pseudo: 0.00%)
[Iter 1460] Gaussian 1 vs 0:
  Original Loss: 0.0036434
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0036434 (Pseudo: 0.00%)
[Iter 1470/20000] Loss: 0.0034702 (Best: 0.0029115 @iter1399) ([92m↓3.00%[0m) [1.38% of initial]
[Iter 1470] Gaussian 0 vs 1:
  Original Loss: 0.0031478
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0031478 (Pseudo: 0.00%)
[Iter 1470] Gaussian 1 vs 0:
  Original Loss: 0.0030841
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0030841 (Pseudo: 0.00%)
[Iter 1480/20000] Loss: 0.0033700 (Best: 0.0028405 @iter1480) ([92m↓2.89%[0m) [1.34% of initial]
[Iter 1480] Gaussian 0 vs 1:
  Original Loss: 0.0028405
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0028405 (Pseudo: 0.00%)
[Iter 1480] Gaussian 1 vs 0:
  Original Loss: 0.0027969
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0027969 (Pseudo: 0.00%)
[Iter 1490/20000] Loss: 0.0032900 (Best: 0.0028405 @iter1480) ([92m↓2.37%[0m) [1.31% of initial]
[Iter 1490] Gaussian 0 vs 1:
  Original Loss: 0.0033119
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0033119 (Pseudo: 0.00%)
[Iter 1490] Gaussian 1 vs 0:
  Original Loss: 0.0033185
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0033185 (Pseudo: 0.00%)
Iter:1499, L1 loss=0.002554, Total loss=0.003328, Time:13
[Iter 1500/20000] Loss: 0.0032415 (Best: 0.0028405 @iter1480) ([92m↓1.47%[0m) [1.29% of initial]
[Iter 1500] Gaussian 0 vs 1:
  Original Loss: 0.0029587
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0029587 (Pseudo: 0.00%)
[Iter 1500] Gaussian 1 vs 0:
  Original Loss: 0.0029637
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0029637 (Pseudo: 0.00%)
[Iter 1510/20000] Loss: 0.0031027 (Best: 0.0026108 @iter1504) ([92m↓4.28%[0m) [1.23% of initial]
[Iter 1510] Gaussian 0 vs 1:
  Original Loss: 0.0028233
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0028233 (Pseudo: 0.00%)
[Iter 1510] Gaussian 1 vs 0:
  Original Loss: 0.0028236
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0028236 (Pseudo: 0.00%)
[Iter 1520/20000] Loss: 0.0030697 (Best: 0.0026014 @iter1520) ([92m↓1.06%[0m) [1.22% of initial]
[Iter 1520] Gaussian 0 vs 1:
  Original Loss: 0.0026014
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0026014 (Pseudo: 0.00%)
[Iter 1520] Gaussian 1 vs 0:
  Original Loss: 0.0026043
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0026043 (Pseudo: 0.00%)
[Iter 1530/20000] Loss: 0.0032212 (Best: 0.0025709 @iter1526) ([91m↑4.94%[0m) [1.28% of initial]
[Iter 1530] Gaussian 0 vs 1:
  Original Loss: 0.0032503
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0032503 (Pseudo: 0.00%)
[Iter 1530] Gaussian 1 vs 0:
  Original Loss: 0.0032234
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0032234 (Pseudo: 0.00%)
[Iter 1540/20000] Loss: 0.0031142 (Best: 0.0025709 @iter1526) ([92m↓3.32%[0m) [1.24% of initial]
[Iter 1540] Gaussian 0 vs 1:
  Original Loss: 0.0029162
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0029162 (Pseudo: 0.00%)
[Iter 1540] Gaussian 1 vs 0:
  Original Loss: 0.0029088
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0029088 (Pseudo: 0.00%)
[Iter 1550/20000] Loss: 0.0030425 (Best: 0.0025709 @iter1526) ([92m↓2.30%[0m) [1.21% of initial]
[Iter 1550] Gaussian 0 vs 1:
  Original Loss: 0.0029399
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0029399 (Pseudo: 0.00%)
[Iter 1550] Gaussian 1 vs 0:
  Original Loss: 0.0030088
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0030088 (Pseudo: 0.00%)
[Iter 1560/20000] Loss: 0.0032830 (Best: 0.0025565 @iter1558) ([91m↑7.90%[0m) [1.30% of initial]
[Iter 1560] Gaussian 0 vs 1:
  Original Loss: 0.0039155
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0039155 (Pseudo: 0.00%)
[Iter 1560] Gaussian 1 vs 0:
  Original Loss: 0.0039436
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0039436 (Pseudo: 0.00%)
[Iter 1570/20000] Loss: 0.0028300 (Best: 0.0024806 @iter1569) ([92m↓13.80%[0m) [1.12% of initial]
[Iter 1570] Gaussian 0 vs 1:
  Original Loss: 0.0027057
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0027057 (Pseudo: 0.00%)
[Iter 1570] Gaussian 1 vs 0:
  Original Loss: 0.0028462
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0028462 (Pseudo: 0.00%)
[Iter 1580/20000] Loss: 0.0028738 (Best: 0.0023563 @iter1573) ([91m↑1.55%[0m) [1.14% of initial]
[Iter 1580] Gaussian 0 vs 1:
  Original Loss: 0.0029608
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0029608 (Pseudo: 0.00%)
[Iter 1580] Gaussian 1 vs 0:
  Original Loss: 0.0032560
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0032560 (Pseudo: 0.00%)
[Iter 1590/20000] Loss: 0.0027756 (Best: 0.0023563 @iter1573) ([92m↓3.42%[0m) [1.10% of initial]
[Iter 1590] Gaussian 0 vs 1:
  Original Loss: 0.0025154
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0025154 (Pseudo: 0.00%)
[Iter 1590] Gaussian 1 vs 0:
  Original Loss: 0.0026211
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0026211 (Pseudo: 0.00%)
Iter:1599, L1 loss=0.002751, Total loss=0.003428, Time:13
[Iter 1600/20000] Loss: 0.0031057 (Best: 0.0023563 @iter1573) ([91m↑11.89%[0m) [1.23% of initial]
[Iter 1600] Gaussian 0 vs 1:
  Original Loss: 0.0033145
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0033145 (Pseudo: 0.00%)
[Iter 1600] Gaussian 1 vs 0:
  Original Loss: 0.0033184
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0033184 (Pseudo: 0.00%)
[Iter 1610/20000] Loss: 0.0086785 (Best: 0.0023563 @iter1573) ([91m↑179.44%[0m) [3.45% of initial]
[Iter 1610] Gaussian 0 vs 1:
  Original Loss: 0.0076747
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0076747 (Pseudo: 0.00%)
[Iter 1610] Gaussian 1 vs 0:
  Original Loss: 0.0075440
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0075440 (Pseudo: 0.00%)
[Iter 1620/20000] Loss: 0.0054409 (Best: 0.0023563 @iter1573) ([92m↓37.31%[0m) [2.16% of initial]
[Iter 1620] Gaussian 0 vs 1:
  Original Loss: 0.0058999
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0058999 (Pseudo: 0.00%)
[Iter 1620] Gaussian 1 vs 0:
  Original Loss: 0.0062347
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0062347 (Pseudo: 0.00%)
[Iter 1630/20000] Loss: 0.0044811 (Best: 0.0023563 @iter1573) ([92m↓17.64%[0m) [1.78% of initial]
[Iter 1630] Gaussian 0 vs 1:
  Original Loss: 0.0040060
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0040060 (Pseudo: 0.00%)
[Iter 1630] Gaussian 1 vs 0:
  Original Loss: 0.0036977
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0036977 (Pseudo: 0.00%)
[Iter 1640/20000] Loss: 0.0039505 (Best: 0.0023563 @iter1573) ([92m↓11.84%[0m) [1.57% of initial]
[Iter 1640] Gaussian 0 vs 1:
  Original Loss: 0.0043569
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0043569 (Pseudo: 0.00%)
[Iter 1640] Gaussian 1 vs 0:
  Original Loss: 0.0042930
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0042930 (Pseudo: 0.00%)
[Iter 1650/20000] Loss: 0.0034588 (Best: 0.0023563 @iter1573) ([92m↓12.45%[0m) [1.37% of initial]
[Iter 1650] Gaussian 0 vs 1:
  Original Loss: 0.0034012
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0034012 (Pseudo: 0.00%)
[Iter 1650] Gaussian 1 vs 0:
  Original Loss: 0.0033312
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0033312 (Pseudo: 0.00%)
[Iter 1660/20000] Loss: 0.0029397 (Best: 0.0023563 @iter1573) ([92m↓15.01%[0m) [1.17% of initial]
[Iter 1660] Gaussian 0 vs 1:
  Original Loss: 0.0025669
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0025669 (Pseudo: 0.00%)
[Iter 1660] Gaussian 1 vs 0:
  Original Loss: 0.0025496
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0025496 (Pseudo: 0.00%)
[Iter 1670/20000] Loss: 0.0027867 (Best: 0.0023075 @iter1669) ([92m↓5.20%[0m) [1.11% of initial]
[Iter 1670] Gaussian 0 vs 1:
  Original Loss: 0.0028154
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0028154 (Pseudo: 0.00%)
[Iter 1670] Gaussian 1 vs 0:
  Original Loss: 0.0028748
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0028748 (Pseudo: 0.00%)
[Iter 1680/20000] Loss: 0.0029164 (Best: 0.0023075 @iter1669) ([91m↑4.66%[0m) [1.16% of initial]
[Iter 1680] Gaussian 0 vs 1:
  Original Loss: 0.0027675
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0027675 (Pseudo: 0.00%)
[Iter 1680] Gaussian 1 vs 0:
  Original Loss: 0.0028827
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0028827 (Pseudo: 0.00%)
[Iter 1690/20000] Loss: 0.0030069 (Best: 0.0023075 @iter1669) ([91m↑3.10%[0m) [1.19% of initial]
[Iter 1690] Gaussian 0 vs 1:
  Original Loss: 0.0027275
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0027275 (Pseudo: 0.00%)
[Iter 1690] Gaussian 1 vs 0:
  Original Loss: 0.0029747
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0029747 (Pseudo: 0.00%)
Iter:1699, L1 loss=0.0026, Total loss=0.00316, Time:14
[Iter 1700/20000] Loss: 0.0027997 (Best: 0.0023075 @iter1669) ([92m↓6.89%[0m) [1.11% of initial]
[Iter 1700] Gaussian 0 vs 1:
  Original Loss: 0.0023884
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0023884 (Pseudo: 0.00%)
[Iter 1700] Gaussian 1 vs 0:
  Original Loss: 0.0024271
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0024271 (Pseudo: 0.00%)
[Iter 1710/20000] Loss: 0.0030678 (Best: 0.0023075 @iter1669) ([91m↑9.58%[0m) [1.22% of initial]
[Iter 1710] Gaussian 0 vs 1:
  Original Loss: 0.0032399
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0032399 (Pseudo: 0.00%)
[Iter 1710] Gaussian 1 vs 0:
  Original Loss: 0.0034713
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0034713 (Pseudo: 0.00%)
[Iter 1720/20000] Loss: 0.0025782 (Best: 0.0023075 @iter1669) ([92m↓15.96%[0m) [1.02% of initial]
[Iter 1720] Gaussian 0 vs 1:
  Original Loss: 0.0023834
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0023834 (Pseudo: 0.00%)
[Iter 1720] Gaussian 1 vs 0:
  Original Loss: 0.0024817
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0024817 (Pseudo: 0.00%)
[Iter 1730/20000] Loss: 0.0026438 (Best: 0.0023075 @iter1669) ([91m↑2.54%[0m) [1.05% of initial]
[Iter 1730] Gaussian 0 vs 1:
  Original Loss: 0.0023442
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0023442 (Pseudo: 0.00%)
[Iter 1730] Gaussian 1 vs 0:
  Original Loss: 0.0024086
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0024086 (Pseudo: 0.00%)
[Iter 1740/20000] Loss: 0.0025877 (Best: 0.0021688 @iter1738) ([92m↓2.12%[0m) [1.03% of initial]
[Iter 1740] Gaussian 0 vs 1:
  Original Loss: 0.0024534
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0024534 (Pseudo: 0.00%)
[Iter 1740] Gaussian 1 vs 0:
  Original Loss: 0.0025344
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0025344 (Pseudo: 0.00%)
[Iter 1750/20000] Loss: 0.0023591 (Best: 0.0021193 @iter1750) ([92m↓8.83%[0m) [0.94% of initial]
[Iter 1750] Gaussian 0 vs 1:
  Original Loss: 0.0021193
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0021193 (Pseudo: 0.00%)
[Iter 1750] Gaussian 1 vs 0:
  Original Loss: 0.0021068
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0021068 (Pseudo: 0.00%)
[Iter 1760/20000] Loss: 0.0026394 (Best: 0.0021193 @iter1750) ([91m↑11.89%[0m) [1.05% of initial]
[Iter 1760] Gaussian 0 vs 1:
  Original Loss: 0.0025693
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0025693 (Pseudo: 0.00%)
[Iter 1760] Gaussian 1 vs 0:
  Original Loss: 0.0025467
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0025467 (Pseudo: 0.00%)
[Iter 1770/20000] Loss: 0.0024132 (Best: 0.0021016 @iter1762) ([92m↓8.57%[0m) [0.96% of initial]
[Iter 1770] Gaussian 0 vs 1:
  Original Loss: 0.0023258
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0023258 (Pseudo: 0.00%)
[Iter 1770] Gaussian 1 vs 0:
  Original Loss: 0.0024920
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0024920 (Pseudo: 0.00%)
[Iter 1780/20000] Loss: 0.0025756 (Best: 0.0020700 @iter1771) ([91m↑6.73%[0m) [1.02% of initial]
[Iter 1780] Gaussian 0 vs 1:
  Original Loss: 0.0028361
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0028361 (Pseudo: 0.00%)
[Iter 1780] Gaussian 1 vs 0:
  Original Loss: 0.0026267
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0026267 (Pseudo: 0.00%)
[Iter 1790/20000] Loss: 0.0021820 (Best: 0.0019079 @iter1786) ([92m↓15.28%[0m) [0.87% of initial]
[Iter 1790] Gaussian 0 vs 1:
  Original Loss: 0.0021447
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0021447 (Pseudo: 0.00%)
[Iter 1790] Gaussian 1 vs 0:
  Original Loss: 0.0022336
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0022336 (Pseudo: 0.00%)
Iter:1799, L1 loss=0.001723, Total loss=0.00191, Time:14
[Iter 1800/20000] Loss: 0.0021893 (Best: 0.0019079 @iter1786) ([91m↑0.34%[0m) [0.87% of initial]
[Iter 1800] Gaussian 0 vs 1:
  Original Loss: 0.0021596
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0021596 (Pseudo: 0.00%)
[Iter 1800] Gaussian 1 vs 0:
  Original Loss: 0.0022137
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0022137 (Pseudo: 0.00%)
[Iter 1810/20000] Loss: 0.0076617 (Best: 0.0019079 @iter1786) ([91m↑249.95%[0m) [3.04% of initial]
[Iter 1810] Gaussian 0 vs 1:
  Original Loss: 0.0067294
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0067294 (Pseudo: 0.00%)
[Iter 1810] Gaussian 1 vs 0:
  Original Loss: 0.0065935
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0065935 (Pseudo: 0.00%)
[Iter 1820/20000] Loss: 0.0046328 (Best: 0.0019079 @iter1786) ([92m↓39.53%[0m) [1.84% of initial]
[Iter 1820] Gaussian 0 vs 1:
  Original Loss: 0.0046501
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0046501 (Pseudo: 0.00%)
[Iter 1820] Gaussian 1 vs 0:
  Original Loss: 0.0044780
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0044780 (Pseudo: 0.00%)
[Iter 1830/20000] Loss: 0.0040344 (Best: 0.0019079 @iter1786) ([92m↓12.92%[0m) [1.60% of initial]
[Iter 1830] Gaussian 0 vs 1:
  Original Loss: 0.0038599
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0038599 (Pseudo: 0.00%)
[Iter 1830] Gaussian 1 vs 0:
  Original Loss: 0.0038179
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0038179 (Pseudo: 0.00%)
[Iter 1840/20000] Loss: 0.0027951 (Best: 0.0019079 @iter1786) ([92m↓30.72%[0m) [1.11% of initial]
[Iter 1840] Gaussian 0 vs 1:
  Original Loss: 0.0024393
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0024393 (Pseudo: 0.00%)
[Iter 1840] Gaussian 1 vs 0:
  Original Loss: 0.0024789
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0024789 (Pseudo: 0.00%)
[Iter 1850/20000] Loss: 0.0026447 (Best: 0.0019079 @iter1786) ([92m↓5.38%[0m) [1.05% of initial]
[Iter 1850] Gaussian 0 vs 1:
  Original Loss: 0.0024306
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0024306 (Pseudo: 0.00%)
[Iter 1850] Gaussian 1 vs 0:
  Original Loss: 0.0025061
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0025061 (Pseudo: 0.00%)
[Iter 1860/20000] Loss: 0.0024080 (Best: 0.0019079 @iter1786) ([92m↓8.95%[0m) [0.96% of initial]
[Iter 1860] Gaussian 0 vs 1:
  Original Loss: 0.0022176
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0022176 (Pseudo: 0.00%)
[Iter 1860] Gaussian 1 vs 0:
  Original Loss: 0.0023248
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0023248 (Pseudo: 0.00%)
[Iter 1870/20000] Loss: 0.0022293 (Best: 0.0018235 @iter1867) ([92m↓7.42%[0m) [0.89% of initial]
[Iter 1870] Gaussian 0 vs 1:
  Original Loss: 0.0020194
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0020194 (Pseudo: 0.00%)
[Iter 1870] Gaussian 1 vs 0:
  Original Loss: 0.0020681
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0020681 (Pseudo: 0.00%)
[Iter 1880/20000] Loss: 0.0021073 (Best: 0.0018235 @iter1867) ([92m↓5.47%[0m) [0.84% of initial]
[Iter 1880] Gaussian 0 vs 1:
  Original Loss: 0.0018463
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0018463 (Pseudo: 0.00%)
[Iter 1880] Gaussian 1 vs 0:
  Original Loss: 0.0018810
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0018810 (Pseudo: 0.00%)
[Iter 1890/20000] Loss: 0.0018982 (Best: 0.0017544 @iter1890) ([92m↓9.92%[0m) [0.75% of initial]
[Iter 1890] Gaussian 0 vs 1:
  Original Loss: 0.0017544
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0017544 (Pseudo: 0.00%)
[Iter 1890] Gaussian 1 vs 0:
  Original Loss: 0.0018244
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0018244 (Pseudo: 0.00%)
Iter:1899, L1 loss=0.001802, Total loss=0.001931, Time:14
[Iter 1900/20000] Loss: 0.0020137 (Best: 0.0016192 @iter1891) ([91m↑6.09%[0m) [0.80% of initial]
[Iter 1900] Gaussian 0 vs 1:
  Original Loss: 0.0018894
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0018894 (Pseudo: 0.00%)
[Iter 1900] Gaussian 1 vs 0:
  Original Loss: 0.0019089
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0019089 (Pseudo: 0.00%)
[Iter 1910/20000] Loss: 0.0020443 (Best: 0.0016192 @iter1891) ([91m↑1.52%[0m) [0.81% of initial]
[Iter 1910] Gaussian 0 vs 1:
  Original Loss: 0.0018147
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0018147 (Pseudo: 0.00%)
[Iter 1910] Gaussian 1 vs 0:
  Original Loss: 0.0018719
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0018719 (Pseudo: 0.00%)
[Iter 1920/20000] Loss: 0.0020844 (Best: 0.0016192 @iter1891) ([91m↑1.96%[0m) [0.83% of initial]
[Iter 1920] Gaussian 0 vs 1:
  Original Loss: 0.0021109
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0021109 (Pseudo: 0.00%)
[Iter 1920] Gaussian 1 vs 0:
  Original Loss: 0.0021875
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0021875 (Pseudo: 0.00%)
[Iter 1930/20000] Loss: 0.0017584 (Best: 0.0015703 @iter1930) ([92m↓15.64%[0m) [0.70% of initial]
[Iter 1930] Gaussian 0 vs 1:
  Original Loss: 0.0015703
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0015703 (Pseudo: 0.00%)
[Iter 1930] Gaussian 1 vs 0:
  Original Loss: 0.0016237
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0016237 (Pseudo: 0.00%)
[Iter 1940/20000] Loss: 0.0019079 (Best: 0.0015630 @iter1939) ([91m↑8.50%[0m) [0.76% of initial]
[Iter 1940] Gaussian 0 vs 1:
  Original Loss: 0.0019935
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0019935 (Pseudo: 0.00%)
[Iter 1940] Gaussian 1 vs 0:
  Original Loss: 0.0020324
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0020324 (Pseudo: 0.00%)
[Iter 1950/20000] Loss: 0.0020725 (Best: 0.0015630 @iter1939) ([91m↑8.63%[0m) [0.82% of initial]
[Iter 1950] Gaussian 0 vs 1:
  Original Loss: 0.0018989
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0018989 (Pseudo: 0.00%)
[Iter 1950] Gaussian 1 vs 0:
  Original Loss: 0.0018829
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0018829 (Pseudo: 0.00%)
[Iter 1960/20000] Loss: 0.0018455 (Best: 0.0015630 @iter1939) ([92m↓10.95%[0m) [0.73% of initial]
[Iter 1960] Gaussian 0 vs 1:
  Original Loss: 0.0017291
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0017291 (Pseudo: 0.00%)
[Iter 1960] Gaussian 1 vs 0:
  Original Loss: 0.0017443
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0017443 (Pseudo: 0.00%)
[Iter 1970/20000] Loss: 0.0016843 (Best: 0.0015053 @iter1963) ([92m↓8.74%[0m) [0.67% of initial]
[Iter 1970] Gaussian 0 vs 1:
  Original Loss: 0.0016038
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0016038 (Pseudo: 0.00%)
[Iter 1970] Gaussian 1 vs 0:
  Original Loss: 0.0016612
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0016612 (Pseudo: 0.00%)
[Iter 1980/20000] Loss: 0.0020370 (Best: 0.0015053 @iter1963) ([91m↑20.94%[0m) [0.81% of initial]
[Iter 1980] Gaussian 0 vs 1:
  Original Loss: 0.0024244
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0024244 (Pseudo: 0.00%)
[Iter 1980] Gaussian 1 vs 0:
  Original Loss: 0.0024139
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0024139 (Pseudo: 0.00%)
[Iter 1990/20000] Loss: 0.0017610 (Best: 0.0015053 @iter1963) ([92m↓13.55%[0m) [0.70% of initial]
[Iter 1990] Gaussian 0 vs 1:
  Original Loss: 0.0016106
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0016106 (Pseudo: 0.00%)
[Iter 1990] Gaussian 1 vs 0:
  Original Loss: 0.0016746
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0016746 (Pseudo: 0.00%)
Iter:1999, L1 loss=0.001565, Total loss=0.001666, Time:14
[Iter 2000/20000] Loss: 0.0018758 (Best: 0.0014433 @iter1996) ([91m↑6.51%[0m) [0.75% of initial]
Testing Speed: 232.01330245947253 fps
Testing Time: 0.21550488471984863 s

[ITER 2000] Evaluating test: SSIM = 0.8421867966651917, PSNR = 17.482737598419188
Testing Speed: 271.1191743336709 fps
Testing Time: 0.011065244674682617 s

[ITER 2000] Evaluating train: SSIM = 0.9999533891677856, PSNR = 49.29673131306966
Iter:2000, total_points:42649
[Iter 2000] Gaussian 0 vs 1:
  Original Loss: 0.0020144
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0020144 (Pseudo: 0.00%)
[Iter 2000] Gaussian 1 vs 0:
  Original Loss: 0.0020813
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0020813 (Pseudo: 0.00%)
[Iter 2010/20000] Loss: 0.0068129 (Best: 0.0014433 @iter1996) ([91m↑263.21%[0m) [2.71% of initial]
[Iter 2010] Gaussian 0 vs 1:
  Original Loss: 0.0061866
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0061866 (Pseudo: 0.00%)
[Iter 2010] Gaussian 1 vs 0:
  Original Loss: 0.0060190
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0060190 (Pseudo: 0.00%)
[Iter 2020/20000] Loss: 0.0037290 (Best: 0.0014433 @iter1996) ([92m↓45.27%[0m) [1.48% of initial]
[Iter 2020] Gaussian 0 vs 1:
  Original Loss: 0.0033068
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0033068 (Pseudo: 0.00%)
[Iter 2020] Gaussian 1 vs 0:
  Original Loss: 0.0033086
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0033086 (Pseudo: 0.00%)
[Iter 2030/20000] Loss: 0.0027863 (Best: 0.0014433 @iter1996) ([92m↓25.28%[0m) [1.11% of initial]
[Iter 2030] Gaussian 0 vs 1:
  Original Loss: 0.0024480
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0024480 (Pseudo: 0.00%)
[Iter 2030] Gaussian 1 vs 0:
  Original Loss: 0.0024404
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0024404 (Pseudo: 0.00%)
[Iter 2040/20000] Loss: 0.0024637 (Best: 0.0014433 @iter1996) ([92m↓11.58%[0m) [0.98% of initial]
[Iter 2040] Gaussian 0 vs 1:
  Original Loss: 0.0026871
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0026871 (Pseudo: 0.00%)
[Iter 2040] Gaussian 1 vs 0:
  Original Loss: 0.0027984
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0027984 (Pseudo: 0.00%)
[Iter 2050/20000] Loss: 0.0020773 (Best: 0.0014433 @iter1996) ([92m↓15.68%[0m) [0.83% of initial]
[Iter 2050] Gaussian 0 vs 1:
  Original Loss: 0.0017442
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0017442 (Pseudo: 0.00%)
[Iter 2050] Gaussian 1 vs 0:
  Original Loss: 0.0018063
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0018063 (Pseudo: 0.00%)
[Iter 2060/20000] Loss: 0.0017279 (Best: 0.0014433 @iter1996) ([92m↓16.82%[0m) [0.69% of initial]
[Iter 2060] Gaussian 0 vs 1:
  Original Loss: 0.0015585
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0015585 (Pseudo: 0.00%)
[Iter 2060] Gaussian 1 vs 0:
  Original Loss: 0.0015658
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0015658 (Pseudo: 0.00%)
[Iter 2070/20000] Loss: 0.0019507 (Best: 0.0014433 @iter1996) ([91m↑12.90%[0m) [0.78% of initial]
[Iter 2070] Gaussian 0 vs 1:
  Original Loss: 0.0021781
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0021781 (Pseudo: 0.00%)
[Iter 2070] Gaussian 1 vs 0:
  Original Loss: 0.0023145
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0023145 (Pseudo: 0.00%)
[Iter 2080/20000] Loss: 0.0018921 (Best: 0.0014433 @iter1996) ([92m↓3.00%[0m) [0.75% of initial]
[Iter 2080] Gaussian 0 vs 1:
  Original Loss: 0.0020830
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0020830 (Pseudo: 0.00%)
[Iter 2080] Gaussian 1 vs 0:
  Original Loss: 0.0021600
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0021600 (Pseudo: 0.00%)
[Iter 2090/20000] Loss: 0.0018262 (Best: 0.0014433 @iter1996) ([92m↓3.49%[0m) [0.73% of initial]
[Iter 2090] Gaussian 0 vs 1:
  Original Loss: 0.0020341
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0020341 (Pseudo: 0.00%)
[Iter 2090] Gaussian 1 vs 0:
  Original Loss: 0.0021387
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0021387 (Pseudo: 0.00%)
Iter:2099, L1 loss=0.001594, Total loss=0.001751, Time:14
[Iter 2100/20000] Loss: 0.0017001 (Best: 0.0014433 @iter1996) ([92m↓6.90%[0m) [0.68% of initial]
[Iter 2100] Gaussian 0 vs 1:
  Original Loss: 0.0015966
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0015966 (Pseudo: 0.00%)
[Iter 2100] Gaussian 1 vs 0:
  Original Loss: 0.0016208
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0016208 (Pseudo: 0.00%)
[Iter 2110/20000] Loss: 0.0016327 (Best: 0.0014320 @iter2101) ([92m↓3.96%[0m) [0.65% of initial]
[Iter 2110] Gaussian 0 vs 1:
  Original Loss: 0.0015009
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0015009 (Pseudo: 0.00%)
[Iter 2110] Gaussian 1 vs 0:
  Original Loss: 0.0014759
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0014759 (Pseudo: 0.00%)
[Iter 2120/20000] Loss: 0.0014301 (Best: 0.0013033 @iter2120) ([92m↓12.41%[0m) [0.57% of initial]
[Iter 2120] Gaussian 0 vs 1:
  Original Loss: 0.0013033
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0013033 (Pseudo: 0.00%)
[Iter 2120] Gaussian 1 vs 0:
  Original Loss: 0.0013208
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0013208 (Pseudo: 0.00%)
[Iter 2130/20000] Loss: 0.0015714 (Best: 0.0012233 @iter2125) ([91m↑9.88%[0m) [0.62% of initial]
[Iter 2130] Gaussian 0 vs 1:
  Original Loss: 0.0016318
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0016318 (Pseudo: 0.00%)
[Iter 2130] Gaussian 1 vs 0:
  Original Loss: 0.0017009
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0017009 (Pseudo: 0.00%)
[Iter 2140/20000] Loss: 0.0017211 (Best: 0.0012233 @iter2125) ([91m↑9.52%[0m) [0.68% of initial]
[Iter 2140] Gaussian 0 vs 1:
  Original Loss: 0.0018433
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0018433 (Pseudo: 0.00%)
[Iter 2140] Gaussian 1 vs 0:
  Original Loss: 0.0019330
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0019330 (Pseudo: 0.00%)
[Iter 2150/20000] Loss: 0.0017331 (Best: 0.0012233 @iter2125) ([91m↑0.70%[0m) [0.69% of initial]
[Iter 2150] Gaussian 0 vs 1:
  Original Loss: 0.0015225
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0015225 (Pseudo: 0.00%)
[Iter 2150] Gaussian 1 vs 0:
  Original Loss: 0.0016663
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0016663 (Pseudo: 0.00%)
[Iter 2160/20000] Loss: 0.0015552 (Best: 0.0012233 @iter2125) ([92m↓10.27%[0m) [0.62% of initial]
[Iter 2160] Gaussian 0 vs 1:
  Original Loss: 0.0013719
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0013719 (Pseudo: 0.00%)
[Iter 2160] Gaussian 1 vs 0:
  Original Loss: 0.0014847
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0014847 (Pseudo: 0.00%)
[Iter 2170/20000] Loss: 0.0016037 (Best: 0.0012233 @iter2125) ([91m↑3.12%[0m) [0.64% of initial]
[Iter 2170] Gaussian 0 vs 1:
  Original Loss: 0.0016857
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0016857 (Pseudo: 0.00%)
[Iter 2170] Gaussian 1 vs 0:
  Original Loss: 0.0017707
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0017707 (Pseudo: 0.00%)
[Iter 2180/20000] Loss: 0.0013268 (Best: 0.0011927 @iter2180) ([92m↓17.27%[0m) [0.53% of initial]
[Iter 2180] Gaussian 0 vs 1:
  Original Loss: 0.0011927
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0011927 (Pseudo: 0.00%)
[Iter 2180] Gaussian 1 vs 0:
  Original Loss: 0.0012543
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0012543 (Pseudo: 0.00%)
[Iter 2190/20000] Loss: 0.0015875 (Best: 0.0011927 @iter2180) ([91m↑19.66%[0m) [0.63% of initial]
[Iter 2190] Gaussian 0 vs 1:
  Original Loss: 0.0018445
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0018445 (Pseudo: 0.00%)
[Iter 2190] Gaussian 1 vs 0:
  Original Loss: 0.0019063
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0019063 (Pseudo: 0.00%)
Iter:2199, L1 loss=0.001456, Total loss=0.001527, Time:14
[Iter 2200/20000] Loss: 0.0016030 (Best: 0.0011927 @iter2180) ([91m↑0.98%[0m) [0.64% of initial]
[Iter 2200] Gaussian 0 vs 1:
  Original Loss: 0.0017738
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0017738 (Pseudo: 0.00%)
[Iter 2200] Gaussian 1 vs 0:
  Original Loss: 0.0018243
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0018243 (Pseudo: 0.00%)
[Iter 2210/20000] Loss: 0.0077410 (Best: 0.0011927 @iter2180) ([91m↑382.90%[0m) [3.08% of initial]
[Iter 2210] Gaussian 0 vs 1:
  Original Loss: 0.0077021
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0077021 (Pseudo: 0.00%)
[Iter 2210] Gaussian 1 vs 0:
  Original Loss: 0.0071833
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0071833 (Pseudo: 0.00%)
[Iter 2220/20000] Loss: 0.0042461 (Best: 0.0011927 @iter2180) ([92m↓45.15%[0m) [1.69% of initial]
[Iter 2220] Gaussian 0 vs 1:
  Original Loss: 0.0038329
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0038329 (Pseudo: 0.00%)
[Iter 2220] Gaussian 1 vs 0:
  Original Loss: 0.0038107
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0038107 (Pseudo: 0.00%)
[Iter 2230/20000] Loss: 0.0026631 (Best: 0.0011927 @iter2180) ([92m↓37.28%[0m) [1.06% of initial]
[Iter 2230] Gaussian 0 vs 1:
  Original Loss: 0.0022700
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0022700 (Pseudo: 0.00%)
[Iter 2230] Gaussian 1 vs 0:
  Original Loss: 0.0023216
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0023216 (Pseudo: 0.00%)
[Iter 2240/20000] Loss: 0.0023037 (Best: 0.0011927 @iter2180) ([92m↓13.49%[0m) [0.92% of initial]
[Iter 2240] Gaussian 0 vs 1:
  Original Loss: 0.0020780
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0020780 (Pseudo: 0.00%)
[Iter 2240] Gaussian 1 vs 0:
  Original Loss: 0.0021179
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0021179 (Pseudo: 0.00%)
[Iter 2250/20000] Loss: 0.0021170 (Best: 0.0011927 @iter2180) ([92m↓8.11%[0m) [0.84% of initial]
[Iter 2250] Gaussian 0 vs 1:
  Original Loss: 0.0022340
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0022340 (Pseudo: 0.00%)
[Iter 2250] Gaussian 1 vs 0:
  Original Loss: 0.0023895
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0023895 (Pseudo: 0.00%)
[Iter 2260/20000] Loss: 0.0017067 (Best: 0.0011927 @iter2180) ([92m↓19.38%[0m) [0.68% of initial]
[Iter 2260] Gaussian 0 vs 1:
  Original Loss: 0.0015235
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0015235 (Pseudo: 0.00%)
[Iter 2260] Gaussian 1 vs 0:
  Original Loss: 0.0015706
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0015706 (Pseudo: 0.00%)
[Iter 2270/20000] Loss: 0.0018361 (Best: 0.0011927 @iter2180) ([91m↑7.58%[0m) [0.73% of initial]
[Iter 2270] Gaussian 0 vs 1:
  Original Loss: 0.0020544
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0020544 (Pseudo: 0.00%)
[Iter 2270] Gaussian 1 vs 0:
  Original Loss: 0.0020416
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0020416 (Pseudo: 0.00%)
[Iter 2280/20000] Loss: 0.0014657 (Best: 0.0011927 @iter2180) ([92m↓20.17%[0m) [0.58% of initial]
[Iter 2280] Gaussian 0 vs 1:
  Original Loss: 0.0013803
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0013803 (Pseudo: 0.00%)
[Iter 2280] Gaussian 1 vs 0:
  Original Loss: 0.0013969
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0013969 (Pseudo: 0.00%)
[Iter 2290/20000] Loss: 0.0014081 (Best: 0.0011847 @iter2287) ([92m↓3.93%[0m) [0.56% of initial]
[Iter 2290] Gaussian 0 vs 1:
  Original Loss: 0.0012662
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0012662 (Pseudo: 0.00%)
[Iter 2290] Gaussian 1 vs 0:
  Original Loss: 0.0012692
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0012692 (Pseudo: 0.00%)
Iter:2299, L1 loss=0.001383, Total loss=0.001363, Time:15
[Iter 2300/20000] Loss: 0.0016795 (Best: 0.0011847 @iter2287) ([91m↑19.27%[0m) [0.67% of initial]
[Iter 2300] Gaussian 0 vs 1:
  Original Loss: 0.0018161
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0018161 (Pseudo: 0.00%)
[Iter 2300] Gaussian 1 vs 0:
  Original Loss: 0.0018484
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0018484 (Pseudo: 0.00%)
[Iter 2310/20000] Loss: 0.0015421 (Best: 0.0011847 @iter2287) ([92m↓8.18%[0m) [0.61% of initial]
[Iter 2310] Gaussian 0 vs 1:
  Original Loss: 0.0014799
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0014799 (Pseudo: 0.00%)
[Iter 2310] Gaussian 1 vs 0:
  Original Loss: 0.0014633
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0014633 (Pseudo: 0.00%)
[Iter 2320/20000] Loss: 0.0013266 (Best: 0.0011847 @iter2287) ([92m↓13.98%[0m) [0.53% of initial]
[Iter 2320] Gaussian 0 vs 1:
  Original Loss: 0.0012061
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0012061 (Pseudo: 0.00%)
[Iter 2320] Gaussian 1 vs 0:
  Original Loss: 0.0012197
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0012197 (Pseudo: 0.00%)
[Iter 2330/20000] Loss: 0.0012977 (Best: 0.0011217 @iter2327) ([92m↓2.17%[0m) [0.52% of initial]
[Iter 2330] Gaussian 0 vs 1:
  Original Loss: 0.0012593
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0012593 (Pseudo: 0.00%)
[Iter 2330] Gaussian 1 vs 0:
  Original Loss: 0.0013112
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0013112 (Pseudo: 0.00%)
[Iter 2340/20000] Loss: 0.0013585 (Best: 0.0011087 @iter2338) ([91m↑4.69%[0m) [0.54% of initial]
[Iter 2340] Gaussian 0 vs 1:
  Original Loss: 0.0012499
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0012499 (Pseudo: 0.00%)
[Iter 2340] Gaussian 1 vs 0:
  Original Loss: 0.0012602
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0012602 (Pseudo: 0.00%)
[Iter 2350/20000] Loss: 0.0014826 (Best: 0.0011087 @iter2338) ([91m↑9.14%[0m) [0.59% of initial]
[Iter 2350] Gaussian 0 vs 1:
  Original Loss: 0.0013784
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0013784 (Pseudo: 0.00%)
[Iter 2350] Gaussian 1 vs 0:
  Original Loss: 0.0014920
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0014920 (Pseudo: 0.00%)
[Iter 2360/20000] Loss: 0.0012897 (Best: 0.0011040 @iter2359) ([92m↓13.01%[0m) [0.51% of initial]
[Iter 2360] Gaussian 0 vs 1:
  Original Loss: 0.0013252
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0013252 (Pseudo: 0.00%)
[Iter 2360] Gaussian 1 vs 0:
  Original Loss: 0.0013850
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0013850 (Pseudo: 0.00%)
[Iter 2370/20000] Loss: 0.0014302 (Best: 0.0011040 @iter2359) ([91m↑10.90%[0m) [0.57% of initial]
[Iter 2370] Gaussian 0 vs 1:
  Original Loss: 0.0012327
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0012327 (Pseudo: 0.00%)
[Iter 2370] Gaussian 1 vs 0:
  Original Loss: 0.0012409
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0012409 (Pseudo: 0.00%)
[Iter 2380/20000] Loss: 0.0015188 (Best: 0.0011040 @iter2359) ([91m↑6.19%[0m) [0.60% of initial]
[Iter 2380] Gaussian 0 vs 1:
  Original Loss: 0.0016113
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0016113 (Pseudo: 0.00%)
[Iter 2380] Gaussian 1 vs 0:
  Original Loss: 0.0016115
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0016115 (Pseudo: 0.00%)
[Iter 2390/20000] Loss: 0.0016539 (Best: 0.0011040 @iter2359) ([91m↑8.90%[0m) [0.66% of initial]
[Iter 2390] Gaussian 0 vs 1:
  Original Loss: 0.0018041
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0018041 (Pseudo: 0.00%)
[Iter 2390] Gaussian 1 vs 0:
  Original Loss: 0.0018221
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0018221 (Pseudo: 0.00%)
Iter:2399, L1 loss=0.001238, Total loss=0.001247, Time:15
[Iter 2400/20000] Loss: 0.0013403 (Best: 0.0011040 @iter2359) ([92m↓18.96%[0m) [0.53% of initial]
[Iter 2400] Gaussian 0 vs 1:
  Original Loss: 0.0012132
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0012132 (Pseudo: 0.00%)
[Iter 2400] Gaussian 1 vs 0:
  Original Loss: 0.0011974
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0011974 (Pseudo: 0.00%)
[Iter 2410/20000] Loss: 0.0058970 (Best: 0.0011040 @iter2359) ([91m↑339.99%[0m) [2.34% of initial]
[Iter 2410] Gaussian 0 vs 1:
  Original Loss: 0.0050345
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0050345 (Pseudo: 0.00%)
[Iter 2410] Gaussian 1 vs 0:
  Original Loss: 0.0049764
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0049764 (Pseudo: 0.00%)
[Iter 2420/20000] Loss: 0.0034625 (Best: 0.0011040 @iter2359) ([92m↓41.28%[0m) [1.38% of initial]
[Iter 2420] Gaussian 0 vs 1:
  Original Loss: 0.0030360
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0030360 (Pseudo: 0.00%)
[Iter 2420] Gaussian 1 vs 0:
  Original Loss: 0.0028888
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0028888 (Pseudo: 0.00%)
[Iter 2430/20000] Loss: 0.0025139 (Best: 0.0011040 @iter2359) ([92m↓27.40%[0m) [1.00% of initial]
[Iter 2430] Gaussian 0 vs 1:
  Original Loss: 0.0026456
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0026456 (Pseudo: 0.00%)
[Iter 2430] Gaussian 1 vs 0:
  Original Loss: 0.0026165
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0026165 (Pseudo: 0.00%)
[Iter 2440/20000] Loss: 0.0019958 (Best: 0.0011040 @iter2359) ([92m↓20.61%[0m) [0.79% of initial]
[Iter 2440] Gaussian 0 vs 1:
  Original Loss: 0.0016754
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0016754 (Pseudo: 0.00%)
[Iter 2440] Gaussian 1 vs 0:
  Original Loss: 0.0016822
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0016822 (Pseudo: 0.00%)
[Iter 2450/20000] Loss: 0.0019416 (Best: 0.0011040 @iter2359) ([92m↓2.72%[0m) [0.77% of initial]
[Iter 2450] Gaussian 0 vs 1:
  Original Loss: 0.0020349
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0020349 (Pseudo: 0.00%)
[Iter 2450] Gaussian 1 vs 0:
  Original Loss: 0.0020311
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0020311 (Pseudo: 0.00%)
[Iter 2460/20000] Loss: 0.0016926 (Best: 0.0011040 @iter2359) ([92m↓12.82%[0m) [0.67% of initial]
[Iter 2460] Gaussian 0 vs 1:
  Original Loss: 0.0015314
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0015314 (Pseudo: 0.00%)
[Iter 2460] Gaussian 1 vs 0:
  Original Loss: 0.0015056
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0015056 (Pseudo: 0.00%)
[Iter 2470/20000] Loss: 0.0016547 (Best: 0.0011040 @iter2359) ([92m↓2.24%[0m) [0.66% of initial]
[Iter 2470] Gaussian 0 vs 1:
  Original Loss: 0.0016678
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0016678 (Pseudo: 0.00%)
[Iter 2470] Gaussian 1 vs 0:
  Original Loss: 0.0016647
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0016647 (Pseudo: 0.00%)
[Iter 2480/20000] Loss: 0.0016521 (Best: 0.0011040 @iter2359) ([92m↓0.16%[0m) [0.66% of initial]
[Iter 2480] Gaussian 0 vs 1:
  Original Loss: 0.0014433
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0014433 (Pseudo: 0.00%)
[Iter 2480] Gaussian 1 vs 0:
  Original Loss: 0.0014422
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0014422 (Pseudo: 0.00%)
[Iter 2490/20000] Loss: 0.0014473 (Best: 0.0011040 @iter2359) ([92m↓12.40%[0m) [0.58% of initial]
[Iter 2490] Gaussian 0 vs 1:
  Original Loss: 0.0013627
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0013627 (Pseudo: 0.00%)
[Iter 2490] Gaussian 1 vs 0:
  Original Loss: 0.0015037
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0015037 (Pseudo: 0.00%)
Iter:2499, L1 loss=0.001207, Total loss=0.001236, Time:16
[Iter 2500/20000] Loss: 0.0012990 (Best: 0.0011040 @iter2359) ([92m↓10.25%[0m) [0.52% of initial]
[Iter 2500] Gaussian 0 vs 1:
  Original Loss: 0.0011932
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0011932 (Pseudo: 0.00%)
[Iter 2500] Gaussian 1 vs 0:
  Original Loss: 0.0012108
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0012108 (Pseudo: 0.00%)
[Iter 2510/20000] Loss: 0.0013522 (Best: 0.0010299 @iter2504) ([91m↑4.09%[0m) [0.54% of initial]
[Iter 2510] Gaussian 0 vs 1:
  Original Loss: 0.0015869
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0015869 (Pseudo: 0.00%)
[Iter 2510] Gaussian 1 vs 0:
  Original Loss: 0.0015752
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0015752 (Pseudo: 0.00%)
[Iter 2520/20000] Loss: 0.0012072 (Best: 0.0009926 @iter2519) ([92m↓10.72%[0m) [0.48% of initial]
[Iter 2520] Gaussian 0 vs 1:
  Original Loss: 0.0012450
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0012450 (Pseudo: 0.00%)
[Iter 2520] Gaussian 1 vs 0:
  Original Loss: 0.0012666
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0012666 (Pseudo: 0.00%)
[Iter 2530/20000] Loss: 0.0010637 (Best: 0.0009588 @iter2528) ([92m↓11.89%[0m) [0.42% of initial]
[Iter 2530] Gaussian 0 vs 1:
  Original Loss: 0.0009593
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0009593 (Pseudo: 0.00%)
[Iter 2530] Gaussian 1 vs 0:
  Original Loss: 0.0009933
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0009933 (Pseudo: 0.00%)
[Iter 2540/20000] Loss: 0.0011748 (Best: 0.0009588 @iter2528) ([91m↑10.45%[0m) [0.47% of initial]
[Iter 2540] Gaussian 0 vs 1:
  Original Loss: 0.0011936
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0011936 (Pseudo: 0.00%)
[Iter 2540] Gaussian 1 vs 0:
  Original Loss: 0.0011846
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0011846 (Pseudo: 0.00%)
[Iter 2550/20000] Loss: 0.0014118 (Best: 0.0009588 @iter2528) ([91m↑20.17%[0m) [0.56% of initial]
[Iter 2550] Gaussian 0 vs 1:
  Original Loss: 0.0015145
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0015145 (Pseudo: 0.00%)
[Iter 2550] Gaussian 1 vs 0:
  Original Loss: 0.0014685
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0014685 (Pseudo: 0.00%)
[Iter 2560/20000] Loss: 0.0011933 (Best: 0.0009588 @iter2528) ([92m↓15.48%[0m) [0.47% of initial]
[Iter 2560] Gaussian 0 vs 1:
  Original Loss: 0.0010902
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0010902 (Pseudo: 0.00%)
[Iter 2560] Gaussian 1 vs 0:
  Original Loss: 0.0010795
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0010795 (Pseudo: 0.00%)
[Iter 2570/20000] Loss: 0.0014361 (Best: 0.0009588 @iter2528) ([91m↑20.34%[0m) [0.57% of initial]
[Iter 2570] Gaussian 0 vs 1:
  Original Loss: 0.0015116
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0015116 (Pseudo: 0.00%)
[Iter 2570] Gaussian 1 vs 0:
  Original Loss: 0.0014185
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0014185 (Pseudo: 0.00%)
[Iter 2580/20000] Loss: 0.0012441 (Best: 0.0009550 @iter2578) ([92m↓13.36%[0m) [0.49% of initial]
[Iter 2580] Gaussian 0 vs 1:
  Original Loss: 0.0012727
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0012727 (Pseudo: 0.00%)
[Iter 2580] Gaussian 1 vs 0:
  Original Loss: 0.0012890
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0012890 (Pseudo: 0.00%)
[Iter 2590/20000] Loss: 0.0013065 (Best: 0.0009482 @iter2584) ([91m↑5.02%[0m) [0.52% of initial]
[Iter 2590] Gaussian 0 vs 1:
  Original Loss: 0.0013726
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0013726 (Pseudo: 0.00%)
[Iter 2590] Gaussian 1 vs 0:
  Original Loss: 0.0013809
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0013809 (Pseudo: 0.00%)
Iter:2599, L1 loss=0.001045, Total loss=0.001041, Time:15
[Iter 2600/20000] Loss: 0.0012026 (Best: 0.0009432 @iter2594) ([92m↓7.96%[0m) [0.48% of initial]
[Iter 2600] Gaussian 0 vs 1:
  Original Loss: 0.0013100
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0013100 (Pseudo: 0.00%)
[Iter 2600] Gaussian 1 vs 0:
  Original Loss: 0.0013116
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0013116 (Pseudo: 0.00%)
[Iter 2610/20000] Loss: 0.0060346 (Best: 0.0009432 @iter2594) ([91m↑401.79%[0m) [2.40% of initial]
[Iter 2610] Gaussian 0 vs 1:
  Original Loss: 0.0054844
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0054844 (Pseudo: 0.00%)
[Iter 2610] Gaussian 1 vs 0:
  Original Loss: 0.0054556
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0054556 (Pseudo: 0.00%)
[Iter 2620/20000] Loss: 0.0033043 (Best: 0.0009432 @iter2594) ([92m↓45.24%[0m) [1.31% of initial]
[Iter 2620] Gaussian 0 vs 1:
  Original Loss: 0.0028631
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0028631 (Pseudo: 0.00%)
[Iter 2620] Gaussian 1 vs 0:
  Original Loss: 0.0027693
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0027693 (Pseudo: 0.00%)
[Iter 2630/20000] Loss: 0.0021755 (Best: 0.0009432 @iter2594) ([92m↓34.16%[0m) [0.86% of initial]
[Iter 2630] Gaussian 0 vs 1:
  Original Loss: 0.0021562
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0021562 (Pseudo: 0.00%)
[Iter 2630] Gaussian 1 vs 0:
  Original Loss: 0.0021418
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0021418 (Pseudo: 0.00%)
[Iter 2640/20000] Loss: 0.0017024 (Best: 0.0009432 @iter2594) ([92m↓21.75%[0m) [0.68% of initial]
[Iter 2640] Gaussian 0 vs 1:
  Original Loss: 0.0015691
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0015691 (Pseudo: 0.00%)
[Iter 2640] Gaussian 1 vs 0:
  Original Loss: 0.0016036
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0016036 (Pseudo: 0.00%)
[Iter 2650/20000] Loss: 0.0013940 (Best: 0.0009432 @iter2594) ([92m↓18.12%[0m) [0.55% of initial]
[Iter 2650] Gaussian 0 vs 1:
  Original Loss: 0.0011346
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0011346 (Pseudo: 0.00%)
[Iter 2650] Gaussian 1 vs 0:
  Original Loss: 0.0011632
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0011632 (Pseudo: 0.00%)
[Iter 2660/20000] Loss: 0.0016448 (Best: 0.0009432 @iter2594) ([91m↑17.99%[0m) [0.65% of initial]
[Iter 2660] Gaussian 0 vs 1:
  Original Loss: 0.0014212
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0014212 (Pseudo: 0.00%)
[Iter 2660] Gaussian 1 vs 0:
  Original Loss: 0.0014193
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0014193 (Pseudo: 0.00%)
[Iter 2670/20000] Loss: 0.0015510 (Best: 0.0009432 @iter2594) ([92m↓5.70%[0m) [0.62% of initial]
[Iter 2670] Gaussian 0 vs 1:
  Original Loss: 0.0016066
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0016066 (Pseudo: 0.00%)
[Iter 2670] Gaussian 1 vs 0:
  Original Loss: 0.0016605
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0016605 (Pseudo: 0.00%)
[Iter 2680/20000] Loss: 0.0011958 (Best: 0.0009432 @iter2594) ([92m↓22.90%[0m) [0.48% of initial]
[Iter 2680] Gaussian 0 vs 1:
  Original Loss: 0.0009974
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0009974 (Pseudo: 0.00%)
[Iter 2680] Gaussian 1 vs 0:
  Original Loss: 0.0010313
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0010313 (Pseudo: 0.00%)
[Iter 2690/20000] Loss: 0.0011470 (Best: 0.0009432 @iter2594) ([92m↓4.08%[0m) [0.46% of initial]
[Iter 2690] Gaussian 0 vs 1:
  Original Loss: 0.0009736
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0009736 (Pseudo: 0.00%)
[Iter 2690] Gaussian 1 vs 0:
  Original Loss: 0.0010132
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0010132 (Pseudo: 0.00%)
Iter:2699, L1 loss=0.001131, Total loss=0.001151, Time:16
[Iter 2700/20000] Loss: 0.0014518 (Best: 0.0009432 @iter2594) ([91m↑26.58%[0m) [0.58% of initial]
[Iter 2700] Gaussian 0 vs 1:
  Original Loss: 0.0017480
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0017480 (Pseudo: 0.00%)
[Iter 2700] Gaussian 1 vs 0:
  Original Loss: 0.0017357
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0017357 (Pseudo: 0.00%)
[Iter 2710/20000] Loss: 0.0012212 (Best: 0.0009432 @iter2594) ([92m↓15.89%[0m) [0.49% of initial]
[Iter 2710] Gaussian 0 vs 1:
  Original Loss: 0.0012359
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0012359 (Pseudo: 0.00%)
[Iter 2710] Gaussian 1 vs 0:
  Original Loss: 0.0013093
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0013093 (Pseudo: 0.00%)
[Iter 2720/20000] Loss: 0.0010870 (Best: 0.0009219 @iter2713) ([92m↓10.99%[0m) [0.43% of initial]
[Iter 2720] Gaussian 0 vs 1:
  Original Loss: 0.0010622
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0010622 (Pseudo: 0.00%)
[Iter 2720] Gaussian 1 vs 0:
  Original Loss: 0.0010742
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0010742 (Pseudo: 0.00%)
[Iter 2730/20000] Loss: 0.0009880 (Best: 0.0008396 @iter2725) ([92m↓9.11%[0m) [0.39% of initial]
[Iter 2730] Gaussian 0 vs 1:
  Original Loss: 0.0009129
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0009129 (Pseudo: 0.00%)
[Iter 2730] Gaussian 1 vs 0:
  Original Loss: 0.0009658
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0009658 (Pseudo: 0.00%)
[Iter 2740/20000] Loss: 0.0008605 (Best: 0.0007722 @iter2740) ([92m↓12.90%[0m) [0.34% of initial]
[Iter 2740] Gaussian 0 vs 1:
  Original Loss: 0.0007722
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0007722 (Pseudo: 0.00%)
[Iter 2740] Gaussian 1 vs 0:
  Original Loss: 0.0007967
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0007967 (Pseudo: 0.00%)
[Iter 2750/20000] Loss: 0.0011328 (Best: 0.0007722 @iter2740) ([91m↑31.64%[0m) [0.45% of initial]
[Iter 2750] Gaussian 0 vs 1:
  Original Loss: 0.0013024
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0013024 (Pseudo: 0.00%)
[Iter 2750] Gaussian 1 vs 0:
  Original Loss: 0.0013330
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0013330 (Pseudo: 0.00%)
[Iter 2760/20000] Loss: 0.0012262 (Best: 0.0007722 @iter2740) ([91m↑8.24%[0m) [0.49% of initial]
[Iter 2760] Gaussian 0 vs 1:
  Original Loss: 0.0014355
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0014355 (Pseudo: 0.00%)
[Iter 2760] Gaussian 1 vs 0:
  Original Loss: 0.0013993
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0013993 (Pseudo: 0.00%)
[Iter 2770/20000] Loss: 0.0013790 (Best: 0.0007722 @iter2740) ([91m↑12.46%[0m) [0.55% of initial]
[Iter 2770] Gaussian 0 vs 1:
  Original Loss: 0.0013902
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0013902 (Pseudo: 0.00%)
[Iter 2770] Gaussian 1 vs 0:
  Original Loss: 0.0012885
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0012885 (Pseudo: 0.00%)
[Iter 2780/20000] Loss: 0.0010937 (Best: 0.0007722 @iter2740) ([92m↓20.69%[0m) [0.43% of initial]
[Iter 2780] Gaussian 0 vs 1:
  Original Loss: 0.0012486
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0012486 (Pseudo: 0.00%)
[Iter 2780] Gaussian 1 vs 0:
  Original Loss: 0.0012622
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0012622 (Pseudo: 0.00%)
[Iter 2790/20000] Loss: 0.0011173 (Best: 0.0007722 @iter2740) ([91m↑2.15%[0m) [0.44% of initial]
[Iter 2790] Gaussian 0 vs 1:
  Original Loss: 0.0011622
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0011622 (Pseudo: 0.00%)
[Iter 2790] Gaussian 1 vs 0:
  Original Loss: 0.0012055
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0012055 (Pseudo: 0.00%)
Iter:2799, L1 loss=0.001276, Total loss=0.001244, Time:16
[Iter 2800/20000] Loss: 0.0011216 (Best: 0.0007722 @iter2740) ([91m↑0.39%[0m) [0.45% of initial]
[Iter 2800] Gaussian 0 vs 1:
  Original Loss: 0.0010034
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0010034 (Pseudo: 0.00%)
[Iter 2800] Gaussian 1 vs 0:
  Original Loss: 0.0010019
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0010019 (Pseudo: 0.00%)
[Iter 2810/20000] Loss: 0.0049751 (Best: 0.0007722 @iter2740) ([91m↑343.57%[0m) [1.98% of initial]
[Iter 2810] Gaussian 0 vs 1:
  Original Loss: 0.0040502
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0040502 (Pseudo: 0.00%)
[Iter 2810] Gaussian 1 vs 0:
  Original Loss: 0.0041810
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0041810 (Pseudo: 0.00%)
[Iter 2820/20000] Loss: 0.0027285 (Best: 0.0007722 @iter2740) ([92m↓45.16%[0m) [1.08% of initial]
[Iter 2820] Gaussian 0 vs 1:
  Original Loss: 0.0025647
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0025647 (Pseudo: 0.00%)
[Iter 2820] Gaussian 1 vs 0:
  Original Loss: 0.0025614
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0025614 (Pseudo: 0.00%)
[Iter 2830/20000] Loss: 0.0017562 (Best: 0.0007722 @iter2740) ([92m↓35.63%[0m) [0.70% of initial]
[Iter 2830] Gaussian 0 vs 1:
  Original Loss: 0.0015239
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0015239 (Pseudo: 0.00%)
[Iter 2830] Gaussian 1 vs 0:
  Original Loss: 0.0015956
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0015956 (Pseudo: 0.00%)
[Iter 2840/20000] Loss: 0.0014863 (Best: 0.0007722 @iter2740) ([92m↓15.37%[0m) [0.59% of initial]
[Iter 2840] Gaussian 0 vs 1:
  Original Loss: 0.0015330
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0015330 (Pseudo: 0.00%)
[Iter 2840] Gaussian 1 vs 0:
  Original Loss: 0.0015765
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0015765 (Pseudo: 0.00%)
[Iter 2850/20000] Loss: 0.0012870 (Best: 0.0007722 @iter2740) ([92m↓13.41%[0m) [0.51% of initial]
[Iter 2850] Gaussian 0 vs 1:
  Original Loss: 0.0013079
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0013079 (Pseudo: 0.00%)
[Iter 2850] Gaussian 1 vs 0:
  Original Loss: 0.0013613
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0013613 (Pseudo: 0.00%)
[Iter 2860/20000] Loss: 0.0013964 (Best: 0.0007722 @iter2740) ([91m↑8.50%[0m) [0.55% of initial]
[Iter 2860] Gaussian 0 vs 1:
  Original Loss: 0.0014240
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0014240 (Pseudo: 0.00%)
[Iter 2860] Gaussian 1 vs 0:
  Original Loss: 0.0014602
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0014602 (Pseudo: 0.00%)
[Iter 2870/20000] Loss: 0.0011742 (Best: 0.0007722 @iter2740) ([92m↓15.91%[0m) [0.47% of initial]
[Iter 2870] Gaussian 0 vs 1:
  Original Loss: 0.0011198
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0011198 (Pseudo: 0.00%)
[Iter 2870] Gaussian 1 vs 0:
  Original Loss: 0.0011681
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0011681 (Pseudo: 0.00%)
[Iter 2880/20000] Loss: 0.0011453 (Best: 0.0007722 @iter2740) ([92m↓2.46%[0m) [0.45% of initial]
[Iter 2880] Gaussian 0 vs 1:
  Original Loss: 0.0011432
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0011432 (Pseudo: 0.00%)
[Iter 2880] Gaussian 1 vs 0:
  Original Loss: 0.0012069
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0012069 (Pseudo: 0.00%)
[Iter 2890/20000] Loss: 0.0010832 (Best: 0.0007722 @iter2740) ([92m↓5.42%[0m) [0.43% of initial]
[Iter 2890] Gaussian 0 vs 1:
  Original Loss: 0.0010850
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0010850 (Pseudo: 0.00%)
[Iter 2890] Gaussian 1 vs 0:
  Original Loss: 0.0011750
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0011750 (Pseudo: 0.00%)
Iter:2899, L1 loss=0.0008849, Total loss=0.0008418, Time:17
[Iter 2900/20000] Loss: 0.0010200 (Best: 0.0007722 @iter2740) ([92m↓5.83%[0m) [0.41% of initial]
[Iter 2900] Gaussian 0 vs 1:
  Original Loss: 0.0010767
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0010767 (Pseudo: 0.00%)
[Iter 2900] Gaussian 1 vs 0:
  Original Loss: 0.0011505
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0011505 (Pseudo: 0.00%)
[Iter 2910/20000] Loss: 0.0011039 (Best: 0.0007722 @iter2740) ([91m↑8.22%[0m) [0.44% of initial]
[Iter 2910] Gaussian 0 vs 1:
  Original Loss: 0.0010608
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0010608 (Pseudo: 0.00%)
[Iter 2910] Gaussian 1 vs 0:
  Original Loss: 0.0011112
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0011112 (Pseudo: 0.00%)
[Iter 2920/20000] Loss: 0.0012353 (Best: 0.0007722 @iter2740) ([91m↑11.90%[0m) [0.49% of initial]
[Iter 2920] Gaussian 0 vs 1:
  Original Loss: 0.0012004
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0012004 (Pseudo: 0.00%)
[Iter 2920] Gaussian 1 vs 0:
  Original Loss: 0.0012262
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0012262 (Pseudo: 0.00%)
[Iter 2930/20000] Loss: 0.0011199 (Best: 0.0007722 @iter2740) ([92m↓9.34%[0m) [0.44% of initial]
[Iter 2930] Gaussian 0 vs 1:
  Original Loss: 0.0011639
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0011639 (Pseudo: 0.00%)
[Iter 2930] Gaussian 1 vs 0:
  Original Loss: 0.0012480
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0012480 (Pseudo: 0.00%)
[Iter 2940/20000] Loss: 0.0009712 (Best: 0.0007722 @iter2740) ([92m↓13.28%[0m) [0.39% of initial]
[Iter 2940] Gaussian 0 vs 1:
  Original Loss: 0.0009994
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0009994 (Pseudo: 0.00%)
[Iter 2940] Gaussian 1 vs 0:
  Original Loss: 0.0009678
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0009678 (Pseudo: 0.00%)
[Iter 2950/20000] Loss: 0.0009147 (Best: 0.0007722 @iter2740) ([92m↓5.82%[0m) [0.36% of initial]
[Iter 2950] Gaussian 0 vs 1:
  Original Loss: 0.0007887
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0007887 (Pseudo: 0.00%)
[Iter 2950] Gaussian 1 vs 0:
  Original Loss: 0.0007735
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0007735 (Pseudo: 0.00%)
[Iter 2960/20000] Loss: 0.0009734 (Best: 0.0007722 @iter2740) ([91m↑6.41%[0m) [0.39% of initial]
[Iter 2960] Gaussian 0 vs 1:
  Original Loss: 0.0010943
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0010943 (Pseudo: 0.00%)
[Iter 2960] Gaussian 1 vs 0:
  Original Loss: 0.0011494
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0011494 (Pseudo: 0.00%)
[Iter 2970/20000] Loss: 0.0008822 (Best: 0.0007206 @iter2969) ([92m↓9.36%[0m) [0.35% of initial]
[Iter 2970] Gaussian 0 vs 1:
  Original Loss: 0.0010197
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0010197 (Pseudo: 0.00%)
[Iter 2970] Gaussian 1 vs 0:
  Original Loss: 0.0010784
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0010784 (Pseudo: 0.00%)
[Iter 2980/20000] Loss: 0.0008343 (Best: 0.0007159 @iter2977) ([92m↓5.44%[0m) [0.33% of initial]
[Iter 2980] Gaussian 0 vs 1:
  Original Loss: 0.0008194
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0008194 (Pseudo: 0.00%)
[Iter 2980] Gaussian 1 vs 0:
  Original Loss: 0.0008404
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0008404 (Pseudo: 0.00%)
[Iter 2990/20000] Loss: 0.0008444 (Best: 0.0006816 @iter2983) ([91m↑1.21%[0m) [0.34% of initial]
[Iter 2990] Gaussian 0 vs 1:
  Original Loss: 0.0008154
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0008154 (Pseudo: 0.00%)
[Iter 2990] Gaussian 1 vs 0:
  Original Loss: 0.0008080
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0008080 (Pseudo: 0.00%)
Iter:2999, L1 loss=0.0006996, Total loss=0.0006523, Time:16
[Iter 3000/20000] Loss: 0.0008215 (Best: 0.0006523 @iter2999) ([92m↓2.71%[0m) [0.33% of initial]
[Iter 3000] Gaussian 0 vs 1:
  Original Loss: 0.0009780
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0009780 (Pseudo: 0.00%)
[Iter 3000] Gaussian 1 vs 0:
  Original Loss: 0.0010524
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0010524 (Pseudo: 0.00%)
[Iter 3010/20000] Loss: 0.0045499 (Best: 0.0006523 @iter2999) ([91m↑453.84%[0m) [1.81% of initial]
[Iter 3010] Gaussian 0 vs 1:
  Original Loss: 0.0035326
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0035326 (Pseudo: 0.00%)
[Iter 3010] Gaussian 1 vs 0:
  Original Loss: 0.0036303
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0036303 (Pseudo: 0.00%)
[Iter 3020/20000] Loss: 0.0026290 (Best: 0.0006523 @iter2999) ([92m↓42.22%[0m) [1.04% of initial]
[Iter 3020] Gaussian 0 vs 1:
  Original Loss: 0.0027240
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0027240 (Pseudo: 0.00%)
[Iter 3020] Gaussian 1 vs 0:
  Original Loss: 0.0027112
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0027112 (Pseudo: 0.00%)
[Iter 3030/20000] Loss: 0.0020890 (Best: 0.0006523 @iter2999) ([92m↓20.54%[0m) [0.83% of initial]
[Iter 3030] Gaussian 0 vs 1:
  Original Loss: 0.0024617
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0024617 (Pseudo: 0.00%)
[Iter 3030] Gaussian 1 vs 0:
  Original Loss: 0.0024983
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0024983 (Pseudo: 0.00%)
[Iter 3040/20000] Loss: 0.0017030 (Best: 0.0006523 @iter2999) ([92m↓18.48%[0m) [0.68% of initial]
[Iter 3040] Gaussian 0 vs 1:
  Original Loss: 0.0014865
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0014865 (Pseudo: 0.00%)
[Iter 3040] Gaussian 1 vs 0:
  Original Loss: 0.0014183
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0014183 (Pseudo: 0.00%)
[Iter 3050/20000] Loss: 0.0014198 (Best: 0.0006523 @iter2999) ([92m↓16.63%[0m) [0.56% of initial]
[Iter 3050] Gaussian 0 vs 1:
  Original Loss: 0.0015042
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0015042 (Pseudo: 0.00%)
[Iter 3050] Gaussian 1 vs 0:
  Original Loss: 0.0015727
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0015727 (Pseudo: 0.00%)
[Iter 3060/20000] Loss: 0.0013639 (Best: 0.0006523 @iter2999) ([92m↓3.94%[0m) [0.54% of initial]
[Iter 3060] Gaussian 0 vs 1:
  Original Loss: 0.0011491
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0011491 (Pseudo: 0.00%)
[Iter 3060] Gaussian 1 vs 0:
  Original Loss: 0.0011235
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0011235 (Pseudo: 0.00%)
[Iter 3070/20000] Loss: 0.0011124 (Best: 0.0006523 @iter2999) ([92m↓18.44%[0m) [0.44% of initial]
[Iter 3070] Gaussian 0 vs 1:
  Original Loss: 0.0010832
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0010832 (Pseudo: 0.00%)
[Iter 3070] Gaussian 1 vs 0:
  Original Loss: 0.0011705
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0011705 (Pseudo: 0.00%)
[Iter 3080/20000] Loss: 0.0011266 (Best: 0.0006523 @iter2999) ([91m↑1.27%[0m) [0.45% of initial]
[Iter 3080] Gaussian 0 vs 1:
  Original Loss: 0.0012836
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0012836 (Pseudo: 0.00%)
[Iter 3080] Gaussian 1 vs 0:
  Original Loss: 0.0013310
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0013310 (Pseudo: 0.00%)
[Iter 3090/20000] Loss: 0.0010788 (Best: 0.0006523 @iter2999) ([92m↓4.24%[0m) [0.43% of initial]
[Iter 3090] Gaussian 0 vs 1:
  Original Loss: 0.0011962
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0011962 (Pseudo: 0.00%)
[Iter 3090] Gaussian 1 vs 0:
  Original Loss: 0.0011747
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0011747 (Pseudo: 0.00%)
Iter:3099, L1 loss=0.000894, Total loss=0.0008774, Time:17
[Iter 3100/20000] Loss: 0.0009867 (Best: 0.0006523 @iter2999) ([92m↓8.54%[0m) [0.39% of initial]
[Iter 3100] Gaussian 0 vs 1:
  Original Loss: 0.0009876
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0009876 (Pseudo: 0.00%)
[Iter 3100] Gaussian 1 vs 0:
  Original Loss: 0.0010478
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0010478 (Pseudo: 0.00%)
[Iter 3110/20000] Loss: 0.0010857 (Best: 0.0006523 @iter2999) ([91m↑10.03%[0m) [0.43% of initial]
[Iter 3110] Gaussian 0 vs 1:
  Original Loss: 0.0012822
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0012822 (Pseudo: 0.00%)
[Iter 3110] Gaussian 1 vs 0:
  Original Loss: 0.0013810
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0013810 (Pseudo: 0.00%)
[Iter 3120/20000] Loss: 0.0010291 (Best: 0.0006523 @iter2999) ([92m↓5.21%[0m) [0.41% of initial]
[Iter 3120] Gaussian 0 vs 1:
  Original Loss: 0.0011599
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0011599 (Pseudo: 0.00%)
[Iter 3120] Gaussian 1 vs 0:
  Original Loss: 0.0012107
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0012107 (Pseudo: 0.00%)
[Iter 3130/20000] Loss: 0.0008298 (Best: 0.0006523 @iter2999) ([92m↓19.37%[0m) [0.33% of initial]
[Iter 3130] Gaussian 0 vs 1:
  Original Loss: 0.0007245
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0007245 (Pseudo: 0.00%)
[Iter 3130] Gaussian 1 vs 0:
  Original Loss: 0.0007683
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0007683 (Pseudo: 0.00%)
[Iter 3140/20000] Loss: 0.0007993 (Best: 0.0006523 @iter2999) ([92m↓3.67%[0m) [0.32% of initial]
[Iter 3140] Gaussian 0 vs 1:
  Original Loss: 0.0008534
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0008534 (Pseudo: 0.00%)
[Iter 3140] Gaussian 1 vs 0:
  Original Loss: 0.0009095
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0009095 (Pseudo: 0.00%)
[Iter 3150/20000] Loss: 0.0008259 (Best: 0.0006523 @iter2999) ([91m↑3.32%[0m) [0.33% of initial]
[Iter 3150] Gaussian 0 vs 1:
  Original Loss: 0.0009833
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0009833 (Pseudo: 0.00%)
[Iter 3150] Gaussian 1 vs 0:
  Original Loss: 0.0010638
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0010638 (Pseudo: 0.00%)
[Iter 3160/20000] Loss: 0.0007774 (Best: 0.0006523 @iter2999) ([92m↓5.87%[0m) [0.31% of initial]
[Iter 3160] Gaussian 0 vs 1:
  Original Loss: 0.0006655
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0006655 (Pseudo: 0.00%)
[Iter 3160] Gaussian 1 vs 0:
  Original Loss: 0.0006967
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0006967 (Pseudo: 0.00%)
[Iter 3170/20000] Loss: 0.0007989 (Best: 0.0006523 @iter2999) ([91m↑2.77%[0m) [0.32% of initial]
[Iter 3170] Gaussian 0 vs 1:
  Original Loss: 0.0008361
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0008361 (Pseudo: 0.00%)
[Iter 3170] Gaussian 1 vs 0:
  Original Loss: 0.0009053
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0009053 (Pseudo: 0.00%)
[Iter 3180/20000] Loss: 0.0008501 (Best: 0.0006523 @iter2999) ([91m↑6.41%[0m) [0.34% of initial]
[Iter 3180] Gaussian 0 vs 1:
  Original Loss: 0.0008853
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0008853 (Pseudo: 0.00%)
[Iter 3180] Gaussian 1 vs 0:
  Original Loss: 0.0009093
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0009093 (Pseudo: 0.00%)
[Iter 3190/20000] Loss: 0.0008299 (Best: 0.0006523 @iter2999) ([92m↓2.38%[0m) [0.33% of initial]
[Iter 3190] Gaussian 0 vs 1:
  Original Loss: 0.0007891
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0007891 (Pseudo: 0.00%)
[Iter 3190] Gaussian 1 vs 0:
  Original Loss: 0.0007642
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0007642 (Pseudo: 0.00%)
Iter:3199, L1 loss=0.0007921, Total loss=0.0007481, Time:17
[Iter 3200/20000] Loss: 0.0007863 (Best: 0.0006254 @iter3196) ([92m↓5.25%[0m) [0.31% of initial]
[Iter 3200] Gaussian 0 vs 1:
  Original Loss: 0.0008014
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0008014 (Pseudo: 0.00%)
[Iter 3200] Gaussian 1 vs 0:
  Original Loss: 0.0008506
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0008506 (Pseudo: 0.00%)
[Iter 3210/20000] Loss: 0.0049806 (Best: 0.0006254 @iter3196) ([91m↑533.39%[0m) [1.98% of initial]
[Iter 3210] Gaussian 0 vs 1:
  Original Loss: 0.0046798
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0046798 (Pseudo: 0.00%)
[Iter 3210] Gaussian 1 vs 0:
  Original Loss: 0.0051557
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0051557 (Pseudo: 0.00%)
[Iter 3220/20000] Loss: 0.0026567 (Best: 0.0006254 @iter3196) ([92m↓46.66%[0m) [1.06% of initial]
[Iter 3220] Gaussian 0 vs 1:
  Original Loss: 0.0023684
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0023684 (Pseudo: 0.00%)
[Iter 3220] Gaussian 1 vs 0:
  Original Loss: 0.0025743
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0025743 (Pseudo: 0.00%)
[Iter 3230/20000] Loss: 0.0016267 (Best: 0.0006254 @iter3196) ([92m↓38.77%[0m) [0.65% of initial]
[Iter 3230] Gaussian 0 vs 1:
  Original Loss: 0.0016169
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0016169 (Pseudo: 0.00%)
[Iter 3230] Gaussian 1 vs 0:
  Original Loss: 0.0016387
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0016387 (Pseudo: 0.00%)
[Iter 3240/20000] Loss: 0.0014270 (Best: 0.0006254 @iter3196) ([92m↓12.27%[0m) [0.57% of initial]
[Iter 3240] Gaussian 0 vs 1:
  Original Loss: 0.0014442
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0014442 (Pseudo: 0.00%)
[Iter 3240] Gaussian 1 vs 0:
  Original Loss: 0.0015464
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0015464 (Pseudo: 0.00%)
[Iter 3250/20000] Loss: 0.0010709 (Best: 0.0006254 @iter3196) ([92m↓24.95%[0m) [0.43% of initial]
[Iter 3250] Gaussian 0 vs 1:
  Original Loss: 0.0009927
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0009927 (Pseudo: 0.00%)
[Iter 3250] Gaussian 1 vs 0:
  Original Loss: 0.0010079
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0010079 (Pseudo: 0.00%)
[Iter 3260/20000] Loss: 0.0009502 (Best: 0.0006254 @iter3196) ([92m↓11.27%[0m) [0.38% of initial]
[Iter 3260] Gaussian 0 vs 1:
  Original Loss: 0.0008659
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0008659 (Pseudo: 0.00%)
[Iter 3260] Gaussian 1 vs 0:
  Original Loss: 0.0008868
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0008868 (Pseudo: 0.00%)
[Iter 3270/20000] Loss: 0.0009700 (Best: 0.0006254 @iter3196) ([91m↑2.09%[0m) [0.39% of initial]
[Iter 3270] Gaussian 0 vs 1:
  Original Loss: 0.0011422
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0011422 (Pseudo: 0.00%)
[Iter 3270] Gaussian 1 vs 0:
  Original Loss: 0.0012094
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0012094 (Pseudo: 0.00%)
[Iter 3280/20000] Loss: 0.0010298 (Best: 0.0006254 @iter3196) ([91m↑6.16%[0m) [0.41% of initial]
[Iter 3280] Gaussian 0 vs 1:
  Original Loss: 0.0009837
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0009837 (Pseudo: 0.00%)
[Iter 3280] Gaussian 1 vs 0:
  Original Loss: 0.0009744
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0009744 (Pseudo: 0.00%)
[Iter 3290/20000] Loss: 0.0007705 (Best: 0.0006254 @iter3196) ([92m↓25.17%[0m) [0.31% of initial]
[Iter 3290] Gaussian 0 vs 1:
  Original Loss: 0.0007109
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0007109 (Pseudo: 0.00%)
[Iter 3290] Gaussian 1 vs 0:
  Original Loss: 0.0007140
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0007140 (Pseudo: 0.00%)
Iter:3299, L1 loss=0.00122, Total loss=0.001178, Time:18
[Iter 3300/20000] Loss: 0.0010454 (Best: 0.0006254 @iter3196) ([91m↑35.68%[0m) [0.42% of initial]
[Iter 3300] Gaussian 0 vs 1:
  Original Loss: 0.0010015
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0010015 (Pseudo: 0.00%)
[Iter 3300] Gaussian 1 vs 0:
  Original Loss: 0.0010017
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0010017 (Pseudo: 0.00%)
[Iter 3310/20000] Loss: 0.0008255 (Best: 0.0006254 @iter3196) ([92m↓21.03%[0m) [0.33% of initial]
[Iter 3310] Gaussian 0 vs 1:
  Original Loss: 0.0007516
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0007516 (Pseudo: 0.00%)
[Iter 3310] Gaussian 1 vs 0:
  Original Loss: 0.0007175
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0007175 (Pseudo: 0.00%)
[Iter 3320/20000] Loss: 0.0009261 (Best: 0.0006254 @iter3196) ([91m↑12.18%[0m) [0.37% of initial]
[Iter 3320] Gaussian 0 vs 1:
  Original Loss: 0.0009551
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0009551 (Pseudo: 0.00%)
[Iter 3320] Gaussian 1 vs 0:
  Original Loss: 0.0010159
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0010159 (Pseudo: 0.00%)
[Iter 3330/20000] Loss: 0.0009646 (Best: 0.0006254 @iter3196) ([91m↑4.15%[0m) [0.38% of initial]
[Iter 3330] Gaussian 0 vs 1:
  Original Loss: 0.0010594
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0010594 (Pseudo: 0.00%)
[Iter 3330] Gaussian 1 vs 0:
  Original Loss: 0.0011511
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0011511 (Pseudo: 0.00%)
[Iter 3340/20000] Loss: 0.0010185 (Best: 0.0006254 @iter3196) ([91m↑5.59%[0m) [0.40% of initial]
[Iter 3340] Gaussian 0 vs 1:
  Original Loss: 0.0009773
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0009773 (Pseudo: 0.00%)
[Iter 3340] Gaussian 1 vs 0:
  Original Loss: 0.0010661
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0010661 (Pseudo: 0.00%)
[Iter 3350/20000] Loss: 0.0008340 (Best: 0.0006254 @iter3196) ([92m↓18.12%[0m) [0.33% of initial]
[Iter 3350] Gaussian 0 vs 1:
  Original Loss: 0.0008964
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0008964 (Pseudo: 0.00%)
[Iter 3350] Gaussian 1 vs 0:
  Original Loss: 0.0009398
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0009398 (Pseudo: 0.00%)
[Iter 3360/20000] Loss: 0.0010132 (Best: 0.0006254 @iter3196) ([91m↑21.49%[0m) [0.40% of initial]
[Iter 3360] Gaussian 0 vs 1:
  Original Loss: 0.0011661
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0011661 (Pseudo: 0.00%)
[Iter 3360] Gaussian 1 vs 0:
  Original Loss: 0.0012574
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0012574 (Pseudo: 0.00%)
[Iter 3370/20000] Loss: 0.0007542 (Best: 0.0006254 @iter3196) ([92m↓25.56%[0m) [0.30% of initial]
[Iter 3370] Gaussian 0 vs 1:
  Original Loss: 0.0006464
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0006464 (Pseudo: 0.00%)
[Iter 3370] Gaussian 1 vs 0:
  Original Loss: 0.0006811
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0006811 (Pseudo: 0.00%)
[Iter 3380/20000] Loss: 0.0007331 (Best: 0.0006254 @iter3196) ([92m↓2.81%[0m) [0.29% of initial]
[Iter 3380] Gaussian 0 vs 1:
  Original Loss: 0.0007587
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0007587 (Pseudo: 0.00%)
[Iter 3380] Gaussian 1 vs 0:
  Original Loss: 0.0008515
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0008515 (Pseudo: 0.00%)
[Iter 3390/20000] Loss: 0.0009832 (Best: 0.0006254 @iter3196) ([91m↑34.12%[0m) [0.39% of initial]
[Iter 3390] Gaussian 0 vs 1:
  Original Loss: 0.0009168
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0009168 (Pseudo: 0.00%)
[Iter 3390] Gaussian 1 vs 0:
  Original Loss: 0.0008658
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0008658 (Pseudo: 0.00%)
Iter:3399, L1 loss=0.001184, Total loss=0.001165, Time:18
[Iter 3400/20000] Loss: 0.0010055 (Best: 0.0006254 @iter3196) ([91m↑2.26%[0m) [0.40% of initial]
[Iter 3400] Gaussian 0 vs 1:
  Original Loss: 0.0008602
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0008602 (Pseudo: 0.00%)
[Iter 3400] Gaussian 1 vs 0:
  Original Loss: 0.0008400
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0008400 (Pseudo: 0.00%)
[Iter 3410/20000] Loss: 0.0045627 (Best: 0.0006254 @iter3196) ([91m↑353.79%[0m) [1.81% of initial]
[Iter 3410] Gaussian 0 vs 1:
  Original Loss: 0.0041822
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0041822 (Pseudo: 0.00%)
[Iter 3410] Gaussian 1 vs 0:
  Original Loss: 0.0038520
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0038520 (Pseudo: 0.00%)
[Iter 3420/20000] Loss: 0.0023096 (Best: 0.0006254 @iter3196) ([92m↓49.38%[0m) [0.92% of initial]
[Iter 3420] Gaussian 0 vs 1:
  Original Loss: 0.0020035
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0020035 (Pseudo: 0.00%)
[Iter 3420] Gaussian 1 vs 0:
  Original Loss: 0.0023234
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0023234 (Pseudo: 0.00%)
[Iter 3430/20000] Loss: 0.0014949 (Best: 0.0006254 @iter3196) ([92m↓35.27%[0m) [0.59% of initial]
[Iter 3430] Gaussian 0 vs 1:
  Original Loss: 0.0013054
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0013054 (Pseudo: 0.00%)
[Iter 3430] Gaussian 1 vs 0:
  Original Loss: 0.0013495
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0013495 (Pseudo: 0.00%)
[Iter 3440/20000] Loss: 0.0012414 (Best: 0.0006254 @iter3196) ([92m↓16.96%[0m) [0.49% of initial]
[Iter 3440] Gaussian 0 vs 1:
  Original Loss: 0.0011336
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0011336 (Pseudo: 0.00%)
[Iter 3440] Gaussian 1 vs 0:
  Original Loss: 0.0012015
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0012015 (Pseudo: 0.00%)
[Iter 3450/20000] Loss: 0.0011392 (Best: 0.0006254 @iter3196) ([92m↓8.23%[0m) [0.45% of initial]
[Iter 3450] Gaussian 0 vs 1:
  Original Loss: 0.0012144
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0012144 (Pseudo: 0.00%)
[Iter 3450] Gaussian 1 vs 0:
  Original Loss: 0.0012491
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0012491 (Pseudo: 0.00%)
[Iter 3460/20000] Loss: 0.0010240 (Best: 0.0006254 @iter3196) ([92m↓10.12%[0m) [0.41% of initial]
[Iter 3460] Gaussian 0 vs 1:
  Original Loss: 0.0009443
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0009443 (Pseudo: 0.00%)
[Iter 3460] Gaussian 1 vs 0:
  Original Loss: 0.0010317
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0010317 (Pseudo: 0.00%)
[Iter 3470/20000] Loss: 0.0009663 (Best: 0.0006254 @iter3196) ([92m↓5.63%[0m) [0.38% of initial]
[Iter 3470] Gaussian 0 vs 1:
  Original Loss: 0.0009094
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0009094 (Pseudo: 0.00%)
[Iter 3470] Gaussian 1 vs 0:
  Original Loss: 0.0009271
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0009271 (Pseudo: 0.00%)
[Iter 3480/20000] Loss: 0.0008994 (Best: 0.0006254 @iter3196) ([92m↓6.92%[0m) [0.36% of initial]
[Iter 3480] Gaussian 0 vs 1:
  Original Loss: 0.0009657
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0009657 (Pseudo: 0.00%)
[Iter 3480] Gaussian 1 vs 0:
  Original Loss: 0.0009375
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0009375 (Pseudo: 0.00%)
[Iter 3490/20000] Loss: 0.0008619 (Best: 0.0006254 @iter3196) ([92m↓4.17%[0m) [0.34% of initial]
[Iter 3490] Gaussian 0 vs 1:
  Original Loss: 0.0009015
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0009015 (Pseudo: 0.00%)
[Iter 3490] Gaussian 1 vs 0:
  Original Loss: 0.0008519
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0008519 (Pseudo: 0.00%)
Iter:3499, L1 loss=0.0006636, Total loss=0.0006071, Time:18
[Iter 3500/20000] Loss: 0.0006592 (Best: 0.0006071 @iter3499) ([92m↓23.51%[0m) [0.26% of initial]
[Iter 3500] Gaussian 0 vs 1:
  Original Loss: 0.0006228
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0006228 (Pseudo: 0.00%)
[Iter 3500] Gaussian 1 vs 0:
  Original Loss: 0.0006255
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0006255 (Pseudo: 0.00%)
[Iter 3510/20000] Loss: 0.0007230 (Best: 0.0006071 @iter3499) ([91m↑9.67%[0m) [0.29% of initial]
[Iter 3510] Gaussian 0 vs 1:
  Original Loss: 0.0006634
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0006634 (Pseudo: 0.00%)
[Iter 3510] Gaussian 1 vs 0:
  Original Loss: 0.0006789
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0006789 (Pseudo: 0.00%)
[Iter 3520/20000] Loss: 0.0007205 (Best: 0.0005927 @iter3517) ([92m↓0.33%[0m) [0.29% of initial]
[Iter 3520] Gaussian 0 vs 1:
  Original Loss: 0.0007404
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0007404 (Pseudo: 0.00%)
[Iter 3520] Gaussian 1 vs 0:
  Original Loss: 0.0007957
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0007957 (Pseudo: 0.00%)
[Iter 3530/20000] Loss: 0.0007630 (Best: 0.0005927 @iter3517) ([91m↑5.90%[0m) [0.30% of initial]
[Iter 3530] Gaussian 0 vs 1:
  Original Loss: 0.0008405
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0008405 (Pseudo: 0.00%)
[Iter 3530] Gaussian 1 vs 0:
  Original Loss: 0.0009009
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0009009 (Pseudo: 0.00%)
[Iter 3540/20000] Loss: 0.0010441 (Best: 0.0005927 @iter3517) ([91m↑36.83%[0m) [0.41% of initial]
[Iter 3540] Gaussian 0 vs 1:
  Original Loss: 0.0011651
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0011651 (Pseudo: 0.00%)
[Iter 3540] Gaussian 1 vs 0:
  Original Loss: 0.0012267
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0012267 (Pseudo: 0.00%)
[Iter 3550/20000] Loss: 0.0010043 (Best: 0.0005927 @iter3517) ([92m↓3.81%[0m) [0.40% of initial]
[Iter 3550] Gaussian 0 vs 1:
  Original Loss: 0.0009668
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0009668 (Pseudo: 0.00%)
[Iter 3550] Gaussian 1 vs 0:
  Original Loss: 0.0009232
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0009232 (Pseudo: 0.00%)
[Iter 3560/20000] Loss: 0.0009004 (Best: 0.0005927 @iter3517) ([92m↓10.34%[0m) [0.36% of initial]
[Iter 3560] Gaussian 0 vs 1:
  Original Loss: 0.0008940
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0008940 (Pseudo: 0.00%)
[Iter 3560] Gaussian 1 vs 0:
  Original Loss: 0.0008804
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0008804 (Pseudo: 0.00%)
[Iter 3570/20000] Loss: 0.0009132 (Best: 0.0005927 @iter3517) ([91m↑1.42%[0m) [0.36% of initial]
[Iter 3570] Gaussian 0 vs 1:
  Original Loss: 0.0011207
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0011207 (Pseudo: 0.00%)
[Iter 3570] Gaussian 1 vs 0:
  Original Loss: 0.0011686
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0011686 (Pseudo: 0.00%)
[Iter 3580/20000] Loss: 0.0007222 (Best: 0.0005927 @iter3517) ([92m↓20.92%[0m) [0.29% of initial]
[Iter 3580] Gaussian 0 vs 1:
  Original Loss: 0.0006439
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0006439 (Pseudo: 0.00%)
[Iter 3580] Gaussian 1 vs 0:
  Original Loss: 0.0006277
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0006277 (Pseudo: 0.00%)
[Iter 3590/20000] Loss: 0.0007093 (Best: 0.0005927 @iter3517) ([92m↓1.78%[0m) [0.28% of initial]
[Iter 3590] Gaussian 0 vs 1:
  Original Loss: 0.0006760
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0006760 (Pseudo: 0.00%)
[Iter 3590] Gaussian 1 vs 0:
  Original Loss: 0.0006701
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0006701 (Pseudo: 0.00%)
Iter:3599, L1 loss=0.0006328, Total loss=0.0005842, Time:18
[Iter 3600/20000] Loss: 0.0006901 (Best: 0.0005735 @iter3598) ([92m↓2.70%[0m) [0.27% of initial]
[Iter 3600] Gaussian 0 vs 1:
  Original Loss: 0.0008013
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0008013 (Pseudo: 0.00%)
[Iter 3600] Gaussian 1 vs 0:
  Original Loss: 0.0008138
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0008138 (Pseudo: 0.00%)
[Iter 3610/20000] Loss: 0.0046911 (Best: 0.0005735 @iter3598) ([91m↑579.72%[0m) [1.86% of initial]
[Iter 3610] Gaussian 0 vs 1:
  Original Loss: 0.0036785
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0036785 (Pseudo: 0.00%)
[Iter 3610] Gaussian 1 vs 0:
  Original Loss: 0.0040060
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0040060 (Pseudo: 0.00%)
[Iter 3620/20000] Loss: 0.0026481 (Best: 0.0005735 @iter3598) ([92m↓43.55%[0m) [1.05% of initial]
[Iter 3620] Gaussian 0 vs 1:
  Original Loss: 0.0024166
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0024166 (Pseudo: 0.00%)
[Iter 3620] Gaussian 1 vs 0:
  Original Loss: 0.0023905
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0023905 (Pseudo: 0.00%)
[Iter 3630/20000] Loss: 0.0015306 (Best: 0.0005735 @iter3598) ([92m↓42.20%[0m) [0.61% of initial]
[Iter 3630] Gaussian 0 vs 1:
  Original Loss: 0.0014425
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0014425 (Pseudo: 0.00%)
[Iter 3630] Gaussian 1 vs 0:
  Original Loss: 0.0015558
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0015558 (Pseudo: 0.00%)
[Iter 3640/20000] Loss: 0.0011170 (Best: 0.0005735 @iter3598) ([92m↓27.02%[0m) [0.44% of initial]
[Iter 3640] Gaussian 0 vs 1:
  Original Loss: 0.0010923
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0010923 (Pseudo: 0.00%)
[Iter 3640] Gaussian 1 vs 0:
  Original Loss: 0.0011783
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0011783 (Pseudo: 0.00%)
[Iter 3650/20000] Loss: 0.0011041 (Best: 0.0005735 @iter3598) ([92m↓1.16%[0m) [0.44% of initial]
[Iter 3650] Gaussian 0 vs 1:
  Original Loss: 0.0011730
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0011730 (Pseudo: 0.00%)
[Iter 3650] Gaussian 1 vs 0:
  Original Loss: 0.0012402
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0012402 (Pseudo: 0.00%)
[Iter 3660/20000] Loss: 0.0008812 (Best: 0.0005735 @iter3598) ([92m↓20.19%[0m) [0.35% of initial]
[Iter 3660] Gaussian 0 vs 1:
  Original Loss: 0.0009422
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0009422 (Pseudo: 0.00%)
[Iter 3660] Gaussian 1 vs 0:
  Original Loss: 0.0009502
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0009502 (Pseudo: 0.00%)
[Iter 3670/20000] Loss: 0.0007818 (Best: 0.0005735 @iter3598) ([92m↓11.27%[0m) [0.31% of initial]
[Iter 3670] Gaussian 0 vs 1:
  Original Loss: 0.0007069
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0007069 (Pseudo: 0.00%)
[Iter 3670] Gaussian 1 vs 0:
  Original Loss: 0.0007167
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0007167 (Pseudo: 0.00%)
[Iter 3680/20000] Loss: 0.0009581 (Best: 0.0005735 @iter3598) ([91m↑22.54%[0m) [0.38% of initial]
[Iter 3680] Gaussian 0 vs 1:
  Original Loss: 0.0010227
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0010227 (Pseudo: 0.00%)
[Iter 3680] Gaussian 1 vs 0:
  Original Loss: 0.0010757
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0010757 (Pseudo: 0.00%)
[Iter 3690/20000] Loss: 0.0012407 (Best: 0.0005735 @iter3598) ([91m↑29.50%[0m) [0.49% of initial]
[Iter 3690] Gaussian 0 vs 1:
  Original Loss: 0.0016362
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0016362 (Pseudo: 0.00%)
[Iter 3690] Gaussian 1 vs 0:
  Original Loss: 0.0014205
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0014205 (Pseudo: 0.00%)
Iter:3699, L1 loss=0.001052, Total loss=0.000964, Time:19
[Iter 3700/20000] Loss: 0.0010279 (Best: 0.0005735 @iter3598) ([92m↓17.15%[0m) [0.41% of initial]
[Iter 3700] Gaussian 0 vs 1:
  Original Loss: 0.0010231
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0010231 (Pseudo: 0.00%)
[Iter 3700] Gaussian 1 vs 0:
  Original Loss: 0.0009627
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0009627 (Pseudo: 0.00%)
[Iter 3710/20000] Loss: 0.0008029 (Best: 0.0005735 @iter3598) ([92m↓21.88%[0m) [0.32% of initial]
[Iter 3710] Gaussian 0 vs 1:
  Original Loss: 0.0008168
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0008168 (Pseudo: 0.00%)
[Iter 3710] Gaussian 1 vs 0:
  Original Loss: 0.0008466
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0008466 (Pseudo: 0.00%)
[Iter 3720/20000] Loss: 0.0008543 (Best: 0.0005735 @iter3598) ([91m↑6.39%[0m) [0.34% of initial]
[Iter 3720] Gaussian 0 vs 1:
  Original Loss: 0.0007890
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0007890 (Pseudo: 0.00%)
[Iter 3720] Gaussian 1 vs 0:
  Original Loss: 0.0007995
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0007995 (Pseudo: 0.00%)
[Iter 3730/20000] Loss: 0.0007381 (Best: 0.0005735 @iter3598) ([92m↓13.59%[0m) [0.29% of initial]
[Iter 3730] Gaussian 0 vs 1:
  Original Loss: 0.0006716
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0006716 (Pseudo: 0.00%)
[Iter 3730] Gaussian 1 vs 0:
  Original Loss: 0.0006608
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0006608 (Pseudo: 0.00%)
[Iter 3740/20000] Loss: 0.0007469 (Best: 0.0005735 @iter3598) ([91m↑1.19%[0m) [0.30% of initial]
[Iter 3740] Gaussian 0 vs 1:
  Original Loss: 0.0008076
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0008076 (Pseudo: 0.00%)
[Iter 3740] Gaussian 1 vs 0:
  Original Loss: 0.0008363
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0008363 (Pseudo: 0.00%)
[Iter 3750/20000] Loss: 0.0007782 (Best: 0.0005735 @iter3598) ([91m↑4.18%[0m) [0.31% of initial]
[Iter 3750] Gaussian 0 vs 1:
  Original Loss: 0.0008450
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0008450 (Pseudo: 0.00%)
[Iter 3750] Gaussian 1 vs 0:
  Original Loss: 0.0008558
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0008558 (Pseudo: 0.00%)
[Iter 3760/20000] Loss: 0.0007585 (Best: 0.0005735 @iter3598) ([92m↓2.53%[0m) [0.30% of initial]
[Iter 3760] Gaussian 0 vs 1:
  Original Loss: 0.0007587
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0007587 (Pseudo: 0.00%)
[Iter 3760] Gaussian 1 vs 0:
  Original Loss: 0.0007966
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0007966 (Pseudo: 0.00%)
[Iter 3770/20000] Loss: 0.0007261 (Best: 0.0005735 @iter3598) ([92m↓4.27%[0m) [0.29% of initial]
[Iter 3770] Gaussian 0 vs 1:
  Original Loss: 0.0006828
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0006828 (Pseudo: 0.00%)
[Iter 3770] Gaussian 1 vs 0:
  Original Loss: 0.0006987
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0006987 (Pseudo: 0.00%)
[Iter 3780/20000] Loss: 0.0006760 (Best: 0.0005296 @iter3775) ([92m↓6.91%[0m) [0.27% of initial]
[Iter 3780] Gaussian 0 vs 1:
  Original Loss: 0.0007353
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0007353 (Pseudo: 0.00%)
[Iter 3780] Gaussian 1 vs 0:
  Original Loss: 0.0007602
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0007602 (Pseudo: 0.00%)
[Iter 3790/20000] Loss: 0.0005558 (Best: 0.0004960 @iter3790) ([92m↓17.77%[0m) [0.22% of initial]
[Iter 3790] Gaussian 0 vs 1:
  Original Loss: 0.0004960
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0004960 (Pseudo: 0.00%)
[Iter 3790] Gaussian 1 vs 0:
  Original Loss: 0.0005037
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0005037 (Pseudo: 0.00%)
Iter:3799, L1 loss=0.0007244, Total loss=0.0006896, Time:19
[Iter 3800/20000] Loss: 0.0006981 (Best: 0.0004960 @iter3790) ([91m↑25.60%[0m) [0.28% of initial]
[Iter 3800] Gaussian 0 vs 1:
  Original Loss: 0.0006585
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0006585 (Pseudo: 0.00%)
[Iter 3800] Gaussian 1 vs 0:
  Original Loss: 0.0006596
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0006596 (Pseudo: 0.00%)
[Iter 3810/20000] Loss: 0.0050216 (Best: 0.0004960 @iter3790) ([91m↑619.32%[0m) [2.00% of initial]
[Iter 3810] Gaussian 0 vs 1:
  Original Loss: 0.0055907
  Pseudo Loss: 0.0012824 (22.94% of original)
  Total Loss: 0.0068731 (Pseudo: 18.66%)
[Iter 3810] Gaussian 1 vs 0:
  Original Loss: 0.0043491
  Pseudo Loss: 0.0012824 (29.49% of original)
  Total Loss: 0.0056315 (Pseudo: 22.77%)
[Iter 3810] Pseudo Loss: 0.0018661
[Iter 3820/20000] Loss: 0.0024094 (Best: 0.0004960 @iter3790) ([92m↓52.02%[0m) [0.96% of initial]
[Iter 3820] Gaussian 0 vs 1:
  Original Loss: 0.0018961
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0018961 (Pseudo: 0.00%)
[Iter 3820] Gaussian 1 vs 0:
  Original Loss: 0.0018700
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0018700 (Pseudo: 0.00%)
[Iter 3830/20000] Loss: 0.0013805 (Best: 0.0004960 @iter3790) ([92m↓42.70%[0m) [0.55% of initial]
[Iter 3830] Gaussian 0 vs 1:
  Original Loss: 0.0013333
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0013333 (Pseudo: 0.00%)
[Iter 3830] Gaussian 1 vs 0:
  Original Loss: 0.0013359
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0013359 (Pseudo: 0.00%)
[Iter 3840/20000] Loss: 0.0013238 (Best: 0.0004960 @iter3790) ([92m↓4.11%[0m) [0.53% of initial]
[Iter 3840] Gaussian 0 vs 1:
  Original Loss: 0.0015472
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0015472 (Pseudo: 0.00%)
[Iter 3840] Gaussian 1 vs 0:
  Original Loss: 0.0015910
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0015910 (Pseudo: 0.00%)
[Iter 3850/20000] Loss: 0.0009814 (Best: 0.0004960 @iter3790) ([92m↓25.86%[0m) [0.39% of initial]
[Iter 3850] Gaussian 0 vs 1:
  Original Loss: 0.0008981
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0008981 (Pseudo: 0.00%)
[Iter 3850] Gaussian 1 vs 0:
  Original Loss: 0.0009928
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0009928 (Pseudo: 0.00%)
[Iter 3860/20000] Loss: 0.0009118 (Best: 0.0004960 @iter3790) ([92m↓7.09%[0m) [0.36% of initial]
[Iter 3860] Gaussian 0 vs 1:
  Original Loss: 0.0008216
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0008216 (Pseudo: 0.00%)
[Iter 3860] Gaussian 1 vs 0:
  Original Loss: 0.0008363
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0008363 (Pseudo: 0.00%)
[Iter 3870/20000] Loss: 0.0007295 (Best: 0.0004960 @iter3790) ([92m↓19.99%[0m) [0.29% of initial]
[Iter 3870] Gaussian 0 vs 1:
  Original Loss: 0.0006749
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0006749 (Pseudo: 0.00%)
[Iter 3870] Gaussian 1 vs 0:
  Original Loss: 0.0006944
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0006944 (Pseudo: 0.00%)
[Iter 3880/20000] Loss: 0.0007598 (Best: 0.0004960 @iter3790) ([91m↑4.15%[0m) [0.30% of initial]
[Iter 3880] Gaussian 0 vs 1:
  Original Loss: 0.0007800
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0007800 (Pseudo: 0.00%)
[Iter 3880] Gaussian 1 vs 0:
  Original Loss: 0.0008151
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0008151 (Pseudo: 0.00%)
[Iter 3890/20000] Loss: 0.0006401 (Best: 0.0004960 @iter3790) ([92m↓15.75%[0m) [0.25% of initial]
[Iter 3890] Gaussian 0 vs 1:
  Original Loss: 0.0006211
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0006211 (Pseudo: 0.00%)
[Iter 3890] Gaussian 1 vs 0:
  Original Loss: 0.0006304
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0006304 (Pseudo: 0.00%)
Iter:3899, L1 loss=0.0007058, Total loss=0.0006656, Time:20
[Iter 3900/20000] Loss: 0.0006498 (Best: 0.0004960 @iter3790) ([91m↑1.51%[0m) [0.26% of initial]
[Iter 3900] Gaussian 0 vs 1:
  Original Loss: 0.0007110
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0007110 (Pseudo: 0.00%)
[Iter 3900] Gaussian 1 vs 0:
  Original Loss: 0.0007462
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0007462 (Pseudo: 0.00%)
[Iter 3910/20000] Loss: 0.0007706 (Best: 0.0004960 @iter3790) ([91m↑18.60%[0m) [0.31% of initial]
[Iter 3910] Gaussian 0 vs 1:
  Original Loss: 0.0007513
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0007513 (Pseudo: 0.00%)
[Iter 3910] Gaussian 1 vs 0:
  Original Loss: 0.0007657
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0007657 (Pseudo: 0.00%)
[Iter 3920/20000] Loss: 0.0007837 (Best: 0.0004960 @iter3790) ([91m↑1.69%[0m) [0.31% of initial]
[Iter 3920] Gaussian 0 vs 1:
  Original Loss: 0.0007686
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0007686 (Pseudo: 0.00%)
[Iter 3920] Gaussian 1 vs 0:
  Original Loss: 0.0008185
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0008185 (Pseudo: 0.00%)
[Iter 3930/20000] Loss: 0.0007636 (Best: 0.0004960 @iter3790) ([92m↓2.57%[0m) [0.30% of initial]
[Iter 3930] Gaussian 0 vs 1:
  Original Loss: 0.0008273
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0008273 (Pseudo: 0.00%)
[Iter 3930] Gaussian 1 vs 0:
  Original Loss: 0.0008852
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0008852 (Pseudo: 0.00%)
[Iter 3940/20000] Loss: 0.0006308 (Best: 0.0004960 @iter3790) ([92m↓17.39%[0m) [0.25% of initial]
[Iter 3940] Gaussian 0 vs 1:
  Original Loss: 0.0005677
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0005677 (Pseudo: 0.00%)
[Iter 3940] Gaussian 1 vs 0:
  Original Loss: 0.0005828
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0005828 (Pseudo: 0.00%)
[Iter 3950/20000] Loss: 0.0007082 (Best: 0.0004960 @iter3790) ([91m↑12.27%[0m) [0.28% of initial]
[Iter 3950] Gaussian 0 vs 1:
  Original Loss: 0.0007613
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0007613 (Pseudo: 0.00%)
[Iter 3950] Gaussian 1 vs 0:
  Original Loss: 0.0007881
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0007881 (Pseudo: 0.00%)
[Iter 3960/20000] Loss: 0.0006954 (Best: 0.0004960 @iter3790) ([92m↓1.80%[0m) [0.28% of initial]
[Iter 3960] Gaussian 0 vs 1:
  Original Loss: 0.0006245
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0006245 (Pseudo: 0.00%)
[Iter 3960] Gaussian 1 vs 0:
  Original Loss: 0.0006529
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0006529 (Pseudo: 0.00%)
[Iter 3970/20000] Loss: 0.0006448 (Best: 0.0004960 @iter3790) ([92m↓7.28%[0m) [0.26% of initial]
[Iter 3970] Gaussian 0 vs 1:
  Original Loss: 0.0006091
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0006091 (Pseudo: 0.00%)
[Iter 3970] Gaussian 1 vs 0:
  Original Loss: 0.0006261
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0006261 (Pseudo: 0.00%)
[Iter 3980/20000] Loss: 0.0008916 (Best: 0.0004960 @iter3790) ([91m↑38.28%[0m) [0.35% of initial]
[Iter 3980] Gaussian 0 vs 1:
  Original Loss: 0.0011151
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0011151 (Pseudo: 0.00%)
[Iter 3980] Gaussian 1 vs 0:
  Original Loss: 0.0011227
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0011227 (Pseudo: 0.00%)
[Iter 3990/20000] Loss: 0.0006917 (Best: 0.0004960 @iter3790) ([92m↓22.42%[0m) [0.27% of initial]
[Iter 3990] Gaussian 0 vs 1:
  Original Loss: 0.0007158
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0007158 (Pseudo: 0.00%)
[Iter 3990] Gaussian 1 vs 0:
  Original Loss: 0.0007824
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0007824 (Pseudo: 0.00%)
Iter:3999, L1 loss=0.0008329, Total loss=0.000744, Time:19
[Iter 4000/20000] Loss: 0.0006823 (Best: 0.0004960 @iter3790) ([92m↓1.37%[0m) [0.27% of initial]
[Iter 4000] Gaussian 0 vs 1:
  Original Loss: 0.0006674
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0006674 (Pseudo: 0.00%)
[Iter 4000] Gaussian 1 vs 0:
  Original Loss: 0.0007162
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0007162 (Pseudo: 0.00%)
[Iter 4010/20000] Loss: 0.0830499 (Best: 0.0004960 @iter3790) ([91m↑12072.75%[0m) [32.99% of initial]
[Iter 4010] Gaussian 0 vs 1:
  Original Loss: 0.0628251
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0628251 (Pseudo: 0.00%)
[Iter 4010] Gaussian 1 vs 0:
  Original Loss: 0.0669051
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0669051 (Pseudo: 0.00%)
[Iter 4020/20000] Loss: 0.0176885 (Best: 0.0004960 @iter3790) ([92m↓78.70%[0m) [7.03% of initial]
[Iter 4020] Gaussian 0 vs 1:
  Original Loss: 0.0171846
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0171846 (Pseudo: 0.00%)
[Iter 4020] Gaussian 1 vs 0:
  Original Loss: 0.0180868
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0180868 (Pseudo: 0.00%)
[Iter 4030/20000] Loss: 0.0079570 (Best: 0.0004960 @iter3790) ([92m↓55.02%[0m) [3.16% of initial]
[Iter 4030] Gaussian 0 vs 1:
  Original Loss: 0.0067906
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0067906 (Pseudo: 0.00%)
[Iter 4030] Gaussian 1 vs 0:
  Original Loss: 0.0072487
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0072487 (Pseudo: 0.00%)
[Iter 4040/20000] Loss: 0.0043407 (Best: 0.0004960 @iter3790) ([92m↓45.45%[0m) [1.72% of initial]
[Iter 4040] Gaussian 0 vs 1:
  Original Loss: 0.0042812
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0042812 (Pseudo: 0.00%)
[Iter 4040] Gaussian 1 vs 0:
  Original Loss: 0.0044462
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0044462 (Pseudo: 0.00%)
[Iter 4050/20000] Loss: 0.0028533 (Best: 0.0004960 @iter3790) ([92m↓34.27%[0m) [1.13% of initial]
[Iter 4050] Gaussian 0 vs 1:
  Original Loss: 0.0032042
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0032042 (Pseudo: 0.00%)
[Iter 4050] Gaussian 1 vs 0:
  Original Loss: 0.0033310
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0033310 (Pseudo: 0.00%)
[Iter 4060/20000] Loss: 0.0019675 (Best: 0.0004960 @iter3790) ([92m↓31.04%[0m) [0.78% of initial]
[Iter 4060] Gaussian 0 vs 1:
  Original Loss: 0.0016984
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0016984 (Pseudo: 0.00%)
[Iter 4060] Gaussian 1 vs 0:
  Original Loss: 0.0017168
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0017168 (Pseudo: 0.00%)
[Iter 4070/20000] Loss: 0.0015495 (Best: 0.0004960 @iter3790) ([92m↓21.25%[0m) [0.62% of initial]
[Iter 4070] Gaussian 0 vs 1:
  Original Loss: 0.0013186
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0013186 (Pseudo: 0.00%)
[Iter 4070] Gaussian 1 vs 0:
  Original Loss: 0.0013433
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0013433 (Pseudo: 0.00%)
[Iter 4080/20000] Loss: 0.0014502 (Best: 0.0004960 @iter3790) ([92m↓6.41%[0m) [0.58% of initial]
[Iter 4080] Gaussian 0 vs 1:
  Original Loss: 0.0014219
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0014219 (Pseudo: 0.00%)
[Iter 4080] Gaussian 1 vs 0:
  Original Loss: 0.0014111
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0014111 (Pseudo: 0.00%)
[Iter 4090/20000] Loss: 0.0011408 (Best: 0.0004960 @iter3790) ([92m↓21.33%[0m) [0.45% of initial]
[Iter 4090] Gaussian 0 vs 1:
  Original Loss: 0.0009300
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0009300 (Pseudo: 0.00%)
[Iter 4090] Gaussian 1 vs 0:
  Original Loss: 0.0009206
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0009206 (Pseudo: 0.00%)
Iter:4099, L1 loss=0.0009519, Total loss=0.0009221, Time:26
[Iter 4100/20000] Loss: 0.0009774 (Best: 0.0004960 @iter3790) ([92m↓14.33%[0m) [0.39% of initial]
[Iter 4100] Gaussian 0 vs 1:
  Original Loss: 0.0008976
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0008976 (Pseudo: 0.00%)
[Iter 4100] Gaussian 1 vs 0:
  Original Loss: 0.0009014
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0009014 (Pseudo: 0.00%)
[Iter 4110/20000] Loss: 0.0010797 (Best: 0.0004960 @iter3790) ([91m↑10.47%[0m) [0.43% of initial]
[Iter 4110] Gaussian 0 vs 1:
  Original Loss: 0.0010649
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0010649 (Pseudo: 0.00%)
[Iter 4110] Gaussian 1 vs 0:
  Original Loss: 0.0010803
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0010803 (Pseudo: 0.00%)
[Iter 4120/20000] Loss: 0.0009807 (Best: 0.0004960 @iter3790) ([92m↓9.17%[0m) [0.39% of initial]
[Iter 4120] Gaussian 0 vs 1:
  Original Loss: 0.0008971
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0008971 (Pseudo: 0.00%)
[Iter 4120] Gaussian 1 vs 0:
  Original Loss: 0.0009273
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0009273 (Pseudo: 0.00%)
[Iter 4130/20000] Loss: 0.0011419 (Best: 0.0004960 @iter3790) ([91m↑16.44%[0m) [0.45% of initial]
[Iter 4130] Gaussian 0 vs 1:
  Original Loss: 0.0011315
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0011315 (Pseudo: 0.00%)
[Iter 4130] Gaussian 1 vs 0:
  Original Loss: 0.0011722
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0011722 (Pseudo: 0.00%)
[Iter 4140/20000] Loss: 0.0010394 (Best: 0.0004960 @iter3790) ([92m↓8.98%[0m) [0.41% of initial]
[Iter 4140] Gaussian 0 vs 1:
  Original Loss: 0.0009517
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0009517 (Pseudo: 0.00%)
[Iter 4140] Gaussian 1 vs 0:
  Original Loss: 0.0009529
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0009529 (Pseudo: 0.00%)
[Iter 4150/20000] Loss: 0.0008622 (Best: 0.0004960 @iter3790) ([92m↓17.05%[0m) [0.34% of initial]
[Iter 4150] Gaussian 0 vs 1:
  Original Loss: 0.0009051
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0009051 (Pseudo: 0.00%)
[Iter 4150] Gaussian 1 vs 0:
  Original Loss: 0.0009587
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0009587 (Pseudo: 0.00%)
[Iter 4160/20000] Loss: 0.0009796 (Best: 0.0004960 @iter3790) ([91m↑13.61%[0m) [0.39% of initial]
[Iter 4160] Gaussian 0 vs 1:
  Original Loss: 0.0010697
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0010697 (Pseudo: 0.00%)
[Iter 4160] Gaussian 1 vs 0:
  Original Loss: 0.0010509
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0010509 (Pseudo: 0.00%)
[Iter 4170/20000] Loss: 0.0008635 (Best: 0.0004960 @iter3790) ([92m↓11.85%[0m) [0.34% of initial]
[Iter 4170] Gaussian 0 vs 1:
  Original Loss: 0.0009053
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0009053 (Pseudo: 0.00%)
[Iter 4170] Gaussian 1 vs 0:
  Original Loss: 0.0009302
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0009302 (Pseudo: 0.00%)
[Iter 4180/20000] Loss: 0.0008697 (Best: 0.0004960 @iter3790) ([91m↑0.72%[0m) [0.35% of initial]
[Iter 4180] Gaussian 0 vs 1:
  Original Loss: 0.0008946
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0008946 (Pseudo: 0.00%)
[Iter 4180] Gaussian 1 vs 0:
  Original Loss: 0.0009215
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0009215 (Pseudo: 0.00%)
[Iter 4190/20000] Loss: 0.0008247 (Best: 0.0004960 @iter3790) ([92m↓5.18%[0m) [0.33% of initial]
[Iter 4190] Gaussian 0 vs 1:
  Original Loss: 0.0008724
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0008724 (Pseudo: 0.00%)
[Iter 4190] Gaussian 1 vs 0:
  Original Loss: 0.0008940
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0008940 (Pseudo: 0.00%)
Iter:4199, L1 loss=0.0007909, Total loss=0.0007209, Time:25
[Iter 4200/20000] Loss: 0.0008706 (Best: 0.0004960 @iter3790) ([91m↑5.57%[0m) [0.35% of initial]
[Iter 4200] Gaussian 0 vs 1:
  Original Loss: 0.0010202
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0010202 (Pseudo: 0.00%)
[Iter 4200] Gaussian 1 vs 0:
  Original Loss: 0.0009940
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0009940 (Pseudo: 0.00%)
[Iter 4210/20000] Loss: 0.0032216 (Best: 0.0004960 @iter3790) ([91m↑270.04%[0m) [1.28% of initial]
[Iter 4210] Gaussian 0 vs 1:
  Original Loss: 0.0029205
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0029205 (Pseudo: 0.00%)
[Iter 4210] Gaussian 1 vs 0:
  Original Loss: 0.0026561
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0026561 (Pseudo: 0.00%)
[Iter 4220/20000] Loss: 0.0017691 (Best: 0.0004960 @iter3790) ([92m↓45.09%[0m) [0.70% of initial]
[Iter 4220] Gaussian 0 vs 1:
  Original Loss: 0.0017586
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0017586 (Pseudo: 0.00%)
[Iter 4220] Gaussian 1 vs 0:
  Original Loss: 0.0018296
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0018296 (Pseudo: 0.00%)
[Iter 4230/20000] Loss: 0.0011901 (Best: 0.0004960 @iter3790) ([92m↓32.73%[0m) [0.47% of initial]
[Iter 4230] Gaussian 0 vs 1:
  Original Loss: 0.0010829
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0010829 (Pseudo: 0.00%)
[Iter 4230] Gaussian 1 vs 0:
  Original Loss: 0.0010628
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0010628 (Pseudo: 0.00%)
[Iter 4240/20000] Loss: 0.0009523 (Best: 0.0004960 @iter3790) ([92m↓19.99%[0m) [0.38% of initial]
[Iter 4240] Gaussian 0 vs 1:
  Original Loss: 0.0008983
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0008983 (Pseudo: 0.00%)
[Iter 4240] Gaussian 1 vs 0:
  Original Loss: 0.0009253
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0009253 (Pseudo: 0.00%)
[Iter 4250/20000] Loss: 0.0008985 (Best: 0.0004960 @iter3790) ([92m↓5.64%[0m) [0.36% of initial]
[Iter 4250] Gaussian 0 vs 1:
  Original Loss: 0.0008312
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0008312 (Pseudo: 0.00%)
[Iter 4250] Gaussian 1 vs 0:
  Original Loss: 0.0008220
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0008220 (Pseudo: 0.00%)
[Iter 4260/20000] Loss: 0.0009440 (Best: 0.0004960 @iter3790) ([91m↑5.06%[0m) [0.38% of initial]
[Iter 4260] Gaussian 0 vs 1:
  Original Loss: 0.0009777
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0009777 (Pseudo: 0.00%)
[Iter 4260] Gaussian 1 vs 0:
  Original Loss: 0.0010602
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0010602 (Pseudo: 0.00%)
[Iter 4270/20000] Loss: 0.0008684 (Best: 0.0004960 @iter3790) ([92m↓8.00%[0m) [0.35% of initial]
[Iter 4270] Gaussian 0 vs 1:
  Original Loss: 0.0008144
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0008144 (Pseudo: 0.00%)
[Iter 4270] Gaussian 1 vs 0:
  Original Loss: 0.0008384
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0008384 (Pseudo: 0.00%)
[Iter 4280/20000] Loss: 0.0007053 (Best: 0.0004960 @iter3790) ([92m↓18.78%[0m) [0.28% of initial]
[Iter 4280] Gaussian 0 vs 1:
  Original Loss: 0.0006289
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0006289 (Pseudo: 0.00%)
[Iter 4280] Gaussian 1 vs 0:
  Original Loss: 0.0006319
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0006319 (Pseudo: 0.00%)
[Iter 4290/20000] Loss: 0.0006906 (Best: 0.0004960 @iter3790) ([92m↓2.09%[0m) [0.27% of initial]
[Iter 4290] Gaussian 0 vs 1:
  Original Loss: 0.0007308
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0007308 (Pseudo: 0.00%)
[Iter 4290] Gaussian 1 vs 0:
  Original Loss: 0.0007305
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0007305 (Pseudo: 0.00%)
Iter:4299, L1 loss=0.0008367, Total loss=0.0008201, Time:26
[Iter 4300/20000] Loss: 0.0006946 (Best: 0.0004960 @iter3790) ([91m↑0.59%[0m) [0.28% of initial]
[Iter 4300] Gaussian 0 vs 1:
  Original Loss: 0.0005996
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0005996 (Pseudo: 0.00%)
[Iter 4300] Gaussian 1 vs 0:
  Original Loss: 0.0006044
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0006044 (Pseudo: 0.00%)
[Iter 4310/20000] Loss: 0.0006465 (Best: 0.0004960 @iter3790) ([92m↓6.93%[0m) [0.26% of initial]
[Iter 4310] Gaussian 0 vs 1:
  Original Loss: 0.0005864
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0005864 (Pseudo: 0.00%)
[Iter 4310] Gaussian 1 vs 0:
  Original Loss: 0.0005957
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0005957 (Pseudo: 0.00%)
[Iter 4320/20000] Loss: 0.0007737 (Best: 0.0004960 @iter3790) ([91m↑19.68%[0m) [0.31% of initial]
[Iter 4320] Gaussian 0 vs 1:
  Original Loss: 0.0007814
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0007814 (Pseudo: 0.00%)
[Iter 4320] Gaussian 1 vs 0:
  Original Loss: 0.0008116
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0008116 (Pseudo: 0.00%)
[Iter 4330/20000] Loss: 0.0006661 (Best: 0.0004960 @iter3790) ([92m↓13.90%[0m) [0.26% of initial]
[Iter 4330] Gaussian 0 vs 1:
  Original Loss: 0.0006113
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0006113 (Pseudo: 0.00%)
[Iter 4330] Gaussian 1 vs 0:
  Original Loss: 0.0006419
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0006419 (Pseudo: 0.00%)
[Iter 4340/20000] Loss: 0.0006244 (Best: 0.0004960 @iter3790) ([92m↓6.27%[0m) [0.25% of initial]
[Iter 4340] Gaussian 0 vs 1:
  Original Loss: 0.0005908
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0005908 (Pseudo: 0.00%)
[Iter 4340] Gaussian 1 vs 0:
  Original Loss: 0.0006147
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0006147 (Pseudo: 0.00%)
[Iter 4350/20000] Loss: 0.0006223 (Best: 0.0004960 @iter3790) ([92m↓0.34%[0m) [0.25% of initial]
[Iter 4350] Gaussian 0 vs 1:
  Original Loss: 0.0006552
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0006552 (Pseudo: 0.00%)
[Iter 4350] Gaussian 1 vs 0:
  Original Loss: 0.0006864
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0006864 (Pseudo: 0.00%)
[Iter 4360/20000] Loss: 0.0006343 (Best: 0.0004960 @iter3790) ([91m↑1.93%[0m) [0.25% of initial]
[Iter 4360] Gaussian 0 vs 1:
  Original Loss: 0.0006256
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0006256 (Pseudo: 0.00%)
[Iter 4360] Gaussian 1 vs 0:
  Original Loss: 0.0006072
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0006072 (Pseudo: 0.00%)
[Iter 4370/20000] Loss: 0.0006151 (Best: 0.0004960 @iter3790) ([92m↓3.01%[0m) [0.24% of initial]
[Iter 4370] Gaussian 0 vs 1:
  Original Loss: 0.0006500
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0006500 (Pseudo: 0.00%)
[Iter 4370] Gaussian 1 vs 0:
  Original Loss: 0.0006691
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0006691 (Pseudo: 0.00%)
[Iter 4380/20000] Loss: 0.0006585 (Best: 0.0004960 @iter3790) ([91m↑7.06%[0m) [0.26% of initial]
[Iter 4380] Gaussian 0 vs 1:
  Original Loss: 0.0006323
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0006323 (Pseudo: 0.00%)
[Iter 4380] Gaussian 1 vs 0:
  Original Loss: 0.0006524
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0006524 (Pseudo: 0.00%)
[Iter 4390/20000] Loss: 0.0006000 (Best: 0.0004960 @iter3790) ([92m↓8.89%[0m) [0.24% of initial]
[Iter 4390] Gaussian 0 vs 1:
  Original Loss: 0.0005894
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0005894 (Pseudo: 0.00%)
[Iter 4390] Gaussian 1 vs 0:
  Original Loss: 0.0005939
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0005939 (Pseudo: 0.00%)
Iter:4399, L1 loss=0.0005902, Total loss=0.0005228, Time:25
[Iter 4400/20000] Loss: 0.0005862 (Best: 0.0004960 @iter3790) ([92m↓2.29%[0m) [0.23% of initial]
[Iter 4400] Gaussian 0 vs 1:
  Original Loss: 0.0005987
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0005987 (Pseudo: 0.00%)
[Iter 4400] Gaussian 1 vs 0:
  Original Loss: 0.0006082
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0006082 (Pseudo: 0.00%)
[Iter 4410/20000] Loss: 0.0028615 (Best: 0.0004960 @iter3790) ([91m↑388.11%[0m) [1.14% of initial]
[Iter 4410] Gaussian 0 vs 1:
  Original Loss: 0.0030272
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0030272 (Pseudo: 0.00%)
[Iter 4410] Gaussian 1 vs 0:
  Original Loss: 0.0030397
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0030397 (Pseudo: 0.00%)
[Iter 4420/20000] Loss: 0.0014503 (Best: 0.0004960 @iter3790) ([92m↓49.31%[0m) [0.58% of initial]
[Iter 4420] Gaussian 0 vs 1:
  Original Loss: 0.0012164
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0012164 (Pseudo: 0.00%)
[Iter 4420] Gaussian 1 vs 0:
  Original Loss: 0.0012238
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0012238 (Pseudo: 0.00%)
[Iter 4430/20000] Loss: 0.0010519 (Best: 0.0004960 @iter3790) ([92m↓27.48%[0m) [0.42% of initial]
[Iter 4430] Gaussian 0 vs 1:
  Original Loss: 0.0009973
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0009973 (Pseudo: 0.00%)
[Iter 4430] Gaussian 1 vs 0:
  Original Loss: 0.0010715
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0010715 (Pseudo: 0.00%)
[Iter 4440/20000] Loss: 0.0008089 (Best: 0.0004960 @iter3790) ([92m↓23.10%[0m) [0.32% of initial]
[Iter 4440] Gaussian 0 vs 1:
  Original Loss: 0.0008589
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0008589 (Pseudo: 0.00%)
[Iter 4440] Gaussian 1 vs 0:
  Original Loss: 0.0008996
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0008996 (Pseudo: 0.00%)
[Iter 4450/20000] Loss: 0.0006922 (Best: 0.0004960 @iter3790) ([92m↓14.43%[0m) [0.28% of initial]
[Iter 4450] Gaussian 0 vs 1:
  Original Loss: 0.0006378
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0006378 (Pseudo: 0.00%)
[Iter 4450] Gaussian 1 vs 0:
  Original Loss: 0.0006858
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0006858 (Pseudo: 0.00%)
[Iter 4460/20000] Loss: 0.0006666 (Best: 0.0004960 @iter3790) ([92m↓3.70%[0m) [0.26% of initial]
[Iter 4460] Gaussian 0 vs 1:
  Original Loss: 0.0006955
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0006955 (Pseudo: 0.00%)
[Iter 4460] Gaussian 1 vs 0:
  Original Loss: 0.0007196
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0007196 (Pseudo: 0.00%)
[Iter 4470/20000] Loss: 0.0007264 (Best: 0.0004960 @iter3790) ([91m↑8.97%[0m) [0.29% of initial]
[Iter 4470] Gaussian 0 vs 1:
  Original Loss: 0.0008609
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0008609 (Pseudo: 0.00%)
[Iter 4470] Gaussian 1 vs 0:
  Original Loss: 0.0008449
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0008449 (Pseudo: 0.00%)
[Iter 4480/20000] Loss: 0.0006834 (Best: 0.0004960 @iter3790) ([92m↓5.92%[0m) [0.27% of initial]
[Iter 4480] Gaussian 0 vs 1:
  Original Loss: 0.0006641
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0006641 (Pseudo: 0.00%)
[Iter 4480] Gaussian 1 vs 0:
  Original Loss: 0.0006770
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0006770 (Pseudo: 0.00%)
[Iter 4490/20000] Loss: 0.0006792 (Best: 0.0004960 @iter3790) ([92m↓0.62%[0m) [0.27% of initial]
[Iter 4490] Gaussian 0 vs 1:
  Original Loss: 0.0006402
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0006402 (Pseudo: 0.00%)
[Iter 4490] Gaussian 1 vs 0:
  Original Loss: 0.0006430
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0006430 (Pseudo: 0.00%)
Iter:4499, L1 loss=0.0007677, Total loss=0.0007554, Time:25
[Iter 4500/20000] Loss: 0.0007754 (Best: 0.0004960 @iter3790) ([91m↑14.16%[0m) [0.31% of initial]
[Iter 4500] Gaussian 0 vs 1:
  Original Loss: 0.0008015
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0008015 (Pseudo: 0.00%)
[Iter 4500] Gaussian 1 vs 0:
  Original Loss: 0.0007658
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0007658 (Pseudo: 0.00%)
[Iter 4510/20000] Loss: 0.0005671 (Best: 0.0004960 @iter3790) ([92m↓26.86%[0m) [0.23% of initial]
[Iter 4510] Gaussian 0 vs 1:
  Original Loss: 0.0004978
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0004978 (Pseudo: 0.00%)
[Iter 4510] Gaussian 1 vs 0:
  Original Loss: 0.0005075
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0005075 (Pseudo: 0.00%)
[Iter 4520/20000] Loss: 0.0005902 (Best: 0.0004960 @iter3790) ([91m↑4.07%[0m) [0.23% of initial]
[Iter 4520] Gaussian 0 vs 1:
  Original Loss: 0.0005389
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0005389 (Pseudo: 0.00%)
[Iter 4520] Gaussian 1 vs 0:
  Original Loss: 0.0005248
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0005248 (Pseudo: 0.00%)
[Iter 4530/20000] Loss: 0.0006506 (Best: 0.0004960 @iter3790) ([91m↑10.23%[0m) [0.26% of initial]
[Iter 4530] Gaussian 0 vs 1:
  Original Loss: 0.0007248
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0007248 (Pseudo: 0.00%)
[Iter 4530] Gaussian 1 vs 0:
  Original Loss: 0.0007388
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0007388 (Pseudo: 0.00%)
[Iter 4540/20000] Loss: 0.0005850 (Best: 0.0004960 @iter3790) ([92m↓10.09%[0m) [0.23% of initial]
[Iter 4540] Gaussian 0 vs 1:
  Original Loss: 0.0005693
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0005693 (Pseudo: 0.00%)
[Iter 4540] Gaussian 1 vs 0:
  Original Loss: 0.0005809
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0005809 (Pseudo: 0.00%)
[Iter 4550/20000] Loss: 0.0006076 (Best: 0.0004958 @iter4546) ([91m↑3.87%[0m) [0.24% of initial]
[Iter 4550] Gaussian 0 vs 1:
  Original Loss: 0.0006686
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0006686 (Pseudo: 0.00%)
[Iter 4550] Gaussian 1 vs 0:
  Original Loss: 0.0006609
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0006609 (Pseudo: 0.00%)
[Iter 4560/20000] Loss: 0.0006277 (Best: 0.0004958 @iter4546) ([91m↑3.31%[0m) [0.25% of initial]
[Iter 4560] Gaussian 0 vs 1:
  Original Loss: 0.0006137
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0006137 (Pseudo: 0.00%)
[Iter 4560] Gaussian 1 vs 0:
  Original Loss: 0.0006441
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0006441 (Pseudo: 0.00%)
[Iter 4570/20000] Loss: 0.0005608 (Best: 0.0004958 @iter4546) ([92m↓10.65%[0m) [0.22% of initial]
[Iter 4570] Gaussian 0 vs 1:
  Original Loss: 0.0005010
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0005010 (Pseudo: 0.00%)
[Iter 4570] Gaussian 1 vs 0:
  Original Loss: 0.0005075
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0005075 (Pseudo: 0.00%)
[Iter 4580/20000] Loss: 0.0005579 (Best: 0.0004958 @iter4546) ([92m↓0.53%[0m) [0.22% of initial]
[Iter 4580] Gaussian 0 vs 1:
  Original Loss: 0.0005346
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0005346 (Pseudo: 0.00%)
[Iter 4580] Gaussian 1 vs 0:
  Original Loss: 0.0005462
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0005462 (Pseudo: 0.00%)
[Iter 4590/20000] Loss: 0.0006025 (Best: 0.0004958 @iter4546) ([91m↑8.01%[0m) [0.24% of initial]
[Iter 4590] Gaussian 0 vs 1:
  Original Loss: 0.0006562
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0006562 (Pseudo: 0.00%)
[Iter 4590] Gaussian 1 vs 0:
  Original Loss: 0.0007002
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0007002 (Pseudo: 0.00%)
Iter:4599, L1 loss=0.0006549, Total loss=0.000591, Time:25
[Iter 4600/20000] Loss: 0.0005809 (Best: 0.0004958 @iter4546) ([92m↓3.59%[0m) [0.23% of initial]
[Iter 4600] Gaussian 0 vs 1:
  Original Loss: 0.0005290
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0005290 (Pseudo: 0.00%)
[Iter 4600] Gaussian 1 vs 0:
  Original Loss: 0.0005785
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0005785 (Pseudo: 0.00%)
[Iter 4610/20000] Loss: 0.0023845 (Best: 0.0004958 @iter4546) ([91m↑310.48%[0m) [0.95% of initial]
[Iter 4610] Gaussian 0 vs 1:
  Original Loss: 0.0021444
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0021444 (Pseudo: 0.00%)
[Iter 4610] Gaussian 1 vs 0:
  Original Loss: 0.0021264
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0021264 (Pseudo: 0.00%)
[Iter 4620/20000] Loss: 0.0013882 (Best: 0.0004958 @iter4546) ([92m↓41.78%[0m) [0.55% of initial]
[Iter 4620] Gaussian 0 vs 1:
  Original Loss: 0.0013074
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0013074 (Pseudo: 0.00%)
[Iter 4620] Gaussian 1 vs 0:
  Original Loss: 0.0013730
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0013730 (Pseudo: 0.00%)
[Iter 4630/20000] Loss: 0.0009955 (Best: 0.0004958 @iter4546) ([92m↓28.29%[0m) [0.40% of initial]
[Iter 4630] Gaussian 0 vs 1:
  Original Loss: 0.0008966
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0008966 (Pseudo: 0.00%)
[Iter 4630] Gaussian 1 vs 0:
  Original Loss: 0.0008770
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0008770 (Pseudo: 0.00%)
[Iter 4640/20000] Loss: 0.0007858 (Best: 0.0004958 @iter4546) ([92m↓21.06%[0m) [0.31% of initial]
[Iter 4640] Gaussian 0 vs 1:
  Original Loss: 0.0007518
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0007518 (Pseudo: 0.00%)
[Iter 4640] Gaussian 1 vs 0:
  Original Loss: 0.0007265
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0007265 (Pseudo: 0.00%)
[Iter 4650/20000] Loss: 0.0006614 (Best: 0.0004958 @iter4546) ([92m↓15.83%[0m) [0.26% of initial]
[Iter 4650] Gaussian 0 vs 1:
  Original Loss: 0.0006522
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0006522 (Pseudo: 0.00%)
[Iter 4650] Gaussian 1 vs 0:
  Original Loss: 0.0006806
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0006806 (Pseudo: 0.00%)
[Iter 4660/20000] Loss: 0.0005725 (Best: 0.0004958 @iter4546) ([92m↓13.45%[0m) [0.23% of initial]
[Iter 4660] Gaussian 0 vs 1:
  Original Loss: 0.0005074
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0005074 (Pseudo: 0.00%)
[Iter 4660] Gaussian 1 vs 0:
  Original Loss: 0.0005063
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0005063 (Pseudo: 0.00%)
[Iter 4670/20000] Loss: 0.0005406 (Best: 0.0004958 @iter4546) ([92m↓5.56%[0m) [0.21% of initial]
[Iter 4670] Gaussian 0 vs 1:
  Original Loss: 0.0005083
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0005083 (Pseudo: 0.00%)
[Iter 4670] Gaussian 1 vs 0:
  Original Loss: 0.0005127
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0005127 (Pseudo: 0.00%)
[Iter 4680/20000] Loss: 0.0005401 (Best: 0.0004723 @iter4672) ([92m↓0.11%[0m) [0.21% of initial]
[Iter 4680] Gaussian 0 vs 1:
  Original Loss: 0.0005231
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0005231 (Pseudo: 0.00%)
[Iter 4680] Gaussian 1 vs 0:
  Original Loss: 0.0005377
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0005377 (Pseudo: 0.00%)
[Iter 4690/20000] Loss: 0.0005031 (Best: 0.0004613 @iter4681) ([92m↓6.85%[0m) [0.20% of initial]
[Iter 4690] Gaussian 0 vs 1:
  Original Loss: 0.0004639
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0004639 (Pseudo: 0.00%)
[Iter 4690] Gaussian 1 vs 0:
  Original Loss: 0.0004734
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0004734 (Pseudo: 0.00%)
Iter:4699, L1 loss=0.0006205, Total loss=0.0005877, Time:26
[Iter 4700/20000] Loss: 0.0005593 (Best: 0.0004613 @iter4681) ([91m↑11.19%[0m) [0.22% of initial]
[Iter 4700] Gaussian 0 vs 1:
  Original Loss: 0.0005397
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0005397 (Pseudo: 0.00%)
[Iter 4700] Gaussian 1 vs 0:
  Original Loss: 0.0005435
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0005435 (Pseudo: 0.00%)
[Iter 4710/20000] Loss: 0.0005006 (Best: 0.0004613 @iter4681) ([92m↓10.50%[0m) [0.20% of initial]
[Iter 4710] Gaussian 0 vs 1:
  Original Loss: 0.0004801
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0004801 (Pseudo: 0.00%)
[Iter 4710] Gaussian 1 vs 0:
  Original Loss: 0.0004784
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0004784 (Pseudo: 0.00%)
[Iter 4720/20000] Loss: 0.0005441 (Best: 0.0004274 @iter4711) ([91m↑8.68%[0m) [0.22% of initial]
[Iter 4720] Gaussian 0 vs 1:
  Original Loss: 0.0005148
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0005148 (Pseudo: 0.00%)
[Iter 4720] Gaussian 1 vs 0:
  Original Loss: 0.0005481
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0005481 (Pseudo: 0.00%)
[Iter 4730/20000] Loss: 0.0005439 (Best: 0.0004274 @iter4711) ([92m↓0.02%[0m) [0.22% of initial]
[Iter 4730] Gaussian 0 vs 1:
  Original Loss: 0.0005787
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0005787 (Pseudo: 0.00%)
[Iter 4730] Gaussian 1 vs 0:
  Original Loss: 0.0005923
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0005923 (Pseudo: 0.00%)
[Iter 4740/20000] Loss: 0.0006026 (Best: 0.0004274 @iter4711) ([91m↑10.79%[0m) [0.24% of initial]
[Iter 4740] Gaussian 0 vs 1:
  Original Loss: 0.0006525
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0006525 (Pseudo: 0.00%)
[Iter 4740] Gaussian 1 vs 0:
  Original Loss: 0.0006744
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0006744 (Pseudo: 0.00%)
[Iter 4750/20000] Loss: 0.0005872 (Best: 0.0004274 @iter4711) ([92m↓2.57%[0m) [0.23% of initial]
[Iter 4750] Gaussian 0 vs 1:
  Original Loss: 0.0005234
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0005234 (Pseudo: 0.00%)
[Iter 4750] Gaussian 1 vs 0:
  Original Loss: 0.0005343
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0005343 (Pseudo: 0.00%)
[Iter 4760/20000] Loss: 0.0005171 (Best: 0.0004274 @iter4711) ([92m↓11.94%[0m) [0.21% of initial]
[Iter 4760] Gaussian 0 vs 1:
  Original Loss: 0.0005165
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0005165 (Pseudo: 0.00%)
[Iter 4760] Gaussian 1 vs 0:
  Original Loss: 0.0005232
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0005232 (Pseudo: 0.00%)
[Iter 4770/20000] Loss: 0.0005719 (Best: 0.0004274 @iter4711) ([91m↑10.61%[0m) [0.23% of initial]
[Iter 4770] Gaussian 0 vs 1:
  Original Loss: 0.0006342
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0006342 (Pseudo: 0.00%)
[Iter 4770] Gaussian 1 vs 0:
  Original Loss: 0.0006249
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0006249 (Pseudo: 0.00%)
[Iter 4780/20000] Loss: 0.0005989 (Best: 0.0004274 @iter4711) ([91m↑4.72%[0m) [0.24% of initial]
[Iter 4780] Gaussian 0 vs 1:
  Original Loss: 0.0006204
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0006204 (Pseudo: 0.00%)
[Iter 4780] Gaussian 1 vs 0:
  Original Loss: 0.0006187
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0006187 (Pseudo: 0.00%)
[Iter 4790/20000] Loss: 0.0005370 (Best: 0.0004274 @iter4711) ([92m↓10.34%[0m) [0.21% of initial]
[Iter 4790] Gaussian 0 vs 1:
  Original Loss: 0.0005410
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0005410 (Pseudo: 0.00%)
[Iter 4790] Gaussian 1 vs 0:
  Original Loss: 0.0005353
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0005353 (Pseudo: 0.00%)
Iter:4799, L1 loss=0.0006151, Total loss=0.0005367, Time:26
[Iter 4800/20000] Loss: 0.0006011 (Best: 0.0004274 @iter4711) ([91m↑11.93%[0m) [0.24% of initial]
[Iter 4800] Gaussian 0 vs 1:
  Original Loss: 0.0005989
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0005989 (Pseudo: 0.00%)
[Iter 4800] Gaussian 1 vs 0:
  Original Loss: 0.0006200
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0006200 (Pseudo: 0.00%)
[Iter 4810/20000] Loss: 0.0022142 (Best: 0.0004274 @iter4711) ([91m↑268.38%[0m) [0.88% of initial]
[Iter 4810] Gaussian 0 vs 1:
  Original Loss: 0.0019742
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0019742 (Pseudo: 0.00%)
[Iter 4810] Gaussian 1 vs 0:
  Original Loss: 0.0019924
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0019924 (Pseudo: 0.00%)
[Iter 4820/20000] Loss: 0.0014381 (Best: 0.0004274 @iter4711) ([92m↓35.05%[0m) [0.57% of initial]
[Iter 4820] Gaussian 0 vs 1:
  Original Loss: 0.0013612
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0013612 (Pseudo: 0.00%)
[Iter 4820] Gaussian 1 vs 0:
  Original Loss: 0.0012232
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0012232 (Pseudo: 0.00%)
[Iter 4830/20000] Loss: 0.0010098 (Best: 0.0004274 @iter4711) ([92m↓29.78%[0m) [0.40% of initial]
[Iter 4830] Gaussian 0 vs 1:
  Original Loss: 0.0011656
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0011656 (Pseudo: 0.00%)
[Iter 4830] Gaussian 1 vs 0:
  Original Loss: 0.0012024
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0012024 (Pseudo: 0.00%)
[Iter 4840/20000] Loss: 0.0007297 (Best: 0.0004274 @iter4711) ([92m↓27.74%[0m) [0.29% of initial]
[Iter 4840] Gaussian 0 vs 1:
  Original Loss: 0.0007068
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0007068 (Pseudo: 0.00%)
[Iter 4840] Gaussian 1 vs 0:
  Original Loss: 0.0007453
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0007453 (Pseudo: 0.00%)
[Iter 4850/20000] Loss: 0.0005775 (Best: 0.0004274 @iter4711) ([92m↓20.86%[0m) [0.23% of initial]
[Iter 4850] Gaussian 0 vs 1:
  Original Loss: 0.0005885
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0005885 (Pseudo: 0.00%)
[Iter 4850] Gaussian 1 vs 0:
  Original Loss: 0.0005878
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0005878 (Pseudo: 0.00%)
[Iter 4860/20000] Loss: 0.0005751 (Best: 0.0004274 @iter4711) ([92m↓0.42%[0m) [0.23% of initial]
[Iter 4860] Gaussian 0 vs 1:
  Original Loss: 0.0005606
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0005606 (Pseudo: 0.00%)
[Iter 4860] Gaussian 1 vs 0:
  Original Loss: 0.0005522
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0005522 (Pseudo: 0.00%)
[Iter 4870/20000] Loss: 0.0005010 (Best: 0.0004274 @iter4711) ([92m↓12.89%[0m) [0.20% of initial]
[Iter 4870] Gaussian 0 vs 1:
  Original Loss: 0.0004794
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0004794 (Pseudo: 0.00%)
[Iter 4870] Gaussian 1 vs 0:
  Original Loss: 0.0004906
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0004906 (Pseudo: 0.00%)
[Iter 4880/20000] Loss: 0.0005299 (Best: 0.0004274 @iter4711) ([91m↑5.78%[0m) [0.21% of initial]
[Iter 4880] Gaussian 0 vs 1:
  Original Loss: 0.0005882
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0005882 (Pseudo: 0.00%)
[Iter 4880] Gaussian 1 vs 0:
  Original Loss: 0.0006252
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0006252 (Pseudo: 0.00%)
[Iter 4890/20000] Loss: 0.0005025 (Best: 0.0004274 @iter4711) ([92m↓5.18%[0m) [0.20% of initial]
[Iter 4890] Gaussian 0 vs 1:
  Original Loss: 0.0004738
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0004738 (Pseudo: 0.00%)
[Iter 4890] Gaussian 1 vs 0:
  Original Loss: 0.0004713
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0004713 (Pseudo: 0.00%)
Iter:4899, L1 loss=0.0006285, Total loss=0.0006064, Time:26
[Iter 4900/20000] Loss: 0.0005066 (Best: 0.0004274 @iter4711) ([91m↑0.82%[0m) [0.20% of initial]
[Iter 4900] Gaussian 0 vs 1:
  Original Loss: 0.0004588
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0004588 (Pseudo: 0.00%)
[Iter 4900] Gaussian 1 vs 0:
  Original Loss: 0.0004574
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0004574 (Pseudo: 0.00%)
[Iter 4910/20000] Loss: 0.0006128 (Best: 0.0004274 @iter4711) ([91m↑20.96%[0m) [0.24% of initial]
[Iter 4910] Gaussian 0 vs 1:
  Original Loss: 0.0006989
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0006989 (Pseudo: 0.00%)
[Iter 4910] Gaussian 1 vs 0:
  Original Loss: 0.0007132
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0007132 (Pseudo: 0.00%)
[Iter 4920/20000] Loss: 0.0005358 (Best: 0.0004274 @iter4711) ([92m↓12.57%[0m) [0.21% of initial]
[Iter 4920] Gaussian 0 vs 1:
  Original Loss: 0.0005164
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0005164 (Pseudo: 0.00%)
[Iter 4920] Gaussian 1 vs 0:
  Original Loss: 0.0005265
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0005265 (Pseudo: 0.00%)
[Iter 4930/20000] Loss: 0.0004932 (Best: 0.0004274 @iter4711) ([92m↓7.94%[0m) [0.20% of initial]
[Iter 4930] Gaussian 0 vs 1:
  Original Loss: 0.0004460
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0004460 (Pseudo: 0.00%)
[Iter 4930] Gaussian 1 vs 0:
  Original Loss: 0.0004301
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0004301 (Pseudo: 0.00%)
[Iter 4940/20000] Loss: 0.0005007 (Best: 0.0004239 @iter4936) ([91m↑1.51%[0m) [0.20% of initial]
[Iter 4940] Gaussian 0 vs 1:
  Original Loss: 0.0004696
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0004696 (Pseudo: 0.00%)
[Iter 4940] Gaussian 1 vs 0:
  Original Loss: 0.0004740
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0004740 (Pseudo: 0.00%)
[Iter 4950/20000] Loss: 0.0004395 (Best: 0.0003990 @iter4949) ([92m↓12.22%[0m) [0.17% of initial]
[Iter 4950] Gaussian 0 vs 1:
  Original Loss: 0.0004491
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0004491 (Pseudo: 0.00%)
[Iter 4950] Gaussian 1 vs 0:
  Original Loss: 0.0004665
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0004665 (Pseudo: 0.00%)
[Iter 4960/20000] Loss: 0.0004730 (Best: 0.0003919 @iter4957) ([91m↑7.61%[0m) [0.19% of initial]
[Iter 4960] Gaussian 0 vs 1:
  Original Loss: 0.0004816
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0004816 (Pseudo: 0.00%)
[Iter 4960] Gaussian 1 vs 0:
  Original Loss: 0.0004924
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0004924 (Pseudo: 0.00%)
[Iter 4970/20000] Loss: 0.0004916 (Best: 0.0003919 @iter4957) ([91m↑3.95%[0m) [0.20% of initial]
[Iter 4970] Gaussian 0 vs 1:
  Original Loss: 0.0005132
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0005132 (Pseudo: 0.00%)
[Iter 4970] Gaussian 1 vs 0:
  Original Loss: 0.0005291
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0005291 (Pseudo: 0.00%)
[Iter 4980/20000] Loss: 0.0005039 (Best: 0.0003919 @iter4957) ([91m↑2.51%[0m) [0.20% of initial]
[Iter 4980] Gaussian 0 vs 1:
  Original Loss: 0.0004982
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0004982 (Pseudo: 0.00%)
[Iter 4980] Gaussian 1 vs 0:
  Original Loss: 0.0005025
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0005025 (Pseudo: 0.00%)
[Iter 4990/20000] Loss: 0.0004720 (Best: 0.0003919 @iter4957) ([92m↓6.34%[0m) [0.19% of initial]
[Iter 4990] Gaussian 0 vs 1:
  Original Loss: 0.0004472
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0004472 (Pseudo: 0.00%)
[Iter 4990] Gaussian 1 vs 0:
  Original Loss: 0.0004742
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0004742 (Pseudo: 0.00%)
Iter:4999, L1 loss=0.0005039, Total loss=0.0004511, Time:26
[Iter 5000/20000] Loss: 0.0004504 (Best: 0.0003919 @iter4957) ([92m↓4.57%[0m) [0.18% of initial]
[Iter 5000] Gaussian 0 vs 1:
  Original Loss: 0.0004456
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0004456 (Pseudo: 0.00%)
[Iter 5000] Gaussian 1 vs 0:
  Original Loss: 0.0004504
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0004504 (Pseudo: 0.00%)
[Iter 5010/20000] Loss: 0.0020000 (Best: 0.0003919 @iter4957) ([91m↑344.01%[0m) [0.79% of initial]
[Iter 5010] Gaussian 0 vs 1:
  Original Loss: 0.0017774
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0017774 (Pseudo: 0.00%)
[Iter 5010] Gaussian 1 vs 0:
  Original Loss: 0.0018643
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0018643 (Pseudo: 0.00%)
[Iter 5020/20000] Loss: 0.0012416 (Best: 0.0003919 @iter4957) ([92m↓37.92%[0m) [0.49% of initial]
[Iter 5020] Gaussian 0 vs 1:
  Original Loss: 0.0010813
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0010813 (Pseudo: 0.00%)
[Iter 5020] Gaussian 1 vs 0:
  Original Loss: 0.0011327
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0011327 (Pseudo: 0.00%)
[Iter 5030/20000] Loss: 0.0007899 (Best: 0.0003919 @iter4957) ([92m↓36.38%[0m) [0.31% of initial]
[Iter 5030] Gaussian 0 vs 1:
  Original Loss: 0.0007408
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0007408 (Pseudo: 0.00%)
[Iter 5030] Gaussian 1 vs 0:
  Original Loss: 0.0007635
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0007635 (Pseudo: 0.00%)
[Iter 5040/20000] Loss: 0.0007624 (Best: 0.0003919 @iter4957) ([92m↓3.49%[0m) [0.30% of initial]
[Iter 5040] Gaussian 0 vs 1:
  Original Loss: 0.0009047
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0009047 (Pseudo: 0.00%)
[Iter 5040] Gaussian 1 vs 0:
  Original Loss: 0.0010128
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0010128 (Pseudo: 0.00%)
[Iter 5050/20000] Loss: 0.0006661 (Best: 0.0003919 @iter4957) ([92m↓12.62%[0m) [0.26% of initial]
[Iter 5050] Gaussian 0 vs 1:
  Original Loss: 0.0006240
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0006240 (Pseudo: 0.00%)
[Iter 5050] Gaussian 1 vs 0:
  Original Loss: 0.0006461
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0006461 (Pseudo: 0.00%)
[Iter 5060/20000] Loss: 0.0005100 (Best: 0.0003919 @iter4957) ([92m↓23.43%[0m) [0.20% of initial]
[Iter 5060] Gaussian 0 vs 1:
  Original Loss: 0.0004923
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0004923 (Pseudo: 0.00%)
[Iter 5060] Gaussian 1 vs 0:
  Original Loss: 0.0004990
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0004990 (Pseudo: 0.00%)
[Iter 5070/20000] Loss: 0.0005726 (Best: 0.0003919 @iter4957) ([91m↑12.28%[0m) [0.23% of initial]
[Iter 5070] Gaussian 0 vs 1:
  Original Loss: 0.0005774
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0005774 (Pseudo: 0.00%)
[Iter 5070] Gaussian 1 vs 0:
  Original Loss: 0.0006201
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0006201 (Pseudo: 0.00%)
[Iter 5080/20000] Loss: 0.0004795 (Best: 0.0003919 @iter4957) ([92m↓16.27%[0m) [0.19% of initial]
[Iter 5080] Gaussian 0 vs 1:
  Original Loss: 0.0004328
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0004328 (Pseudo: 0.00%)
[Iter 5080] Gaussian 1 vs 0:
  Original Loss: 0.0004625
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0004625 (Pseudo: 0.00%)
[Iter 5090/20000] Loss: 0.0005054 (Best: 0.0003919 @iter4957) ([91m↑5.39%[0m) [0.20% of initial]
[Iter 5090] Gaussian 0 vs 1:
  Original Loss: 0.0004937
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0004937 (Pseudo: 0.00%)
[Iter 5090] Gaussian 1 vs 0:
  Original Loss: 0.0005162
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0005162 (Pseudo: 0.00%)
Iter:5099, L1 loss=0.000589, Total loss=0.0005209, Time:26
[Iter 5100/20000] Loss: 0.0005343 (Best: 0.0003919 @iter4957) ([91m↑5.73%[0m) [0.21% of initial]
[Iter 5100] Gaussian 0 vs 1:
  Original Loss: 0.0005605
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0005605 (Pseudo: 0.00%)
[Iter 5100] Gaussian 1 vs 0:
  Original Loss: 0.0005813
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0005813 (Pseudo: 0.00%)
[Iter 5110/20000] Loss: 0.0005233 (Best: 0.0003919 @iter4957) ([92m↓2.05%[0m) [0.21% of initial]
[Iter 5110] Gaussian 0 vs 1:
  Original Loss: 0.0005190
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0005190 (Pseudo: 0.00%)
[Iter 5110] Gaussian 1 vs 0:
  Original Loss: 0.0005381
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0005381 (Pseudo: 0.00%)
[Iter 5120/20000] Loss: 0.0005067 (Best: 0.0003919 @iter4957) ([92m↓3.18%[0m) [0.20% of initial]
[Iter 5120] Gaussian 0 vs 1:
  Original Loss: 0.0004883
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0004883 (Pseudo: 0.00%)
[Iter 5120] Gaussian 1 vs 0:
  Original Loss: 0.0004883
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0004883 (Pseudo: 0.00%)
[Iter 5130/20000] Loss: 0.0005346 (Best: 0.0003919 @iter4957) ([91m↑5.52%[0m) [0.21% of initial]
[Iter 5130] Gaussian 0 vs 1:
  Original Loss: 0.0005585
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0005585 (Pseudo: 0.00%)
[Iter 5130] Gaussian 1 vs 0:
  Original Loss: 0.0005183
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0005183 (Pseudo: 0.00%)
[Iter 5140/20000] Loss: 0.0004594 (Best: 0.0003919 @iter4957) ([92m↓14.07%[0m) [0.18% of initial]
[Iter 5140] Gaussian 0 vs 1:
  Original Loss: 0.0004250
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0004250 (Pseudo: 0.00%)
[Iter 5140] Gaussian 1 vs 0:
  Original Loss: 0.0004314
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0004314 (Pseudo: 0.00%)
[Iter 5150/20000] Loss: 0.0004578 (Best: 0.0003919 @iter4957) ([92m↓0.37%[0m) [0.18% of initial]
[Iter 5150] Gaussian 0 vs 1:
  Original Loss: 0.0004697
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0004697 (Pseudo: 0.00%)
[Iter 5150] Gaussian 1 vs 0:
  Original Loss: 0.0004655
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0004655 (Pseudo: 0.00%)
[Iter 5160/20000] Loss: 0.0004514 (Best: 0.0003852 @iter5158) ([92m↓1.39%[0m) [0.18% of initial]
[Iter 5160] Gaussian 0 vs 1:
  Original Loss: 0.0005203
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0005203 (Pseudo: 0.00%)
[Iter 5160] Gaussian 1 vs 0:
  Original Loss: 0.0005114
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0005114 (Pseudo: 0.00%)
[Iter 5170/20000] Loss: 0.0004985 (Best: 0.0003852 @iter5158) ([91m↑10.44%[0m) [0.20% of initial]
[Iter 5170] Gaussian 0 vs 1:
  Original Loss: 0.0004631
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0004631 (Pseudo: 0.00%)
[Iter 5170] Gaussian 1 vs 0:
  Original Loss: 0.0004651
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0004651 (Pseudo: 0.00%)
[Iter 5180/20000] Loss: 0.0004599 (Best: 0.0003852 @iter5158) ([92m↓7.74%[0m) [0.18% of initial]
[Iter 5180] Gaussian 0 vs 1:
  Original Loss: 0.0004285
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0004285 (Pseudo: 0.00%)
[Iter 5180] Gaussian 1 vs 0:
  Original Loss: 0.0004279
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0004279 (Pseudo: 0.00%)
[Iter 5190/20000] Loss: 0.0004641 (Best: 0.0003852 @iter5158) ([91m↑0.91%[0m) [0.18% of initial]
[Iter 5190] Gaussian 0 vs 1:
  Original Loss: 0.0004652
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0004652 (Pseudo: 0.00%)
[Iter 5190] Gaussian 1 vs 0:
  Original Loss: 0.0004603
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0004603 (Pseudo: 0.00%)
Iter:5199, L1 loss=0.0005453, Total loss=0.0004728, Time:26
[Iter 5200/20000] Loss: 0.0004447 (Best: 0.0003852 @iter5158) ([92m↓4.19%[0m) [0.18% of initial]
[Iter 5200] Gaussian 0 vs 1:
  Original Loss: 0.0004282
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0004282 (Pseudo: 0.00%)
[Iter 5200] Gaussian 1 vs 0:
  Original Loss: 0.0004426
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0004426 (Pseudo: 0.00%)
[Iter 5210/20000] Loss: 0.0020439 (Best: 0.0003852 @iter5158) ([91m↑359.63%[0m) [0.81% of initial]
[Iter 5210] Gaussian 0 vs 1:
  Original Loss: 0.0019072
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0019072 (Pseudo: 0.00%)
[Iter 5210] Gaussian 1 vs 0:
  Original Loss: 0.0017368
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0017368 (Pseudo: 0.00%)
[Iter 5220/20000] Loss: 0.0010863 (Best: 0.0003852 @iter5158) ([92m↓46.85%[0m) [0.43% of initial]
[Iter 5220] Gaussian 0 vs 1:
  Original Loss: 0.0009518
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0009518 (Pseudo: 0.00%)
[Iter 5220] Gaussian 1 vs 0:
  Original Loss: 0.0009353
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0009353 (Pseudo: 0.00%)
[Iter 5230/20000] Loss: 0.0007540 (Best: 0.0003852 @iter5158) ([92m↓30.60%[0m) [0.30% of initial]
[Iter 5230] Gaussian 0 vs 1:
  Original Loss: 0.0006424
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0006424 (Pseudo: 0.00%)
[Iter 5230] Gaussian 1 vs 0:
  Original Loss: 0.0006366
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0006366 (Pseudo: 0.00%)
[Iter 5240/20000] Loss: 0.0005946 (Best: 0.0003852 @iter5158) ([92m↓21.13%[0m) [0.24% of initial]
[Iter 5240] Gaussian 0 vs 1:
  Original Loss: 0.0005735
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0005735 (Pseudo: 0.00%)
[Iter 5240] Gaussian 1 vs 0:
  Original Loss: 0.0005707
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0005707 (Pseudo: 0.00%)
[Iter 5250/20000] Loss: 0.0007194 (Best: 0.0003852 @iter5158) ([91m↑20.97%[0m) [0.29% of initial]
[Iter 5250] Gaussian 0 vs 1:
  Original Loss: 0.0008054
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0008054 (Pseudo: 0.00%)
[Iter 5250] Gaussian 1 vs 0:
  Original Loss: 0.0007992
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0007992 (Pseudo: 0.00%)
[Iter 5260/20000] Loss: 0.0005719 (Best: 0.0003852 @iter5158) ([92m↓20.50%[0m) [0.23% of initial]
[Iter 5260] Gaussian 0 vs 1:
  Original Loss: 0.0005432
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0005432 (Pseudo: 0.00%)
[Iter 5260] Gaussian 1 vs 0:
  Original Loss: 0.0005679
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0005679 (Pseudo: 0.00%)
[Iter 5270/20000] Loss: 0.0005244 (Best: 0.0003852 @iter5158) ([92m↓8.30%[0m) [0.21% of initial]
[Iter 5270] Gaussian 0 vs 1:
  Original Loss: 0.0004773
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0004773 (Pseudo: 0.00%)
[Iter 5270] Gaussian 1 vs 0:
  Original Loss: 0.0004808
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0004808 (Pseudo: 0.00%)
[Iter 5280/20000] Loss: 0.0005607 (Best: 0.0003852 @iter5158) ([91m↑6.91%[0m) [0.22% of initial]
[Iter 5280] Gaussian 0 vs 1:
  Original Loss: 0.0006231
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0006231 (Pseudo: 0.00%)
[Iter 5280] Gaussian 1 vs 0:
  Original Loss: 0.0006089
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0006089 (Pseudo: 0.00%)
[Iter 5290/20000] Loss: 0.0005058 (Best: 0.0003852 @iter5158) ([92m↓9.79%[0m) [0.20% of initial]
[Iter 5290] Gaussian 0 vs 1:
  Original Loss: 0.0004949
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0004949 (Pseudo: 0.00%)
[Iter 5290] Gaussian 1 vs 0:
  Original Loss: 0.0005125
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0005125 (Pseudo: 0.00%)
Iter:5299, L1 loss=0.0006141, Total loss=0.000543, Time:27
[Iter 5300/20000] Loss: 0.0005946 (Best: 0.0003852 @iter5158) ([91m↑17.57%[0m) [0.24% of initial]
[Iter 5300] Gaussian 0 vs 1:
  Original Loss: 0.0005704
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0005704 (Pseudo: 0.00%)
[Iter 5300] Gaussian 1 vs 0:
  Original Loss: 0.0005799
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0005799 (Pseudo: 0.00%)
[Iter 5310/20000] Loss: 0.0005659 (Best: 0.0003852 @iter5158) ([92m↓4.83%[0m) [0.22% of initial]
[Iter 5310] Gaussian 0 vs 1:
  Original Loss: 0.0006307
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0006307 (Pseudo: 0.00%)
[Iter 5310] Gaussian 1 vs 0:
  Original Loss: 0.0006211
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0006211 (Pseudo: 0.00%)
[Iter 5320/20000] Loss: 0.0005058 (Best: 0.0003852 @iter5158) ([92m↓10.61%[0m) [0.20% of initial]
[Iter 5320] Gaussian 0 vs 1:
  Original Loss: 0.0005329
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0005329 (Pseudo: 0.00%)
[Iter 5320] Gaussian 1 vs 0:
  Original Loss: 0.0005365
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0005365 (Pseudo: 0.00%)
[Iter 5330/20000] Loss: 0.0004408 (Best: 0.0003852 @iter5158) ([92m↓12.86%[0m) [0.18% of initial]
[Iter 5330] Gaussian 0 vs 1:
  Original Loss: 0.0004347
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0004347 (Pseudo: 0.00%)
[Iter 5330] Gaussian 1 vs 0:
  Original Loss: 0.0004311
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0004311 (Pseudo: 0.00%)
[Iter 5340/20000] Loss: 0.0004592 (Best: 0.0003852 @iter5158) ([91m↑4.17%[0m) [0.18% of initial]
[Iter 5340] Gaussian 0 vs 1:
  Original Loss: 0.0004720
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0004720 (Pseudo: 0.00%)
[Iter 5340] Gaussian 1 vs 0:
  Original Loss: 0.0004675
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0004675 (Pseudo: 0.00%)
[Iter 5350/20000] Loss: 0.0005032 (Best: 0.0003743 @iter5344) ([91m↑9.60%[0m) [0.20% of initial]
[Iter 5350] Gaussian 0 vs 1:
  Original Loss: 0.0005089
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0005089 (Pseudo: 0.00%)
[Iter 5350] Gaussian 1 vs 0:
  Original Loss: 0.0004905
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0004905 (Pseudo: 0.00%)
[Iter 5360/20000] Loss: 0.0004787 (Best: 0.0003743 @iter5344) ([92m↓4.88%[0m) [0.19% of initial]
[Iter 5360] Gaussian 0 vs 1:
  Original Loss: 0.0005092
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0005092 (Pseudo: 0.00%)
[Iter 5360] Gaussian 1 vs 0:
  Original Loss: 0.0005004
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0005004 (Pseudo: 0.00%)
[Iter 5370/20000] Loss: 0.0004925 (Best: 0.0003743 @iter5344) ([91m↑2.89%[0m) [0.20% of initial]
[Iter 5370] Gaussian 0 vs 1:
  Original Loss: 0.0005438
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0005438 (Pseudo: 0.00%)
[Iter 5370] Gaussian 1 vs 0:
  Original Loss: 0.0005461
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0005461 (Pseudo: 0.00%)
[Iter 5380/20000] Loss: 0.0004930 (Best: 0.0003743 @iter5344) ([91m↑0.10%[0m) [0.20% of initial]
[Iter 5380] Gaussian 0 vs 1:
  Original Loss: 0.0004687
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0004687 (Pseudo: 0.00%)
[Iter 5380] Gaussian 1 vs 0:
  Original Loss: 0.0004647
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0004647 (Pseudo: 0.00%)
[Iter 5390/20000] Loss: 0.0005183 (Best: 0.0003743 @iter5344) ([91m↑5.13%[0m) [0.21% of initial]
[Iter 5390] Gaussian 0 vs 1:
  Original Loss: 0.0005179
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0005179 (Pseudo: 0.00%)
[Iter 5390] Gaussian 1 vs 0:
  Original Loss: 0.0005277
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0005277 (Pseudo: 0.00%)
Iter:5399, L1 loss=0.0004904, Total loss=0.0004312, Time:27
[Iter 5400/20000] Loss: 0.0004932 (Best: 0.0003743 @iter5344) ([92m↓4.83%[0m) [0.20% of initial]
[Iter 5400] Gaussian 0 vs 1:
  Original Loss: 0.0005586
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0005586 (Pseudo: 0.00%)
[Iter 5400] Gaussian 1 vs 0:
  Original Loss: 0.0005494
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0005494 (Pseudo: 0.00%)
[Iter 5410/20000] Loss: 0.0019107 (Best: 0.0003743 @iter5344) ([91m↑287.38%[0m) [0.76% of initial]
[Iter 5410] Gaussian 0 vs 1:
  Original Loss: 0.0017142
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0017142 (Pseudo: 0.00%)
[Iter 5410] Gaussian 1 vs 0:
  Original Loss: 0.0016123
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0016123 (Pseudo: 0.00%)
[Iter 5420/20000] Loss: 0.0013418 (Best: 0.0003743 @iter5344) ([92m↓29.78%[0m) [0.53% of initial]
[Iter 5420] Gaussian 0 vs 1:
  Original Loss: 0.0012715
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0012715 (Pseudo: 0.00%)
[Iter 5420] Gaussian 1 vs 0:
  Original Loss: 0.0012866
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0012866 (Pseudo: 0.00%)
[Iter 5430/20000] Loss: 0.0007748 (Best: 0.0003743 @iter5344) ([92m↓42.26%[0m) [0.31% of initial]
[Iter 5430] Gaussian 0 vs 1:
  Original Loss: 0.0007303
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0007303 (Pseudo: 0.00%)
[Iter 5430] Gaussian 1 vs 0:
  Original Loss: 0.0007303
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0007303 (Pseudo: 0.00%)
[Iter 5440/20000] Loss: 0.0006721 (Best: 0.0003743 @iter5344) ([92m↓13.26%[0m) [0.27% of initial]
[Iter 5440] Gaussian 0 vs 1:
  Original Loss: 0.0006718
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0006718 (Pseudo: 0.00%)
[Iter 5440] Gaussian 1 vs 0:
  Original Loss: 0.0006512
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0006512 (Pseudo: 0.00%)
[Iter 5450/20000] Loss: 0.0005434 (Best: 0.0003743 @iter5344) ([92m↓19.14%[0m) [0.22% of initial]
[Iter 5450] Gaussian 0 vs 1:
  Original Loss: 0.0005125
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0005125 (Pseudo: 0.00%)
[Iter 5450] Gaussian 1 vs 0:
  Original Loss: 0.0005144
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0005144 (Pseudo: 0.00%)
[Iter 5460/20000] Loss: 0.0005624 (Best: 0.0003743 @iter5344) ([91m↑3.48%[0m) [0.22% of initial]
[Iter 5460] Gaussian 0 vs 1:
  Original Loss: 0.0006061
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0006061 (Pseudo: 0.00%)
[Iter 5460] Gaussian 1 vs 0:
  Original Loss: 0.0005823
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0005823 (Pseudo: 0.00%)
[Iter 5470/20000] Loss: 0.0004613 (Best: 0.0003743 @iter5344) ([92m↓17.98%[0m) [0.18% of initial]
[Iter 5470] Gaussian 0 vs 1:
  Original Loss: 0.0004414
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0004414 (Pseudo: 0.00%)
[Iter 5470] Gaussian 1 vs 0:
  Original Loss: 0.0004537
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0004537 (Pseudo: 0.00%)
[Iter 5480/20000] Loss: 0.0004350 (Best: 0.0003685 @iter5476) ([92m↓5.69%[0m) [0.17% of initial]
[Iter 5480] Gaussian 0 vs 1:
  Original Loss: 0.0004090
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0004090 (Pseudo: 0.00%)
[Iter 5480] Gaussian 1 vs 0:
  Original Loss: 0.0004115
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0004115 (Pseudo: 0.00%)
[Iter 5490/20000] Loss: 0.0004563 (Best: 0.0003685 @iter5476) ([91m↑4.88%[0m) [0.18% of initial]
[Iter 5490] Gaussian 0 vs 1:
  Original Loss: 0.0004577
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0004577 (Pseudo: 0.00%)
[Iter 5490] Gaussian 1 vs 0:
  Original Loss: 0.0004633
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0004633 (Pseudo: 0.00%)
Iter:5499, L1 loss=0.0005348, Total loss=0.0004745, Time:27
[Iter 5500/20000] Loss: 0.0004167 (Best: 0.0003685 @iter5476) ([92m↓8.67%[0m) [0.17% of initial]
[Iter 5500] Gaussian 0 vs 1:
  Original Loss: 0.0003825
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0003825 (Pseudo: 0.00%)
[Iter 5500] Gaussian 1 vs 0:
  Original Loss: 0.0003892
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0003892 (Pseudo: 0.00%)
[Iter 5510/20000] Loss: 0.0004362 (Best: 0.0003685 @iter5476) ([91m↑4.68%[0m) [0.17% of initial]
[Iter 5510] Gaussian 0 vs 1:
  Original Loss: 0.0004132
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0004132 (Pseudo: 0.00%)
[Iter 5510] Gaussian 1 vs 0:
  Original Loss: 0.0004299
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0004299 (Pseudo: 0.00%)
[Iter 5520/20000] Loss: 0.0004078 (Best: 0.0003685 @iter5476) ([92m↓6.52%[0m) [0.16% of initial]
[Iter 5520] Gaussian 0 vs 1:
  Original Loss: 0.0003905
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0003905 (Pseudo: 0.00%)
[Iter 5520] Gaussian 1 vs 0:
  Original Loss: 0.0004030
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0004030 (Pseudo: 0.00%)
[Iter 5530/20000] Loss: 0.0004523 (Best: 0.0003577 @iter5521) ([91m↑10.91%[0m) [0.18% of initial]
[Iter 5530] Gaussian 0 vs 1:
  Original Loss: 0.0004165
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0004165 (Pseudo: 0.00%)
[Iter 5530] Gaussian 1 vs 0:
  Original Loss: 0.0004367
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0004367 (Pseudo: 0.00%)
[Iter 5540/20000] Loss: 0.0004664 (Best: 0.0003577 @iter5521) ([91m↑3.11%[0m) [0.19% of initial]
[Iter 5540] Gaussian 0 vs 1:
  Original Loss: 0.0004669
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0004669 (Pseudo: 0.00%)
[Iter 5540] Gaussian 1 vs 0:
  Original Loss: 0.0004832
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0004832 (Pseudo: 0.00%)
[Iter 5550/20000] Loss: 0.0004727 (Best: 0.0003577 @iter5521) ([91m↑1.36%[0m) [0.19% of initial]
[Iter 5550] Gaussian 0 vs 1:
  Original Loss: 0.0004533
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0004533 (Pseudo: 0.00%)
[Iter 5550] Gaussian 1 vs 0:
  Original Loss: 0.0004633
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0004633 (Pseudo: 0.00%)
[Iter 5560/20000] Loss: 0.0004371 (Best: 0.0003577 @iter5521) ([92m↓7.54%[0m) [0.17% of initial]
[Iter 5560] Gaussian 0 vs 1:
  Original Loss: 0.0004031
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0004031 (Pseudo: 0.00%)
[Iter 5560] Gaussian 1 vs 0:
  Original Loss: 0.0004050
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0004050 (Pseudo: 0.00%)
[Iter 5570/20000] Loss: 0.0004412 (Best: 0.0003577 @iter5521) ([91m↑0.95%[0m) [0.18% of initial]
[Iter 5570] Gaussian 0 vs 1:
  Original Loss: 0.0004672
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0004672 (Pseudo: 0.00%)
[Iter 5570] Gaussian 1 vs 0:
  Original Loss: 0.0004568
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0004568 (Pseudo: 0.00%)
[Iter 5580/20000] Loss: 0.0005043 (Best: 0.0003577 @iter5521) ([91m↑14.31%[0m) [0.20% of initial]
[Iter 5580] Gaussian 0 vs 1:
  Original Loss: 0.0004983
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0004983 (Pseudo: 0.00%)
[Iter 5580] Gaussian 1 vs 0:
  Original Loss: 0.0005098
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0005098 (Pseudo: 0.00%)
[Iter 5590/20000] Loss: 0.0005611 (Best: 0.0003577 @iter5521) ([91m↑11.25%[0m) [0.22% of initial]
[Iter 5590] Gaussian 0 vs 1:
  Original Loss: 0.0005269
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0005269 (Pseudo: 0.00%)
[Iter 5590] Gaussian 1 vs 0:
  Original Loss: 0.0005320
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0005320 (Pseudo: 0.00%)
Iter:5599, L1 loss=0.0004741, Total loss=0.0004166, Time:27
[Iter 5600/20000] Loss: 0.0004301 (Best: 0.0003577 @iter5521) ([92m↓23.34%[0m) [0.17% of initial]
[Iter 5600] Gaussian 0 vs 1:
  Original Loss: 0.0004087
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0004087 (Pseudo: 0.00%)
[Iter 5600] Gaussian 1 vs 0:
  Original Loss: 0.0004172
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0004172 (Pseudo: 0.00%)
[Iter 5610/20000] Loss: 0.0022138 (Best: 0.0003577 @iter5521) ([91m↑414.70%[0m) [0.88% of initial]
[Iter 5610] Gaussian 0 vs 1:
  Original Loss: 0.0018313
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0018313 (Pseudo: 0.00%)
[Iter 5610] Gaussian 1 vs 0:
  Original Loss: 0.0018145
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0018145 (Pseudo: 0.00%)
[Iter 5620/20000] Loss: 0.0010737 (Best: 0.0003577 @iter5521) ([92m↓51.50%[0m) [0.43% of initial]
[Iter 5620] Gaussian 0 vs 1:
  Original Loss: 0.0009233
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0009233 (Pseudo: 0.00%)
[Iter 5620] Gaussian 1 vs 0:
  Original Loss: 0.0009362
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0009362 (Pseudo: 0.00%)
[Iter 5630/20000] Loss: 0.0008146 (Best: 0.0003577 @iter5521) ([92m↓24.13%[0m) [0.32% of initial]
[Iter 5630] Gaussian 0 vs 1:
  Original Loss: 0.0007819
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0007819 (Pseudo: 0.00%)
[Iter 5630] Gaussian 1 vs 0:
  Original Loss: 0.0007545
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0007545 (Pseudo: 0.00%)
[Iter 5640/20000] Loss: 0.0006714 (Best: 0.0003577 @iter5521) ([92m↓17.58%[0m) [0.27% of initial]
[Iter 5640] Gaussian 0 vs 1:
  Original Loss: 0.0006103
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0006103 (Pseudo: 0.00%)
[Iter 5640] Gaussian 1 vs 0:
  Original Loss: 0.0005753
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0005753 (Pseudo: 0.00%)
[Iter 5650/20000] Loss: 0.0005509 (Best: 0.0003577 @iter5521) ([92m↓17.96%[0m) [0.22% of initial]
[Iter 5650] Gaussian 0 vs 1:
  Original Loss: 0.0005030
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0005030 (Pseudo: 0.00%)
[Iter 5650] Gaussian 1 vs 0:
  Original Loss: 0.0004926
  Pseudo Loss: 0.0000000 (0.00% of original)
  Total Loss: 0.0004926 (Pseudo: 0.00%)
[Iter 5660/20000] Loss: 0.0005126 (Best: 0.0003577 @iter5521) ([92m↓6.95%[0m) [0.20% of initial]
Optimizing 
Training parameters: {'iterations': 20000, 'position_lr_init': 0.00019, 'position_lr_final': 1.9e-06, 'position_lr_delay_mult': 0.01, 'position_lr_max_steps': 20000, 'feature_lr': 0.002, 'opacity_lr': 0.008, 'scaling_lr': 0.005, 'rotation_lr': 0.001, 'percent_dense': 0.01, 'lambda_dssim': 0.2, 'densification_interval': 200, 'opacity_reset_interval': 4000, 'densify_from_iter': 500, 'densify_until_iter': 8000, 'densify_grad_threshold': 2.6e-05, 'random_background': False, 'sample_pseudo_interval': 1, 'start_sample_pseudo': 0, 'end_sample_pseudo': 30000}
Create gaussians1
GsDict.keys() is dict_keys(['gs0', 'gs1'])

Starting training...
Total iterations: 20000
==================================================
[Iter 10/20000] Loss: 0.2179127 (Best: 0.2126908 @iter10) [100.00% of initial]
[Iter 10] Gaussian 0 vs 1:
  Original Loss: 0.2126908
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.020)
  Ratio to Original: 0.00%
  Total Loss: 0.2126908 (Pseudo: 0.00%)
[Iter 10] Gaussian 1 vs 0:
  Original Loss: 0.2126908
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.020)
  Ratio to Original: 0.00%
  Total Loss: 0.2126908 (Pseudo: 0.00%)
[Iter 20/20000] Loss: 0.1746701 (Best: 0.1693031 @iter20) ([92m↓19.84%[0m) [69.39% of initial]
[Iter 20] Gaussian 0 vs 1:
  Original Loss: 0.1693031
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.040)
  Ratio to Original: 0.00%
  Total Loss: 0.1693031 (Pseudo: 0.00%)
[Iter 20] Gaussian 1 vs 0:
  Original Loss: 0.1693034
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.040)
  Ratio to Original: 0.00%
  Total Loss: 0.1693034 (Pseudo: 0.00%)
[Iter 30/20000] Loss: 0.1374909 (Best: 0.1327888 @iter30) ([92m↓21.29%[0m) [54.62% of initial]
[Iter 30] Gaussian 0 vs 1:
  Original Loss: 0.1327888
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.060)
  Ratio to Original: 0.00%
  Total Loss: 0.1327888 (Pseudo: 0.00%)
[Iter 30] Gaussian 1 vs 0:
  Original Loss: 0.1327883
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.060)
  Ratio to Original: 0.00%
  Total Loss: 0.1327883 (Pseudo: 0.00%)
[Iter 40/20000] Loss: 0.1123920 (Best: 0.1098377 @iter40) ([92m↓18.25%[0m) [44.65% of initial]
[Iter 40] Gaussian 0 vs 1:
  Original Loss: 0.1098377
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.080)
  Ratio to Original: 0.00%
  Total Loss: 0.1098377 (Pseudo: 0.00%)
[Iter 40] Gaussian 1 vs 0:
  Original Loss: 0.1098365
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.080)
  Ratio to Original: 0.00%
  Total Loss: 0.1098365 (Pseudo: 0.00%)
[Iter 50/20000] Loss: 0.0993453 (Best: 0.0965455 @iter49) ([92m↓11.61%[0m) [39.47% of initial]
[Iter 50] Gaussian 0 vs 1:
  Original Loss: 0.0994871
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.100)
  Ratio to Original: 0.00%
  Total Loss: 0.0994871 (Pseudo: 0.00%)
[Iter 50] Gaussian 1 vs 0:
  Original Loss: 0.0994848
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.100)
  Ratio to Original: 0.00%
  Total Loss: 0.0994848 (Pseudo: 0.00%)
[Iter 60/20000] Loss: 0.0936756 (Best: 0.0908531 @iter59) ([92m↓5.71%[0m) [37.22% of initial]
[Iter 60] Gaussian 0 vs 1:
  Original Loss: 0.0940257
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.120)
  Ratio to Original: 0.00%
  Total Loss: 0.0940257 (Pseudo: 0.00%)
[Iter 60] Gaussian 1 vs 0:
  Original Loss: 0.0940318
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.120)
  Ratio to Original: 0.00%
  Total Loss: 0.0940318 (Pseudo: 0.00%)
[Iter 70/20000] Loss: 0.0884526 (Best: 0.0869432 @iter70) ([92m↓5.58%[0m) [35.14% of initial]
[Iter 70] Gaussian 0 vs 1:
  Original Loss: 0.0869432
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.140)
  Ratio to Original: 0.00%
  Total Loss: 0.0869432 (Pseudo: 0.00%)
[Iter 70] Gaussian 1 vs 0:
  Original Loss: 0.0869452
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.140)
  Ratio to Original: 0.00%
  Total Loss: 0.0869452 (Pseudo: 0.00%)
[Iter 80/20000] Loss: 0.0851848 (Best: 0.0830971 @iter80) ([92m↓3.69%[0m) [33.84% of initial]
[Iter 80] Gaussian 0 vs 1:
  Original Loss: 0.0830971
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.160)
  Ratio to Original: 0.00%
  Total Loss: 0.0830971 (Pseudo: 0.00%)
[Iter 80] Gaussian 1 vs 0:
  Original Loss: 0.0831055
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.160)
  Ratio to Original: 0.00%
  Total Loss: 0.0831055 (Pseudo: 0.00%)
[Iter 90/20000] Loss: 0.0824174 (Best: 0.0801601 @iter88) ([92m↓3.25%[0m) [32.74% of initial]
[Iter 90] Gaussian 0 vs 1:
  Original Loss: 0.0823502
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.180)
  Ratio to Original: 0.00%
  Total Loss: 0.0823502 (Pseudo: 0.00%)
[Iter 90] Gaussian 1 vs 0:
  Original Loss: 0.0823528
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.180)
  Ratio to Original: 0.00%
  Total Loss: 0.0823528 (Pseudo: 0.00%)
Iter:99, L1 loss=0.05723, Total loss=0.07876, Time:15
[Iter 100/20000] Loss: 0.0786689 (Best: 0.0766193 @iter97) ([92m↓4.55%[0m) [31.25% of initial]
[Iter 100] Gaussian 0 vs 1:
  Original Loss: 0.0783040
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.200)
  Ratio to Original: 0.00%
  Total Loss: 0.0783040 (Pseudo: 0.00%)
[Iter 100] Gaussian 1 vs 0:
  Original Loss: 0.0783039
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.200)
  Ratio to Original: 0.00%
  Total Loss: 0.0783039 (Pseudo: 0.00%)
[Iter 110/20000] Loss: 0.0753274 (Best: 0.0731456 @iter106) ([92m↓4.25%[0m) [29.93% of initial]
[Iter 110] Gaussian 0 vs 1:
  Original Loss: 0.0744795
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.220)
  Ratio to Original: 0.00%
  Total Loss: 0.0744795 (Pseudo: 0.00%)
[Iter 110] Gaussian 1 vs 0:
  Original Loss: 0.0744907
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.220)
  Ratio to Original: 0.00%
  Total Loss: 0.0744907 (Pseudo: 0.00%)
[Iter 120/20000] Loss: 0.0714338 (Best: 0.0685779 @iter118) ([92m↓5.17%[0m) [28.38% of initial]
[Iter 120] Gaussian 0 vs 1:
  Original Loss: 0.0722980
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.240)
  Ratio to Original: 0.00%
  Total Loss: 0.0722980 (Pseudo: 0.00%)
[Iter 120] Gaussian 1 vs 0:
  Original Loss: 0.0723168
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.240)
  Ratio to Original: 0.00%
  Total Loss: 0.0723168 (Pseudo: 0.00%)
[Iter 130/20000] Loss: 0.0667094 (Best: 0.0642144 @iter130) ([92m↓6.61%[0m) [26.50% of initial]
[Iter 130] Gaussian 0 vs 1:
  Original Loss: 0.0642144
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.260)
  Ratio to Original: 0.00%
  Total Loss: 0.0642144 (Pseudo: 0.00%)
[Iter 130] Gaussian 1 vs 0:
  Original Loss: 0.0642080
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.260)
  Ratio to Original: 0.00%
  Total Loss: 0.0642080 (Pseudo: 0.00%)
[Iter 140/20000] Loss: 0.0635396 (Best: 0.0612876 @iter140) ([92m↓4.75%[0m) [25.24% of initial]
[Iter 140] Gaussian 0 vs 1:
  Original Loss: 0.0612876
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.280)
  Ratio to Original: 0.00%
  Total Loss: 0.0612876 (Pseudo: 0.00%)
[Iter 140] Gaussian 1 vs 0:
  Original Loss: 0.0613061
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.280)
  Ratio to Original: 0.00%
  Total Loss: 0.0613061 (Pseudo: 0.00%)
[Iter 150/20000] Loss: 0.0612688 (Best: 0.0583680 @iter148) ([92m↓3.57%[0m) [24.34% of initial]
[Iter 150] Gaussian 0 vs 1:
  Original Loss: 0.0605453
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.300)
  Ratio to Original: 0.00%
  Total Loss: 0.0605453 (Pseudo: 0.00%)
[Iter 150] Gaussian 1 vs 0:
  Original Loss: 0.0605823
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.300)
  Ratio to Original: 0.00%
  Total Loss: 0.0605823 (Pseudo: 0.00%)
[Iter 160/20000] Loss: 0.0590484 (Best: 0.0559286 @iter157) ([92m↓3.62%[0m) [23.46% of initial]
[Iter 160] Gaussian 0 vs 1:
  Original Loss: 0.0599981
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.320)
  Ratio to Original: 0.00%
  Total Loss: 0.0599981 (Pseudo: 0.00%)
[Iter 160] Gaussian 1 vs 0:
  Original Loss: 0.0600279
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.320)
  Ratio to Original: 0.00%
  Total Loss: 0.0600279 (Pseudo: 0.00%)
[Iter 170/20000] Loss: 0.0563500 (Best: 0.0534931 @iter167) ([92m↓4.57%[0m) [22.39% of initial]
[Iter 170] Gaussian 0 vs 1:
  Original Loss: 0.0573921
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.340)
  Ratio to Original: 0.00%
  Total Loss: 0.0573921 (Pseudo: 0.00%)
[Iter 170] Gaussian 1 vs 0:
  Original Loss: 0.0574288
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.340)
  Ratio to Original: 0.00%
  Total Loss: 0.0574288 (Pseudo: 0.00%)
[Iter 180/20000] Loss: 0.0523277 (Best: 0.0500123 @iter179) ([92m↓7.14%[0m) [20.79% of initial]
[Iter 180] Gaussian 0 vs 1:
  Original Loss: 0.0518854
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.360)
  Ratio to Original: 0.00%
  Total Loss: 0.0518854 (Pseudo: 0.00%)
[Iter 180] Gaussian 1 vs 0:
  Original Loss: 0.0518935
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.360)
  Ratio to Original: 0.00%
  Total Loss: 0.0518935 (Pseudo: 0.00%)
[Iter 190/20000] Loss: 0.0494977 (Best: 0.0477921 @iter188) ([92m↓5.41%[0m) [19.66% of initial]
[Iter 190] Gaussian 0 vs 1:
  Original Loss: 0.0489988
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.380)
  Ratio to Original: 0.00%
  Total Loss: 0.0489988 (Pseudo: 0.00%)
[Iter 190] Gaussian 1 vs 0:
  Original Loss: 0.0490586
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.380)
  Ratio to Original: 0.00%
  Total Loss: 0.0490586 (Pseudo: 0.00%)
Iter:199, L1 loss=0.03443, Total loss=0.04979, Time:14
[Iter 200/20000] Loss: 0.0478234 (Best: 0.0456826 @iter198) ([92m↓3.38%[0m) [19.00% of initial]
[Iter 200] Gaussian 0 vs 1:
  Original Loss: 0.0467065
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.400)
  Ratio to Original: 0.00%
  Total Loss: 0.0467065 (Pseudo: 0.00%)
[Iter 200] Gaussian 1 vs 0:
  Original Loss: 0.0467326
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.400)
  Ratio to Original: 0.00%
  Total Loss: 0.0467326 (Pseudo: 0.00%)
[Iter 210/20000] Loss: 0.0450144 (Best: 0.0428364 @iter209) ([92m↓5.87%[0m) [17.88% of initial]
[Iter 210] Gaussian 0 vs 1:
  Original Loss: 0.0447366
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.420)
  Ratio to Original: 0.00%
  Total Loss: 0.0447366 (Pseudo: 0.00%)
[Iter 210] Gaussian 1 vs 0:
  Original Loss: 0.0447613
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.420)
  Ratio to Original: 0.00%
  Total Loss: 0.0447613 (Pseudo: 0.00%)
[Iter 220/20000] Loss: 0.0440382 (Best: 0.0411982 @iter219) ([92m↓2.17%[0m) [17.50% of initial]
[Iter 220] Gaussian 0 vs 1:
  Original Loss: 0.0454602
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.440)
  Ratio to Original: 0.00%
  Total Loss: 0.0454602 (Pseudo: 0.00%)
[Iter 220] Gaussian 1 vs 0:
  Original Loss: 0.0454481
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.440)
  Ratio to Original: 0.00%
  Total Loss: 0.0454481 (Pseudo: 0.00%)
[Iter 230/20000] Loss: 0.0422803 (Best: 0.0398412 @iter227) ([92m↓3.99%[0m) [16.80% of initial]
[Iter 230] Gaussian 0 vs 1:
  Original Loss: 0.0410086
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.460)
  Ratio to Original: 0.00%
  Total Loss: 0.0410086 (Pseudo: 0.00%)
[Iter 230] Gaussian 1 vs 0:
  Original Loss: 0.0410920
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.460)
  Ratio to Original: 0.00%
  Total Loss: 0.0410920 (Pseudo: 0.00%)
[Iter 240/20000] Loss: 0.0402027 (Best: 0.0377617 @iter238) ([92m↓4.91%[0m) [15.97% of initial]
[Iter 240] Gaussian 0 vs 1:
  Original Loss: 0.0393227
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.480)
  Ratio to Original: 0.00%
  Total Loss: 0.0393227 (Pseudo: 0.00%)
[Iter 240] Gaussian 1 vs 0:
  Original Loss: 0.0394214
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.480)
  Ratio to Original: 0.00%
  Total Loss: 0.0394214 (Pseudo: 0.00%)
[Iter 250/20000] Loss: 0.0379483 (Best: 0.0361903 @iter248) ([92m↓5.61%[0m) [15.08% of initial]
[Iter 250] Gaussian 0 vs 1:
  Original Loss: 0.0375778
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.500)
  Ratio to Original: 0.00%
  Total Loss: 0.0375778 (Pseudo: 0.00%)
[Iter 250] Gaussian 1 vs 0:
  Original Loss: 0.0376327
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.500)
  Ratio to Original: 0.00%
  Total Loss: 0.0376327 (Pseudo: 0.00%)
[Iter 260/20000] Loss: 0.0359141 (Best: 0.0343693 @iter260) ([92m↓5.36%[0m) [14.27% of initial]
[Iter 260] Gaussian 0 vs 1:
  Original Loss: 0.0343693
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.520)
  Ratio to Original: 0.00%
  Total Loss: 0.0343693 (Pseudo: 0.00%)
[Iter 260] Gaussian 1 vs 0:
  Original Loss: 0.0343404
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.520)
  Ratio to Original: 0.00%
  Total Loss: 0.0343404 (Pseudo: 0.00%)
[Iter 270/20000] Loss: 0.0349382 (Best: 0.0328604 @iter269) ([92m↓2.72%[0m) [13.88% of initial]
[Iter 270] Gaussian 0 vs 1:
  Original Loss: 0.0346533
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.540)
  Ratio to Original: 0.00%
  Total Loss: 0.0346533 (Pseudo: 0.00%)
[Iter 270] Gaussian 1 vs 0:
  Original Loss: 0.0347309
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.540)
  Ratio to Original: 0.00%
  Total Loss: 0.0347309 (Pseudo: 0.00%)
[Iter 280/20000] Loss: 0.0346945 (Best: 0.0319148 @iter277) ([92m↓0.70%[0m) [13.78% of initial]
[Iter 280] Gaussian 0 vs 1:
  Original Loss: 0.0358568
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.560)
  Ratio to Original: 0.00%
  Total Loss: 0.0358568 (Pseudo: 0.00%)
[Iter 280] Gaussian 1 vs 0:
  Original Loss: 0.0359731
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.560)
  Ratio to Original: 0.00%
  Total Loss: 0.0359731 (Pseudo: 0.00%)
[Iter 290/20000] Loss: 0.0331024 (Best: 0.0303852 @iter287) ([92m↓4.59%[0m) [13.15% of initial]
[Iter 290] Gaussian 0 vs 1:
  Original Loss: 0.0321202
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.580)
  Ratio to Original: 0.00%
  Total Loss: 0.0321202 (Pseudo: 0.00%)
[Iter 290] Gaussian 1 vs 0:
  Original Loss: 0.0321934
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.580)
  Ratio to Original: 0.00%
  Total Loss: 0.0321934 (Pseudo: 0.00%)
Iter:299, L1 loss=0.02221, Total loss=0.03345, Time:14
[Iter 300/20000] Loss: 0.0308937 (Best: 0.0290417 @iter300) ([92m↓6.67%[0m) [12.27% of initial]
[Iter 300] Gaussian 0 vs 1:
  Original Loss: 0.0290417
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.600)
  Ratio to Original: 0.00%
  Total Loss: 0.0290417 (Pseudo: 0.00%)
[Iter 300] Gaussian 1 vs 0:
  Original Loss: 0.0290223
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.600)
  Ratio to Original: 0.00%
  Total Loss: 0.0290223 (Pseudo: 0.00%)
[Iter 310/20000] Loss: 0.0293910 (Best: 0.0274530 @iter310) ([92m↓4.86%[0m) [11.68% of initial]
[Iter 310] Gaussian 0 vs 1:
  Original Loss: 0.0274530
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.620)
  Ratio to Original: 0.00%
  Total Loss: 0.0274530 (Pseudo: 0.00%)
[Iter 310] Gaussian 1 vs 0:
  Original Loss: 0.0274441
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.620)
  Ratio to Original: 0.00%
  Total Loss: 0.0274441 (Pseudo: 0.00%)
[Iter 320/20000] Loss: 0.0278181 (Best: 0.0263720 @iter320) ([92m↓5.35%[0m) [11.05% of initial]
[Iter 320] Gaussian 0 vs 1:
  Original Loss: 0.0263720
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.640)
  Ratio to Original: 0.00%
  Total Loss: 0.0263720 (Pseudo: 0.00%)
[Iter 320] Gaussian 1 vs 0:
  Original Loss: 0.0266206
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.640)
  Ratio to Original: 0.00%
  Total Loss: 0.0266206 (Pseudo: 0.00%)
[Iter 330/20000] Loss: 0.0275578 (Best: 0.0256517 @iter330) ([92m↓0.94%[0m) [10.95% of initial]
[Iter 330] Gaussian 0 vs 1:
  Original Loss: 0.0256517
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.660)
  Ratio to Original: 0.00%
  Total Loss: 0.0256517 (Pseudo: 0.00%)
[Iter 330] Gaussian 1 vs 0:
  Original Loss: 0.0258970
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.660)
  Ratio to Original: 0.00%
  Total Loss: 0.0258970 (Pseudo: 0.00%)
[Iter 340/20000] Loss: 0.0253621 (Best: 0.0242631 @iter340) ([92m↓7.97%[0m) [10.08% of initial]
[Iter 340] Gaussian 0 vs 1:
  Original Loss: 0.0242631
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.680)
  Ratio to Original: 0.00%
  Total Loss: 0.0242631 (Pseudo: 0.00%)
[Iter 340] Gaussian 1 vs 0:
  Original Loss: 0.0244139
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.680)
  Ratio to Original: 0.00%
  Total Loss: 0.0244139 (Pseudo: 0.00%)
[Iter 350/20000] Loss: 0.0260154 (Best: 0.0234693 @iter349) ([91m↑2.58%[0m) [10.34% of initial]
[Iter 350] Gaussian 0 vs 1:
  Original Loss: 0.0273475
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.700)
  Ratio to Original: 0.00%
  Total Loss: 0.0273475 (Pseudo: 0.00%)
[Iter 350] Gaussian 1 vs 0:
  Original Loss: 0.0274214
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.700)
  Ratio to Original: 0.00%
  Total Loss: 0.0274214 (Pseudo: 0.00%)
[Iter 360/20000] Loss: 0.0244330 (Best: 0.0225299 @iter358) ([92m↓6.08%[0m) [9.71% of initial]
[Iter 360] Gaussian 0 vs 1:
  Original Loss: 0.0240026
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.720)
  Ratio to Original: 0.00%
  Total Loss: 0.0240026 (Pseudo: 0.00%)
[Iter 360] Gaussian 1 vs 0:
  Original Loss: 0.0245428
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.720)
  Ratio to Original: 0.00%
  Total Loss: 0.0245428 (Pseudo: 0.00%)
[Iter 370/20000] Loss: 0.0241634 (Best: 0.0219332 @iter368) ([92m↓1.10%[0m) [9.60% of initial]
[Iter 370] Gaussian 0 vs 1:
  Original Loss: 0.0253552
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.740)
  Ratio to Original: 0.00%
  Total Loss: 0.0253552 (Pseudo: 0.00%)
[Iter 370] Gaussian 1 vs 0:
  Original Loss: 0.0255873
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.740)
  Ratio to Original: 0.00%
  Total Loss: 0.0255873 (Pseudo: 0.00%)
[Iter 380/20000] Loss: 0.0219556 (Best: 0.0208685 @iter379) ([92m↓9.14%[0m) [8.72% of initial]
[Iter 380] Gaussian 0 vs 1:
  Original Loss: 0.0222538
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.760)
  Ratio to Original: 0.00%
  Total Loss: 0.0222538 (Pseudo: 0.00%)
[Iter 380] Gaussian 1 vs 0:
  Original Loss: 0.0228190
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.760)
  Ratio to Original: 0.00%
  Total Loss: 0.0228190 (Pseudo: 0.00%)
[Iter 390/20000] Loss: 0.0214535 (Best: 0.0198951 @iter385) ([92m↓2.29%[0m) [8.52% of initial]
[Iter 390] Gaussian 0 vs 1:
  Original Loss: 0.0200861
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.780)
  Ratio to Original: 0.00%
  Total Loss: 0.0200861 (Pseudo: 0.00%)
[Iter 390] Gaussian 1 vs 0:
  Original Loss: 0.0203170
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.780)
  Ratio to Original: 0.00%
  Total Loss: 0.0203170 (Pseudo: 0.00%)
Iter:399, L1 loss=0.01372, Total loss=0.02121, Time:14
[Iter 400/20000] Loss: 0.0204162 (Best: 0.0189818 @iter400) ([92m↓4.83%[0m) [8.11% of initial]
[Iter 400] Gaussian 0 vs 1:
  Original Loss: 0.0189818
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.800)
  Ratio to Original: 0.00%
  Total Loss: 0.0189818 (Pseudo: 0.00%)
[Iter 400] Gaussian 1 vs 0:
  Original Loss: 0.0191063
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.800)
  Ratio to Original: 0.00%
  Total Loss: 0.0191063 (Pseudo: 0.00%)
[Iter 410/20000] Loss: 0.0194617 (Best: 0.0183653 @iter410) ([92m↓4.68%[0m) [7.73% of initial]
[Iter 410] Gaussian 0 vs 1:
  Original Loss: 0.0183653
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.820)
  Ratio to Original: 0.00%
  Total Loss: 0.0183653 (Pseudo: 0.00%)
[Iter 410] Gaussian 1 vs 0:
  Original Loss: 0.0185541
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.820)
  Ratio to Original: 0.00%
  Total Loss: 0.0185541 (Pseudo: 0.00%)
[Iter 420/20000] Loss: 0.0199603 (Best: 0.0177621 @iter418) ([91m↑2.56%[0m) [7.93% of initial]
[Iter 420] Gaussian 0 vs 1:
  Original Loss: 0.0212062
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.840)
  Ratio to Original: 0.00%
  Total Loss: 0.0212062 (Pseudo: 0.00%)
[Iter 420] Gaussian 1 vs 0:
  Original Loss: 0.0213478
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.840)
  Ratio to Original: 0.00%
  Total Loss: 0.0213478 (Pseudo: 0.00%)
[Iter 430/20000] Loss: 0.0182680 (Best: 0.0173801 @iter430) ([92m↓8.48%[0m) [7.26% of initial]
[Iter 430] Gaussian 0 vs 1:
  Original Loss: 0.0173801
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.860)
  Ratio to Original: 0.00%
  Total Loss: 0.0173801 (Pseudo: 0.00%)
[Iter 430] Gaussian 1 vs 0:
  Original Loss: 0.0168240
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.860)
  Ratio to Original: 0.00%
  Total Loss: 0.0168240 (Pseudo: 0.00%)
[Iter 440/20000] Loss: 0.0188349 (Best: 0.0170196 @iter438) ([91m↑3.10%[0m) [7.48% of initial]
[Iter 440] Gaussian 0 vs 1:
  Original Loss: 0.0197487
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.880)
  Ratio to Original: 0.00%
  Total Loss: 0.0197487 (Pseudo: 0.00%)
[Iter 440] Gaussian 1 vs 0:
  Original Loss: 0.0190885
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.880)
  Ratio to Original: 0.00%
  Total Loss: 0.0190885 (Pseudo: 0.00%)
[Iter 450/20000] Loss: 0.0180006 (Best: 0.0160581 @iter449) ([92m↓4.43%[0m) [7.15% of initial]
[Iter 450] Gaussian 0 vs 1:
  Original Loss: 0.0192332
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.900)
  Ratio to Original: 0.00%
  Total Loss: 0.0192332 (Pseudo: 0.00%)
[Iter 450] Gaussian 1 vs 0:
  Original Loss: 0.0188306
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.900)
  Ratio to Original: 0.00%
  Total Loss: 0.0188306 (Pseudo: 0.00%)
[Iter 460/20000] Loss: 0.0173075 (Best: 0.0153155 @iter458) ([92m↓3.85%[0m) [6.88% of initial]
[Iter 460] Gaussian 0 vs 1:
  Original Loss: 0.0180222
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.920)
  Ratio to Original: 0.00%
  Total Loss: 0.0180222 (Pseudo: 0.00%)
[Iter 460] Gaussian 1 vs 0:
  Original Loss: 0.0172426
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.920)
  Ratio to Original: 0.00%
  Total Loss: 0.0172426 (Pseudo: 0.00%)
[Iter 470/20000] Loss: 0.0158542 (Best: 0.0148551 @iter463) ([92m↓8.40%[0m) [6.30% of initial]
[Iter 470] Gaussian 0 vs 1:
  Original Loss: 0.0150190
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.940)
  Ratio to Original: 0.00%
  Total Loss: 0.0150190 (Pseudo: 0.00%)
[Iter 470] Gaussian 1 vs 0:
  Original Loss: 0.0138350
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.940)
  Ratio to Original: 0.00%
  Total Loss: 0.0138350 (Pseudo: 0.00%)
[Iter 480/20000] Loss: 0.0158497 (Best: 0.0144849 @iter479) ([92m↓0.03%[0m) [6.30% of initial]
[Iter 480] Gaussian 0 vs 1:
  Original Loss: 0.0160275
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.960)
  Ratio to Original: 0.00%
  Total Loss: 0.0160275 (Pseudo: 0.00%)
[Iter 480] Gaussian 1 vs 0:
  Original Loss: 0.0143497
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.960)
  Ratio to Original: 0.00%
  Total Loss: 0.0143497 (Pseudo: 0.00%)
[Iter 490/20000] Loss: 0.0148425 (Best: 0.0137517 @iter490) ([92m↓6.35%[0m) [5.90% of initial]
[Iter 490] Gaussian 0 vs 1:
  Original Loss: 0.0137517
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.980)
  Ratio to Original: 0.00%
  Total Loss: 0.0137517 (Pseudo: 0.00%)
[Iter 490] Gaussian 1 vs 0:
  Original Loss: 0.0130682
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 0.980)
  Ratio to Original: 0.00%
  Total Loss: 0.0130682 (Pseudo: 0.00%)
Iter:499, L1 loss=0.008805, Total loss=0.01551, Time:14
[Iter 500/20000] Loss: 0.0146356 (Best: 0.0135424 @iter493) ([92m↓1.39%[0m) [5.81% of initial]
[Iter 500] Gaussian 0 vs 1:
  Original Loss: 0.0142567
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0142567 (Pseudo: 0.00%)
[Iter 500] Gaussian 1 vs 0:
  Original Loss: 0.0137677
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0137677 (Pseudo: 0.00%)
[Iter 510/20000] Loss: 0.0141573 (Best: 0.0129006 @iter508) ([92m↓3.27%[0m) [5.62% of initial]
[Iter 510] Gaussian 0 vs 1:
  Original Loss: 0.0141517
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0141517 (Pseudo: 0.00%)
[Iter 510] Gaussian 1 vs 0:
  Original Loss: 0.0136837
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0136837 (Pseudo: 0.00%)
[Iter 520/20000] Loss: 0.0132030 (Best: 0.0123284 @iter514) ([92m↓6.74%[0m) [5.25% of initial]
[Iter 520] Gaussian 0 vs 1:
  Original Loss: 0.0130815
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0130815 (Pseudo: 0.00%)
[Iter 520] Gaussian 1 vs 0:
  Original Loss: 0.0129185
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0129185 (Pseudo: 0.00%)
[Iter 530/20000] Loss: 0.0125356 (Best: 0.0115212 @iter529) ([92m↓5.05%[0m) [4.98% of initial]
[Iter 530] Gaussian 0 vs 1:
  Original Loss: 0.0125486
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0125486 (Pseudo: 0.00%)
[Iter 530] Gaussian 1 vs 0:
  Original Loss: 0.0127318
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0127318 (Pseudo: 0.00%)
[Iter 540/20000] Loss: 0.0123495 (Best: 0.0110516 @iter538) ([92m↓1.48%[0m) [4.91% of initial]
[Iter 540] Gaussian 0 vs 1:
  Original Loss: 0.0121679
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0121679 (Pseudo: 0.00%)
[Iter 540] Gaussian 1 vs 0:
  Original Loss: 0.0125708
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0125708 (Pseudo: 0.00%)
[Iter 550/20000] Loss: 0.0121470 (Best: 0.0109409 @iter548) ([92m↓1.64%[0m) [4.83% of initial]
[Iter 550] Gaussian 0 vs 1:
  Original Loss: 0.0121311
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0121311 (Pseudo: 0.00%)
[Iter 550] Gaussian 1 vs 0:
  Original Loss: 0.0120861
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0120861 (Pseudo: 0.00%)
[Iter 560/20000] Loss: 0.0123161 (Best: 0.0109401 @iter556) ([91m↑1.39%[0m) [4.89% of initial]
[Iter 560] Gaussian 0 vs 1:
  Original Loss: 0.0120802
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0120802 (Pseudo: 0.00%)
[Iter 560] Gaussian 1 vs 0:
  Original Loss: 0.0118709
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0118709 (Pseudo: 0.00%)
[Iter 570/20000] Loss: 0.0117875 (Best: 0.0104893 @iter569) ([92m↓4.29%[0m) [4.68% of initial]
[Iter 570] Gaussian 0 vs 1:
  Original Loss: 0.0127352
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0127352 (Pseudo: 0.00%)
[Iter 570] Gaussian 1 vs 0:
  Original Loss: 0.0125608
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0125608 (Pseudo: 0.00%)
[Iter 580/20000] Loss: 0.0111491 (Best: 0.0101479 @iter578) ([92m↓5.42%[0m) [4.43% of initial]
[Iter 580] Gaussian 0 vs 1:
  Original Loss: 0.0110773
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0110773 (Pseudo: 0.00%)
[Iter 580] Gaussian 1 vs 0:
  Original Loss: 0.0111218
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0111218 (Pseudo: 0.00%)
[Iter 590/20000] Loss: 0.0112763 (Best: 0.0098980 @iter583) ([91m↑1.14%[0m) [4.48% of initial]
[Iter 590] Gaussian 0 vs 1:
  Original Loss: 0.0111352
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0111352 (Pseudo: 0.00%)
[Iter 590] Gaussian 1 vs 0:
  Original Loss: 0.0110003
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0110003 (Pseudo: 0.00%)
Iter:599, L1 loss=0.006812, Total loss=0.01218, Time:13
[Iter 600/20000] Loss: 0.0114872 (Best: 0.0098980 @iter583) ([91m↑1.87%[0m) [4.56% of initial]
[Iter 600] Gaussian 0 vs 1:
  Original Loss: 0.0117089
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0117089 (Pseudo: 0.00%)
[Iter 600] Gaussian 1 vs 0:
  Original Loss: 0.0108206
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0108206 (Pseudo: 0.00%)
[Iter 610/20000] Loss: 0.0197686 (Best: 0.0098980 @iter583) ([91m↑72.09%[0m) [7.85% of initial]
[Iter 610] Gaussian 0 vs 1:
  Original Loss: 0.0173837
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0173837 (Pseudo: 0.00%)
[Iter 610] Gaussian 1 vs 0:
  Original Loss: 0.0179846
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0179846 (Pseudo: 0.00%)
[Iter 620/20000] Loss: 0.0143950 (Best: 0.0098980 @iter583) ([92m↓27.18%[0m) [5.72% of initial]
[Iter 620] Gaussian 0 vs 1:
  Original Loss: 0.0130906
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0130906 (Pseudo: 0.00%)
[Iter 620] Gaussian 1 vs 0:
  Original Loss: 0.0126280
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0126280 (Pseudo: 0.00%)
[Iter 630/20000] Loss: 0.0121635 (Best: 0.0098980 @iter583) ([92m↓15.50%[0m) [4.83% of initial]
[Iter 630] Gaussian 0 vs 1:
  Original Loss: 0.0113021
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0113021 (Pseudo: 0.00%)
[Iter 630] Gaussian 1 vs 0:
  Original Loss: 0.0112670
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0112670 (Pseudo: 0.00%)
[Iter 640/20000] Loss: 0.0105291 (Best: 0.0096385 @iter640) ([92m↓13.44%[0m) [4.18% of initial]
[Iter 640] Gaussian 0 vs 1:
  Original Loss: 0.0096385
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0096385 (Pseudo: 0.00%)
[Iter 640] Gaussian 1 vs 0:
  Original Loss: 0.0091983
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0091983 (Pseudo: 0.00%)
[Iter 650/20000] Loss: 0.0106195 (Best: 0.0093933 @iter646) ([91m↑0.86%[0m) [4.22% of initial]
[Iter 650] Gaussian 0 vs 1:
  Original Loss: 0.0103663
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0103663 (Pseudo: 0.00%)
[Iter 650] Gaussian 1 vs 0:
  Original Loss: 0.0105366
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0105366 (Pseudo: 0.00%)
[Iter 660/20000] Loss: 0.0100114 (Best: 0.0088941 @iter655) ([92m↓5.73%[0m) [3.98% of initial]
[Iter 660] Gaussian 0 vs 1:
  Original Loss: 0.0106640
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0106640 (Pseudo: 0.00%)
[Iter 660] Gaussian 1 vs 0:
  Original Loss: 0.0106458
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0106458 (Pseudo: 0.00%)
[Iter 670/20000] Loss: 0.0095141 (Best: 0.0083938 @iter667) ([92m↓4.97%[0m) [3.78% of initial]
[Iter 670] Gaussian 0 vs 1:
  Original Loss: 0.0093781
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0093781 (Pseudo: 0.00%)
[Iter 670] Gaussian 1 vs 0:
  Original Loss: 0.0093635
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0093635 (Pseudo: 0.00%)
[Iter 680/20000] Loss: 0.0089247 (Best: 0.0083034 @iter680) ([92m↓6.19%[0m) [3.55% of initial]
[Iter 680] Gaussian 0 vs 1:
  Original Loss: 0.0083034
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0083034 (Pseudo: 0.00%)
[Iter 680] Gaussian 1 vs 0:
  Original Loss: 0.0081034
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0081034 (Pseudo: 0.00%)
[Iter 690/20000] Loss: 0.0092628 (Best: 0.0079286 @iter682) ([91m↑3.79%[0m) [3.68% of initial]
[Iter 690] Gaussian 0 vs 1:
  Original Loss: 0.0087427
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0087427 (Pseudo: 0.00%)
[Iter 690] Gaussian 1 vs 0:
  Original Loss: 0.0087921
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0087921 (Pseudo: 0.00%)
Iter:699, L1 loss=0.005753, Total loss=0.009746, Time:14
[Iter 700/20000] Loss: 0.0089896 (Best: 0.0079143 @iter695) ([92m↓2.95%[0m) [3.57% of initial]
[Iter 700] Gaussian 0 vs 1:
  Original Loss: 0.0088658
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0088658 (Pseudo: 0.00%)
[Iter 700] Gaussian 1 vs 0:
  Original Loss: 0.0088044
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0088044 (Pseudo: 0.00%)
[Iter 710/20000] Loss: 0.0082676 (Best: 0.0076574 @iter710) ([92m↓8.03%[0m) [3.28% of initial]
[Iter 710] Gaussian 0 vs 1:
  Original Loss: 0.0076574
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0076574 (Pseudo: 0.00%)
[Iter 710] Gaussian 1 vs 0:
  Original Loss: 0.0076096
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0076096 (Pseudo: 0.00%)
[Iter 720/20000] Loss: 0.0084732 (Best: 0.0075350 @iter715) ([91m↑2.49%[0m) [3.37% of initial]
[Iter 720] Gaussian 0 vs 1:
  Original Loss: 0.0079312
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0079312 (Pseudo: 0.00%)
[Iter 720] Gaussian 1 vs 0:
  Original Loss: 0.0078905
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0078905 (Pseudo: 0.00%)
[Iter 730/20000] Loss: 0.0084763 (Best: 0.0074474 @iter724) ([91m↑0.04%[0m) [3.37% of initial]
[Iter 730] Gaussian 0 vs 1:
  Original Loss: 0.0083475
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0083475 (Pseudo: 0.00%)
[Iter 730] Gaussian 1 vs 0:
  Original Loss: 0.0083282
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0083282 (Pseudo: 0.00%)
[Iter 740/20000] Loss: 0.0085226 (Best: 0.0074370 @iter733) ([91m↑0.55%[0m) [3.39% of initial]
[Iter 740] Gaussian 0 vs 1:
  Original Loss: 0.0088001
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0088001 (Pseudo: 0.00%)
[Iter 740] Gaussian 1 vs 0:
  Original Loss: 0.0086915
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0086915 (Pseudo: 0.00%)
[Iter 750/20000] Loss: 0.0080038 (Best: 0.0068863 @iter748) ([92m↓6.09%[0m) [3.18% of initial]
[Iter 750] Gaussian 0 vs 1:
  Original Loss: 0.0079391
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0079391 (Pseudo: 0.00%)
[Iter 750] Gaussian 1 vs 0:
  Original Loss: 0.0080232
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0080232 (Pseudo: 0.00%)
[Iter 760/20000] Loss: 0.0074151 (Best: 0.0068772 @iter754) ([92m↓7.36%[0m) [2.95% of initial]
[Iter 760] Gaussian 0 vs 1:
  Original Loss: 0.0069822
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0069822 (Pseudo: 0.00%)
[Iter 760] Gaussian 1 vs 0:
  Original Loss: 0.0069306
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0069306 (Pseudo: 0.00%)
[Iter 770/20000] Loss: 0.0076045 (Best: 0.0068772 @iter754) ([91m↑2.55%[0m) [3.02% of initial]
[Iter 770] Gaussian 0 vs 1:
  Original Loss: 0.0079048
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0079048 (Pseudo: 0.00%)
[Iter 770] Gaussian 1 vs 0:
  Original Loss: 0.0079583
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0079583 (Pseudo: 0.00%)
[Iter 780/20000] Loss: 0.0078148 (Best: 0.0066673 @iter775) ([91m↑2.77%[0m) [3.10% of initial]
[Iter 780] Gaussian 0 vs 1:
  Original Loss: 0.0084307
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0084307 (Pseudo: 0.00%)
[Iter 780] Gaussian 1 vs 0:
  Original Loss: 0.0084941
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0084941 (Pseudo: 0.00%)
[Iter 790/20000] Loss: 0.0075900 (Best: 0.0065199 @iter787) ([92m↓2.88%[0m) [3.02% of initial]
[Iter 790] Gaussian 0 vs 1:
  Original Loss: 0.0075703
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0075703 (Pseudo: 0.00%)
[Iter 790] Gaussian 1 vs 0:
  Original Loss: 0.0075662
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0075662 (Pseudo: 0.00%)
Iter:799, L1 loss=0.005018, Total loss=0.008075, Time:14
[Iter 800/20000] Loss: 0.0072786 (Best: 0.0064637 @iter796) ([92m↓4.10%[0m) [2.89% of initial]
[Iter 800] Gaussian 0 vs 1:
  Original Loss: 0.0065196
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0065196 (Pseudo: 0.00%)
[Iter 800] Gaussian 1 vs 0:
  Original Loss: 0.0066094
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0066094 (Pseudo: 0.00%)
[Iter 810/20000] Loss: 0.0152791 (Best: 0.0064637 @iter796) ([91m↑109.92%[0m) [6.07% of initial]
[Iter 810] Gaussian 0 vs 1:
  Original Loss: 0.0137210
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0137210 (Pseudo: 0.00%)
[Iter 810] Gaussian 1 vs 0:
  Original Loss: 0.0142203
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0142203 (Pseudo: 0.00%)
[Iter 820/20000] Loss: 0.0104260 (Best: 0.0064637 @iter796) ([92m↓31.76%[0m) [4.14% of initial]
[Iter 820] Gaussian 0 vs 1:
  Original Loss: 0.0102699
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0102699 (Pseudo: 0.00%)
[Iter 820] Gaussian 1 vs 0:
  Original Loss: 0.0103150
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0103150 (Pseudo: 0.00%)
[Iter 830/20000] Loss: 0.0089167 (Best: 0.0064637 @iter796) ([92m↓14.48%[0m) [3.54% of initial]
[Iter 830] Gaussian 0 vs 1:
  Original Loss: 0.0093644
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0093644 (Pseudo: 0.00%)
[Iter 830] Gaussian 1 vs 0:
  Original Loss: 0.0090817
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0090817 (Pseudo: 0.00%)
[Iter 840/20000] Loss: 0.0080015 (Best: 0.0064637 @iter796) ([92m↓10.26%[0m) [3.18% of initial]
[Iter 840] Gaussian 0 vs 1:
  Original Loss: 0.0087314
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0087314 (Pseudo: 0.00%)
[Iter 840] Gaussian 1 vs 0:
  Original Loss: 0.0086397
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0086397 (Pseudo: 0.00%)
[Iter 850/20000] Loss: 0.0076434 (Best: 0.0064637 @iter796) ([92m↓4.47%[0m) [3.04% of initial]
[Iter 850] Gaussian 0 vs 1:
  Original Loss: 0.0076028
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0076028 (Pseudo: 0.00%)
[Iter 850] Gaussian 1 vs 0:
  Original Loss: 0.0073827
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0073827 (Pseudo: 0.00%)
[Iter 860/20000] Loss: 0.0071295 (Best: 0.0064116 @iter856) ([92m↓6.72%[0m) [2.83% of initial]
[Iter 860] Gaussian 0 vs 1:
  Original Loss: 0.0072083
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0072083 (Pseudo: 0.00%)
[Iter 860] Gaussian 1 vs 0:
  Original Loss: 0.0071205
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0071205 (Pseudo: 0.00%)
[Iter 870/20000] Loss: 0.0067272 (Best: 0.0061713 @iter862) ([92m↓5.64%[0m) [2.67% of initial]
[Iter 870] Gaussian 0 vs 1:
  Original Loss: 0.0062396
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0062396 (Pseudo: 0.00%)
[Iter 870] Gaussian 1 vs 0:
  Original Loss: 0.0062096
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0062096 (Pseudo: 0.00%)
[Iter 880/20000] Loss: 0.0067342 (Best: 0.0058690 @iter875) ([91m↑0.10%[0m) [2.68% of initial]
[Iter 880] Gaussian 0 vs 1:
  Original Loss: 0.0068499
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0068499 (Pseudo: 0.00%)
[Iter 880] Gaussian 1 vs 0:
  Original Loss: 0.0067490
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0067490 (Pseudo: 0.00%)
[Iter 890/20000] Loss: 0.0062803 (Best: 0.0057235 @iter884) ([92m↓6.74%[0m) [2.50% of initial]
[Iter 890] Gaussian 0 vs 1:
  Original Loss: 0.0057370
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0057370 (Pseudo: 0.00%)
[Iter 890] Gaussian 1 vs 0:
  Original Loss: 0.0057240
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0057240 (Pseudo: 0.00%)
Iter:899, L1 loss=0.003652, Total loss=0.005587, Time:14
[Iter 900/20000] Loss: 0.0064505 (Best: 0.0055866 @iter899) ([91m↑2.71%[0m) [2.56% of initial]
[Iter 900] Gaussian 0 vs 1:
  Original Loss: 0.0066263
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0066263 (Pseudo: 0.00%)
[Iter 900] Gaussian 1 vs 0:
  Original Loss: 0.0066667
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0066667 (Pseudo: 0.00%)
[Iter 910/20000] Loss: 0.0066189 (Best: 0.0054380 @iter907) ([91m↑2.61%[0m) [2.63% of initial]
[Iter 910] Gaussian 0 vs 1:
  Original Loss: 0.0069898
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0069898 (Pseudo: 0.00%)
[Iter 910] Gaussian 1 vs 0:
  Original Loss: 0.0068097
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0068097 (Pseudo: 0.00%)
[Iter 920/20000] Loss: 0.0059797 (Best: 0.0053167 @iter916) ([92m↓9.66%[0m) [2.38% of initial]
[Iter 920] Gaussian 0 vs 1:
  Original Loss: 0.0060827
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0060827 (Pseudo: 0.00%)
[Iter 920] Gaussian 1 vs 0:
  Original Loss: 0.0060409
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0060409 (Pseudo: 0.00%)
[Iter 930/20000] Loss: 0.0062437 (Best: 0.0052350 @iter925) ([91m↑4.41%[0m) [2.48% of initial]
[Iter 930] Gaussian 0 vs 1:
  Original Loss: 0.0067163
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0067163 (Pseudo: 0.00%)
[Iter 930] Gaussian 1 vs 0:
  Original Loss: 0.0065603
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0065603 (Pseudo: 0.00%)
[Iter 940/20000] Loss: 0.0063052 (Best: 0.0052350 @iter925) ([91m↑0.99%[0m) [2.50% of initial]
[Iter 940] Gaussian 0 vs 1:
  Original Loss: 0.0066315
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0066315 (Pseudo: 0.00%)
[Iter 940] Gaussian 1 vs 0:
  Original Loss: 0.0065329
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0065329 (Pseudo: 0.00%)
[Iter 950/20000] Loss: 0.0059025 (Best: 0.0052204 @iter946) ([92m↓6.39%[0m) [2.35% of initial]
[Iter 950] Gaussian 0 vs 1:
  Original Loss: 0.0056459
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0056459 (Pseudo: 0.00%)
[Iter 950] Gaussian 1 vs 0:
  Original Loss: 0.0054311
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0054311 (Pseudo: 0.00%)
[Iter 960/20000] Loss: 0.0060538 (Best: 0.0052204 @iter946) ([91m↑2.56%[0m) [2.41% of initial]
[Iter 960] Gaussian 0 vs 1:
  Original Loss: 0.0063931
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0063931 (Pseudo: 0.00%)
[Iter 960] Gaussian 1 vs 0:
  Original Loss: 0.0062423
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0062423 (Pseudo: 0.00%)
[Iter 970/20000] Loss: 0.0060936 (Best: 0.0052204 @iter946) ([91m↑0.66%[0m) [2.42% of initial]
[Iter 970] Gaussian 0 vs 1:
  Original Loss: 0.0060242
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0060242 (Pseudo: 0.00%)
[Iter 970] Gaussian 1 vs 0:
  Original Loss: 0.0058642
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0058642 (Pseudo: 0.00%)
[Iter 980/20000] Loss: 0.0062135 (Best: 0.0052204 @iter946) ([91m↑1.97%[0m) [2.47% of initial]
[Iter 980] Gaussian 0 vs 1:
  Original Loss: 0.0066441
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0066441 (Pseudo: 0.00%)
[Iter 980] Gaussian 1 vs 0:
  Original Loss: 0.0065955
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0065955 (Pseudo: 0.00%)
[Iter 990/20000] Loss: 0.0063943 (Best: 0.0052204 @iter946) ([91m↑2.91%[0m) [2.54% of initial]
[Iter 990] Gaussian 0 vs 1:
  Original Loss: 0.0065431
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0065431 (Pseudo: 0.00%)
[Iter 990] Gaussian 1 vs 0:
  Original Loss: 0.0061843
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0061843 (Pseudo: 0.00%)
Iter:999, L1 loss=0.004345, Total loss=0.006774, Time:14
[Iter 1000/20000] Loss: 0.0064013 (Best: 0.0052204 @iter946) ([91m↑0.11%[0m) [2.54% of initial]
[Iter 1000] Gaussian 0 vs 1:
  Original Loss: 0.0066241
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0066241 (Pseudo: 0.00%)
[Iter 1000] Gaussian 1 vs 0:
  Original Loss: 0.0063582
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0063582 (Pseudo: 0.00%)
[Iter 1010/20000] Loss: 0.0118952 (Best: 0.0052204 @iter946) ([91m↑85.83%[0m) [4.73% of initial]
[Iter 1010] Gaussian 0 vs 1:
  Original Loss: 0.0110860
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0110860 (Pseudo: 0.00%)
[Iter 1010] Gaussian 1 vs 0:
  Original Loss: 0.0111453
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0111453 (Pseudo: 0.00%)
[Iter 1020/20000] Loss: 0.0084708 (Best: 0.0052204 @iter946) ([92m↓28.79%[0m) [3.37% of initial]
[Iter 1020] Gaussian 0 vs 1:
  Original Loss: 0.0080464
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0080464 (Pseudo: 0.00%)
[Iter 1020] Gaussian 1 vs 0:
  Original Loss: 0.0077688
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0077688 (Pseudo: 0.00%)
[Iter 1030/20000] Loss: 0.0068693 (Best: 0.0052204 @iter946) ([92m↓18.91%[0m) [2.73% of initial]
[Iter 1030] Gaussian 0 vs 1:
  Original Loss: 0.0061249
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0061249 (Pseudo: 0.00%)
[Iter 1030] Gaussian 1 vs 0:
  Original Loss: 0.0058401
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0058401 (Pseudo: 0.00%)
[Iter 1040/20000] Loss: 0.0059400 (Best: 0.0052204 @iter946) ([92m↓13.53%[0m) [2.36% of initial]
[Iter 1040] Gaussian 0 vs 1:
  Original Loss: 0.0058220
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0058220 (Pseudo: 0.00%)
[Iter 1040] Gaussian 1 vs 0:
  Original Loss: 0.0059611
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0059611 (Pseudo: 0.00%)
[Iter 1050/20000] Loss: 0.0058424 (Best: 0.0051647 @iter1049) ([92m↓1.64%[0m) [2.32% of initial]
[Iter 1050] Gaussian 0 vs 1:
  Original Loss: 0.0059484
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0059484 (Pseudo: 0.00%)
[Iter 1050] Gaussian 1 vs 0:
  Original Loss: 0.0059871
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0059871 (Pseudo: 0.00%)
[Iter 1060/20000] Loss: 0.0057332 (Best: 0.0049453 @iter1055) ([92m↓1.87%[0m) [2.28% of initial]
[Iter 1060] Gaussian 0 vs 1:
  Original Loss: 0.0058894
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0058894 (Pseudo: 0.00%)
[Iter 1060] Gaussian 1 vs 0:
  Original Loss: 0.0057905
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0057905 (Pseudo: 0.00%)
[Iter 1070/20000] Loss: 0.0054658 (Best: 0.0045379 @iter1066) ([92m↓4.66%[0m) [2.17% of initial]
[Iter 1070] Gaussian 0 vs 1:
  Original Loss: 0.0060349
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0060349 (Pseudo: 0.00%)
[Iter 1070] Gaussian 1 vs 0:
  Original Loss: 0.0058870
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0058870 (Pseudo: 0.00%)
[Iter 1080/20000] Loss: 0.0051875 (Best: 0.0045379 @iter1066) ([92m↓5.09%[0m) [2.06% of initial]
[Iter 1080] Gaussian 0 vs 1:
  Original Loss: 0.0048912
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0048912 (Pseudo: 0.00%)
[Iter 1080] Gaussian 1 vs 0:
  Original Loss: 0.0048083
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0048083 (Pseudo: 0.00%)
[Iter 1090/20000] Loss: 0.0050573 (Best: 0.0044906 @iter1082) ([92m↓2.51%[0m) [2.01% of initial]
[Iter 1090] Gaussian 0 vs 1:
  Original Loss: 0.0049748
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0049748 (Pseudo: 0.00%)
[Iter 1090] Gaussian 1 vs 0:
  Original Loss: 0.0049022
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0049022 (Pseudo: 0.00%)
Iter:1099, L1 loss=0.003485, Total loss=0.004989, Time:14
[Iter 1100/20000] Loss: 0.0049005 (Best: 0.0042519 @iter1093) ([92m↓3.10%[0m) [1.95% of initial]
[Iter 1100] Gaussian 0 vs 1:
  Original Loss: 0.0045945
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0045945 (Pseudo: 0.00%)
[Iter 1100] Gaussian 1 vs 0:
  Original Loss: 0.0046067
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0046067 (Pseudo: 0.00%)
[Iter 1110/20000] Loss: 0.0050984 (Best: 0.0042519 @iter1093) ([91m↑4.04%[0m) [2.03% of initial]
[Iter 1110] Gaussian 0 vs 1:
  Original Loss: 0.0048770
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0048770 (Pseudo: 0.00%)
[Iter 1110] Gaussian 1 vs 0:
  Original Loss: 0.0047438
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0047438 (Pseudo: 0.00%)
[Iter 1120/20000] Loss: 0.0050290 (Best: 0.0041586 @iter1117) ([92m↓1.36%[0m) [2.00% of initial]
[Iter 1120] Gaussian 0 vs 1:
  Original Loss: 0.0049384
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0049384 (Pseudo: 0.00%)
[Iter 1120] Gaussian 1 vs 0:
  Original Loss: 0.0049461
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0049461 (Pseudo: 0.00%)
[Iter 1130/20000] Loss: 0.0052986 (Best: 0.0041586 @iter1117) ([91m↑5.36%[0m) [2.11% of initial]
[Iter 1130] Gaussian 0 vs 1:
  Original Loss: 0.0056109
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0056109 (Pseudo: 0.00%)
[Iter 1130] Gaussian 1 vs 0:
  Original Loss: 0.0054502
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0054502 (Pseudo: 0.00%)
[Iter 1140/20000] Loss: 0.0047730 (Best: 0.0041325 @iter1135) ([92m↓9.92%[0m) [1.90% of initial]
[Iter 1140] Gaussian 0 vs 1:
  Original Loss: 0.0047845
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0047845 (Pseudo: 0.00%)
[Iter 1140] Gaussian 1 vs 0:
  Original Loss: 0.0048541
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0048541 (Pseudo: 0.00%)
[Iter 1150/20000] Loss: 0.0045447 (Best: 0.0040237 @iter1145) ([92m↓4.78%[0m) [1.81% of initial]
[Iter 1150] Gaussian 0 vs 1:
  Original Loss: 0.0041806
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0041806 (Pseudo: 0.00%)
[Iter 1150] Gaussian 1 vs 0:
  Original Loss: 0.0040689
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0040689 (Pseudo: 0.00%)
[Iter 1160/20000] Loss: 0.0050417 (Best: 0.0040237 @iter1145) ([91m↑10.94%[0m) [2.00% of initial]
[Iter 1160] Gaussian 0 vs 1:
  Original Loss: 0.0048474
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0048474 (Pseudo: 0.00%)
[Iter 1160] Gaussian 1 vs 0:
  Original Loss: 0.0050079
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0050079 (Pseudo: 0.00%)
[Iter 1170/20000] Loss: 0.0046799 (Best: 0.0040237 @iter1145) ([92m↓7.18%[0m) [1.86% of initial]
[Iter 1170] Gaussian 0 vs 1:
  Original Loss: 0.0044233
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0044233 (Pseudo: 0.00%)
[Iter 1170] Gaussian 1 vs 0:
  Original Loss: 0.0044434
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0044434 (Pseudo: 0.00%)
[Iter 1180/20000] Loss: 0.0043715 (Best: 0.0039724 @iter1180) ([92m↓6.59%[0m) [1.74% of initial]
[Iter 1180] Gaussian 0 vs 1:
  Original Loss: 0.0039724
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0039724 (Pseudo: 0.00%)
[Iter 1180] Gaussian 1 vs 0:
  Original Loss: 0.0039913
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0039913 (Pseudo: 0.00%)
[Iter 1190/20000] Loss: 0.0046370 (Best: 0.0039724 @iter1180) ([91m↑6.07%[0m) [1.84% of initial]
[Iter 1190] Gaussian 0 vs 1:
  Original Loss: 0.0044128
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0044128 (Pseudo: 0.00%)
[Iter 1190] Gaussian 1 vs 0:
  Original Loss: 0.0043338
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0043338 (Pseudo: 0.00%)
Iter:1199, L1 loss=0.003392, Total loss=0.004931, Time:14
[Iter 1200/20000] Loss: 0.0046300 (Best: 0.0038497 @iter1192) ([92m↓0.15%[0m) [1.84% of initial]
[Iter 1200] Gaussian 0 vs 1:
  Original Loss: 0.0046861
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0046861 (Pseudo: 0.00%)
[Iter 1200] Gaussian 1 vs 0:
  Original Loss: 0.0044913
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0044913 (Pseudo: 0.00%)
[Iter 1210/20000] Loss: 0.0112805 (Best: 0.0038497 @iter1192) ([91m↑143.64%[0m) [4.48% of initial]
[Iter 1210] Gaussian 0 vs 1:
  Original Loss: 0.0102415
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0102415 (Pseudo: 0.00%)
[Iter 1210] Gaussian 1 vs 0:
  Original Loss: 0.0097431
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0097431 (Pseudo: 0.00%)
[Iter 1220/20000] Loss: 0.0069370 (Best: 0.0038497 @iter1192) ([92m↓38.50%[0m) [2.76% of initial]
[Iter 1220] Gaussian 0 vs 1:
  Original Loss: 0.0061284
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0061284 (Pseudo: 0.00%)
[Iter 1220] Gaussian 1 vs 0:
  Original Loss: 0.0061501
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0061501 (Pseudo: 0.00%)
[Iter 1230/20000] Loss: 0.0060462 (Best: 0.0038497 @iter1192) ([92m↓12.84%[0m) [2.40% of initial]
[Iter 1230] Gaussian 0 vs 1:
  Original Loss: 0.0064972
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0064972 (Pseudo: 0.00%)
[Iter 1230] Gaussian 1 vs 0:
  Original Loss: 0.0061536
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0061536 (Pseudo: 0.00%)
[Iter 1240/20000] Loss: 0.0054401 (Best: 0.0038497 @iter1192) ([92m↓10.02%[0m) [2.16% of initial]
[Iter 1240] Gaussian 0 vs 1:
  Original Loss: 0.0055948
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0055948 (Pseudo: 0.00%)
[Iter 1240] Gaussian 1 vs 0:
  Original Loss: 0.0053289
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0053289 (Pseudo: 0.00%)
[Iter 1250/20000] Loss: 0.0047517 (Best: 0.0038497 @iter1192) ([92m↓12.65%[0m) [1.89% of initial]
[Iter 1250] Gaussian 0 vs 1:
  Original Loss: 0.0045857
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0045857 (Pseudo: 0.00%)
[Iter 1250] Gaussian 1 vs 0:
  Original Loss: 0.0046398
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0046398 (Pseudo: 0.00%)
[Iter 1260/20000] Loss: 0.0046090 (Best: 0.0037383 @iter1258) ([92m↓3.00%[0m) [1.83% of initial]
[Iter 1260] Gaussian 0 vs 1:
  Original Loss: 0.0051981
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0051981 (Pseudo: 0.00%)
[Iter 1260] Gaussian 1 vs 0:
  Original Loss: 0.0050528
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0050528 (Pseudo: 0.00%)
[Iter 1270/20000] Loss: 0.0041134 (Best: 0.0037324 @iter1269) ([92m↓10.75%[0m) [1.63% of initial]
[Iter 1270] Gaussian 0 vs 1:
  Original Loss: 0.0040533
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0040533 (Pseudo: 0.00%)
[Iter 1270] Gaussian 1 vs 0:
  Original Loss: 0.0040810
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0040810 (Pseudo: 0.00%)
[Iter 1280/20000] Loss: 0.0043556 (Best: 0.0033887 @iter1273) ([91m↑5.89%[0m) [1.73% of initial]
[Iter 1280] Gaussian 0 vs 1:
  Original Loss: 0.0042083
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0042083 (Pseudo: 0.00%)
[Iter 1280] Gaussian 1 vs 0:
  Original Loss: 0.0043291
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0043291 (Pseudo: 0.00%)
[Iter 1290/20000] Loss: 0.0042341 (Best: 0.0033288 @iter1288) ([92m↓2.79%[0m) [1.68% of initial]
[Iter 1290] Gaussian 0 vs 1:
  Original Loss: 0.0047543
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0047543 (Pseudo: 0.00%)
[Iter 1290] Gaussian 1 vs 0:
  Original Loss: 0.0047068
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0047068 (Pseudo: 0.00%)
Iter:1299, L1 loss=0.002782, Total loss=0.003615, Time:13
[Iter 1300/20000] Loss: 0.0039849 (Best: 0.0033288 @iter1288) ([92m↓5.89%[0m) [1.58% of initial]
[Iter 1300] Gaussian 0 vs 1:
  Original Loss: 0.0039749
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0039749 (Pseudo: 0.00%)
[Iter 1300] Gaussian 1 vs 0:
  Original Loss: 0.0041416
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0041416 (Pseudo: 0.00%)
[Iter 1310/20000] Loss: 0.0040037 (Best: 0.0033161 @iter1301) ([91m↑0.47%[0m) [1.59% of initial]
[Iter 1310] Gaussian 0 vs 1:
  Original Loss: 0.0037666
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0037666 (Pseudo: 0.00%)
[Iter 1310] Gaussian 1 vs 0:
  Original Loss: 0.0038018
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0038018 (Pseudo: 0.00%)
[Iter 1320/20000] Loss: 0.0038967 (Best: 0.0031011 @iter1319) ([92m↓2.67%[0m) [1.55% of initial]
[Iter 1320] Gaussian 0 vs 1:
  Original Loss: 0.0045494
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0045494 (Pseudo: 0.00%)
[Iter 1320] Gaussian 1 vs 0:
  Original Loss: 0.0044507
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0044507 (Pseudo: 0.00%)
[Iter 1330/20000] Loss: 0.0039666 (Best: 0.0030481 @iter1321) ([91m↑1.79%[0m) [1.58% of initial]
[Iter 1330] Gaussian 0 vs 1:
  Original Loss: 0.0041565
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0041565 (Pseudo: 0.00%)
[Iter 1330] Gaussian 1 vs 0:
  Original Loss: 0.0040524
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0040524 (Pseudo: 0.00%)
[Iter 1340/20000] Loss: 0.0037236 (Best: 0.0030481 @iter1321) ([92m↓6.13%[0m) [1.48% of initial]
[Iter 1340] Gaussian 0 vs 1:
  Original Loss: 0.0033659
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0033659 (Pseudo: 0.00%)
[Iter 1340] Gaussian 1 vs 0:
  Original Loss: 0.0033322
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0033322 (Pseudo: 0.00%)
[Iter 1350/20000] Loss: 0.0037979 (Best: 0.0030481 @iter1321) ([91m↑2.00%[0m) [1.51% of initial]
[Iter 1350] Gaussian 0 vs 1:
  Original Loss: 0.0033737
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0033737 (Pseudo: 0.00%)
[Iter 1350] Gaussian 1 vs 0:
  Original Loss: 0.0032764
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0032764 (Pseudo: 0.00%)
[Iter 1360/20000] Loss: 0.0038875 (Best: 0.0030481 @iter1321) ([91m↑2.36%[0m) [1.54% of initial]
[Iter 1360] Gaussian 0 vs 1:
  Original Loss: 0.0037958
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0037958 (Pseudo: 0.00%)
[Iter 1360] Gaussian 1 vs 0:
  Original Loss: 0.0035427
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0035427 (Pseudo: 0.00%)
[Iter 1370/20000] Loss: 0.0037024 (Best: 0.0030481 @iter1321) ([92m↓4.76%[0m) [1.47% of initial]
[Iter 1370] Gaussian 0 vs 1:
  Original Loss: 0.0032455
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0032455 (Pseudo: 0.00%)
[Iter 1370] Gaussian 1 vs 0:
  Original Loss: 0.0031189
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0031189 (Pseudo: 0.00%)
[Iter 1380/20000] Loss: 0.0039643 (Best: 0.0030481 @iter1321) ([91m↑7.07%[0m) [1.57% of initial]
[Iter 1380] Gaussian 0 vs 1:
  Original Loss: 0.0039208
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0039208 (Pseudo: 0.00%)
[Iter 1380] Gaussian 1 vs 0:
  Original Loss: 0.0038356
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0038356 (Pseudo: 0.00%)
[Iter 1390/20000] Loss: 0.0037643 (Best: 0.0030481 @iter1321) ([92m↓5.05%[0m) [1.50% of initial]
[Iter 1390] Gaussian 0 vs 1:
  Original Loss: 0.0036368
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0036368 (Pseudo: 0.00%)
[Iter 1390] Gaussian 1 vs 0:
  Original Loss: 0.0034566
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0034566 (Pseudo: 0.00%)
Iter:1399, L1 loss=0.002266, Total loss=0.002966, Time:13
[Iter 1400/20000] Loss: 0.0034687 (Best: 0.0029658 @iter1399) ([92m↓7.85%[0m) [1.38% of initial]
[Iter 1400] Gaussian 0 vs 1:
  Original Loss: 0.0035717
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0035717 (Pseudo: 0.00%)
[Iter 1400] Gaussian 1 vs 0:
  Original Loss: 0.0034101
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0034101 (Pseudo: 0.00%)
[Iter 1410/20000] Loss: 0.0090176 (Best: 0.0029658 @iter1399) ([91m↑159.97%[0m) [3.58% of initial]
[Iter 1410] Gaussian 0 vs 1:
  Original Loss: 0.0087644
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0087644 (Pseudo: 0.00%)
[Iter 1410] Gaussian 1 vs 0:
  Original Loss: 0.0085339
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0085339 (Pseudo: 0.00%)
[Iter 1420/20000] Loss: 0.0060504 (Best: 0.0029658 @iter1399) ([92m↓32.90%[0m) [2.40% of initial]
[Iter 1420] Gaussian 0 vs 1:
  Original Loss: 0.0057289
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0057289 (Pseudo: 0.00%)
[Iter 1420] Gaussian 1 vs 0:
  Original Loss: 0.0056041
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0056041 (Pseudo: 0.00%)
[Iter 1430/20000] Loss: 0.0050223 (Best: 0.0029658 @iter1399) ([92m↓16.99%[0m) [2.00% of initial]
[Iter 1430] Gaussian 0 vs 1:
  Original Loss: 0.0043139
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0043139 (Pseudo: 0.00%)
[Iter 1430] Gaussian 1 vs 0:
  Original Loss: 0.0043868
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0043868 (Pseudo: 0.00%)
[Iter 1440/20000] Loss: 0.0044881 (Best: 0.0029658 @iter1399) ([92m↓10.64%[0m) [1.78% of initial]
[Iter 1440] Gaussian 0 vs 1:
  Original Loss: 0.0044477
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0044477 (Pseudo: 0.00%)
[Iter 1440] Gaussian 1 vs 0:
  Original Loss: 0.0045174
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0045174 (Pseudo: 0.00%)
[Iter 1450/20000] Loss: 0.0035636 (Best: 0.0029658 @iter1399) ([92m↓20.60%[0m) [1.42% of initial]
[Iter 1450] Gaussian 0 vs 1:
  Original Loss: 0.0031826
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0031826 (Pseudo: 0.00%)
[Iter 1450] Gaussian 1 vs 0:
  Original Loss: 0.0031111
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0031111 (Pseudo: 0.00%)
[Iter 1460/20000] Loss: 0.0036491 (Best: 0.0029658 @iter1399) ([91m↑2.40%[0m) [1.45% of initial]
[Iter 1460] Gaussian 0 vs 1:
  Original Loss: 0.0037542
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0037542 (Pseudo: 0.00%)
[Iter 1460] Gaussian 1 vs 0:
  Original Loss: 0.0035345
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0035345 (Pseudo: 0.00%)
[Iter 1470/20000] Loss: 0.0034877 (Best: 0.0029658 @iter1399) ([92m↓4.42%[0m) [1.39% of initial]
[Iter 1470] Gaussian 0 vs 1:
  Original Loss: 0.0031401
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0031401 (Pseudo: 0.00%)
[Iter 1470] Gaussian 1 vs 0:
  Original Loss: 0.0030656
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0030656 (Pseudo: 0.00%)
[Iter 1480/20000] Loss: 0.0033352 (Best: 0.0027651 @iter1480) ([92m↓4.37%[0m) [1.33% of initial]
[Iter 1480] Gaussian 0 vs 1:
  Original Loss: 0.0027651
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0027651 (Pseudo: 0.00%)
[Iter 1480] Gaussian 1 vs 0:
  Original Loss: 0.0027461
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0027461 (Pseudo: 0.00%)
[Iter 1490/20000] Loss: 0.0032731 (Best: 0.0027651 @iter1480) ([92m↓1.86%[0m) [1.30% of initial]
[Iter 1490] Gaussian 0 vs 1:
  Original Loss: 0.0032476
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0032476 (Pseudo: 0.00%)
[Iter 1490] Gaussian 1 vs 0:
  Original Loss: 0.0031494
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0031494 (Pseudo: 0.00%)
Iter:1499, L1 loss=0.002505, Total loss=0.003348, Time:13
[Iter 1500/20000] Loss: 0.0032264 (Best: 0.0027651 @iter1480) ([92m↓1.43%[0m) [1.28% of initial]
[Iter 1500] Gaussian 0 vs 1:
  Original Loss: 0.0029305
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0029305 (Pseudo: 0.00%)
[Iter 1500] Gaussian 1 vs 0:
  Original Loss: 0.0028685
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0028685 (Pseudo: 0.00%)
[Iter 1510/20000] Loss: 0.0030767 (Best: 0.0025984 @iter1504) ([92m↓4.64%[0m) [1.22% of initial]
[Iter 1510] Gaussian 0 vs 1:
  Original Loss: 0.0027949
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0027949 (Pseudo: 0.00%)
[Iter 1510] Gaussian 1 vs 0:
  Original Loss: 0.0027258
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0027258 (Pseudo: 0.00%)
[Iter 1520/20000] Loss: 0.0030451 (Best: 0.0025810 @iter1520) ([92m↓1.03%[0m) [1.21% of initial]
[Iter 1520] Gaussian 0 vs 1:
  Original Loss: 0.0025810
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0025810 (Pseudo: 0.00%)
[Iter 1520] Gaussian 1 vs 0:
  Original Loss: 0.0025351
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0025351 (Pseudo: 0.00%)
[Iter 1530/20000] Loss: 0.0032258 (Best: 0.0025554 @iter1526) ([91m↑5.93%[0m) [1.28% of initial]
[Iter 1530] Gaussian 0 vs 1:
  Original Loss: 0.0032819
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0032819 (Pseudo: 0.00%)
[Iter 1530] Gaussian 1 vs 0:
  Original Loss: 0.0031134
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0031134 (Pseudo: 0.00%)
[Iter 1540/20000] Loss: 0.0031101 (Best: 0.0025554 @iter1526) ([92m↓3.59%[0m) [1.24% of initial]
[Iter 1540] Gaussian 0 vs 1:
  Original Loss: 0.0028985
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0028985 (Pseudo: 0.00%)
[Iter 1540] Gaussian 1 vs 0:
  Original Loss: 0.0027851
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0027851 (Pseudo: 0.00%)
[Iter 1550/20000] Loss: 0.0031275 (Best: 0.0025554 @iter1526) ([91m↑0.56%[0m) [1.24% of initial]
[Iter 1550] Gaussian 0 vs 1:
  Original Loss: 0.0030683
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0030683 (Pseudo: 0.00%)
[Iter 1550] Gaussian 1 vs 0:
  Original Loss: 0.0028907
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0028907 (Pseudo: 0.00%)
[Iter 1560/20000] Loss: 0.0032943 (Best: 0.0025554 @iter1526) ([91m↑5.33%[0m) [1.31% of initial]
[Iter 1560] Gaussian 0 vs 1:
  Original Loss: 0.0038661
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0038661 (Pseudo: 0.00%)
[Iter 1560] Gaussian 1 vs 0:
  Original Loss: 0.0038242
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0038242 (Pseudo: 0.00%)
[Iter 1570/20000] Loss: 0.0028161 (Best: 0.0025457 @iter1569) ([92m↓14.52%[0m) [1.12% of initial]
[Iter 1570] Gaussian 0 vs 1:
  Original Loss: 0.0026708
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0026708 (Pseudo: 0.00%)
[Iter 1570] Gaussian 1 vs 0:
  Original Loss: 0.0026786
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0026786 (Pseudo: 0.00%)
[Iter 1580/20000] Loss: 0.0028547 (Best: 0.0023658 @iter1573) ([91m↑1.37%[0m) [1.13% of initial]
[Iter 1580] Gaussian 0 vs 1:
  Original Loss: 0.0029220
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0029220 (Pseudo: 0.00%)
[Iter 1580] Gaussian 1 vs 0:
  Original Loss: 0.0029874
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0029874 (Pseudo: 0.00%)
[Iter 1590/20000] Loss: 0.0027772 (Best: 0.0023658 @iter1573) ([92m↓2.72%[0m) [1.10% of initial]
[Iter 1590] Gaussian 0 vs 1:
  Original Loss: 0.0025625
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0025625 (Pseudo: 0.00%)
[Iter 1590] Gaussian 1 vs 0:
  Original Loss: 0.0024796
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0024796 (Pseudo: 0.00%)
Iter:1599, L1 loss=0.002648, Total loss=0.003395, Time:13
[Iter 1600/20000] Loss: 0.0030716 (Best: 0.0023415 @iter1591) ([91m↑10.60%[0m) [1.22% of initial]
[Iter 1600] Gaussian 0 vs 1:
  Original Loss: 0.0032684
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0032684 (Pseudo: 0.00%)
[Iter 1600] Gaussian 1 vs 0:
  Original Loss: 0.0032044
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0032044 (Pseudo: 0.00%)
[Iter 1610/20000] Loss: 0.0084165 (Best: 0.0023415 @iter1591) ([91m↑174.01%[0m) [3.34% of initial]
[Iter 1610] Gaussian 0 vs 1:
  Original Loss: 0.0072288
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0072288 (Pseudo: 0.00%)
[Iter 1610] Gaussian 1 vs 0:
  Original Loss: 0.0074115
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0074115 (Pseudo: 0.00%)
[Iter 1620/20000] Loss: 0.0054665 (Best: 0.0023415 @iter1591) ([92m↓35.05%[0m) [2.17% of initial]
[Iter 1620] Gaussian 0 vs 1:
  Original Loss: 0.0060430
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0060430 (Pseudo: 0.00%)
[Iter 1620] Gaussian 1 vs 0:
  Original Loss: 0.0060418
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0060418 (Pseudo: 0.00%)
[Iter 1630/20000] Loss: 0.0040720 (Best: 0.0023415 @iter1591) ([92m↓25.51%[0m) [1.62% of initial]
[Iter 1630] Gaussian 0 vs 1:
  Original Loss: 0.0035955
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0035955 (Pseudo: 0.00%)
[Iter 1630] Gaussian 1 vs 0:
  Original Loss: 0.0035838
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0035838 (Pseudo: 0.00%)
[Iter 1640/20000] Loss: 0.0038469 (Best: 0.0023415 @iter1591) ([92m↓5.53%[0m) [1.53% of initial]
[Iter 1640] Gaussian 0 vs 1:
  Original Loss: 0.0043440
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0043440 (Pseudo: 0.00%)
[Iter 1640] Gaussian 1 vs 0:
  Original Loss: 0.0043648
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0043648 (Pseudo: 0.00%)
[Iter 1650/20000] Loss: 0.0034028 (Best: 0.0023415 @iter1591) ([92m↓11.54%[0m) [1.35% of initial]
[Iter 1650] Gaussian 0 vs 1:
  Original Loss: 0.0034259
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0034259 (Pseudo: 0.00%)
[Iter 1650] Gaussian 1 vs 0:
  Original Loss: 0.0034279
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0034279 (Pseudo: 0.00%)
[Iter 1660/20000] Loss: 0.0029348 (Best: 0.0023415 @iter1591) ([92m↓13.75%[0m) [1.17% of initial]
[Iter 1660] Gaussian 0 vs 1:
  Original Loss: 0.0025539
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0025539 (Pseudo: 0.00%)
[Iter 1660] Gaussian 1 vs 0:
  Original Loss: 0.0024665
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0024665 (Pseudo: 0.00%)
[Iter 1670/20000] Loss: 0.0027250 (Best: 0.0022377 @iter1669) ([92m↓7.15%[0m) [1.08% of initial]
[Iter 1670] Gaussian 0 vs 1:
  Original Loss: 0.0027261
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0027261 (Pseudo: 0.00%)
[Iter 1670] Gaussian 1 vs 0:
  Original Loss: 0.0027722
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0027722 (Pseudo: 0.00%)
[Iter 1680/20000] Loss: 0.0029824 (Best: 0.0022377 @iter1669) ([91m↑9.45%[0m) [1.18% of initial]
[Iter 1680] Gaussian 0 vs 1:
  Original Loss: 0.0028817
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0028817 (Pseudo: 0.00%)
[Iter 1680] Gaussian 1 vs 0:
  Original Loss: 0.0026041
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0026041 (Pseudo: 0.00%)
[Iter 1690/20000] Loss: 0.0031696 (Best: 0.0022377 @iter1669) ([91m↑6.28%[0m) [1.26% of initial]
[Iter 1690] Gaussian 0 vs 1:
  Original Loss: 0.0029565
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0029565 (Pseudo: 0.00%)
[Iter 1690] Gaussian 1 vs 0:
  Original Loss: 0.0027443
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0027443 (Pseudo: 0.00%)
Iter:1699, L1 loss=0.002619, Total loss=0.003281, Time:13
[Iter 1700/20000] Loss: 0.0028313 (Best: 0.0022377 @iter1669) ([92m↓10.67%[0m) [1.12% of initial]
[Iter 1700] Gaussian 0 vs 1:
  Original Loss: 0.0024241
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0024241 (Pseudo: 0.00%)
[Iter 1700] Gaussian 1 vs 0:
  Original Loss: 0.0023661
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0023661 (Pseudo: 0.00%)
[Iter 1710/20000] Loss: 0.0030485 (Best: 0.0022377 @iter1669) ([91m↑7.67%[0m) [1.21% of initial]
[Iter 1710] Gaussian 0 vs 1:
  Original Loss: 0.0031695
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0031695 (Pseudo: 0.00%)
[Iter 1710] Gaussian 1 vs 0:
  Original Loss: 0.0031918
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0031918 (Pseudo: 0.00%)
[Iter 1720/20000] Loss: 0.0025166 (Best: 0.0022377 @iter1669) ([92m↓17.45%[0m) [1.00% of initial]
[Iter 1720] Gaussian 0 vs 1:
  Original Loss: 0.0022935
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0022935 (Pseudo: 0.00%)
[Iter 1720] Gaussian 1 vs 0:
  Original Loss: 0.0022894
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0022894 (Pseudo: 0.00%)
[Iter 1730/20000] Loss: 0.0025801 (Best: 0.0022285 @iter1726) ([91m↑2.52%[0m) [1.03% of initial]
[Iter 1730] Gaussian 0 vs 1:
  Original Loss: 0.0023047
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0023047 (Pseudo: 0.00%)
[Iter 1730] Gaussian 1 vs 0:
  Original Loss: 0.0022506
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0022506 (Pseudo: 0.00%)
[Iter 1740/20000] Loss: 0.0024981 (Best: 0.0021822 @iter1738) ([92m↓3.18%[0m) [0.99% of initial]
[Iter 1740] Gaussian 0 vs 1:
  Original Loss: 0.0023131
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0023131 (Pseudo: 0.00%)
[Iter 1740] Gaussian 1 vs 0:
  Original Loss: 0.0022769
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0022769 (Pseudo: 0.00%)
[Iter 1750/20000] Loss: 0.0023554 (Best: 0.0020337 @iter1742) ([92m↓5.71%[0m) [0.94% of initial]
[Iter 1750] Gaussian 0 vs 1:
  Original Loss: 0.0020960
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0020960 (Pseudo: 0.00%)
[Iter 1750] Gaussian 1 vs 0:
  Original Loss: 0.0020044
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0020044 (Pseudo: 0.00%)
[Iter 1760/20000] Loss: 0.0027007 (Best: 0.0020337 @iter1742) ([91m↑14.66%[0m) [1.07% of initial]
[Iter 1760] Gaussian 0 vs 1:
  Original Loss: 0.0026516
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0026516 (Pseudo: 0.00%)
[Iter 1760] Gaussian 1 vs 0:
  Original Loss: 0.0024420
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0024420 (Pseudo: 0.00%)
[Iter 1770/20000] Loss: 0.0024499 (Best: 0.0020337 @iter1742) ([92m↓9.29%[0m) [0.97% of initial]
[Iter 1770] Gaussian 0 vs 1:
  Original Loss: 0.0024179
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0024179 (Pseudo: 0.00%)
[Iter 1770] Gaussian 1 vs 0:
  Original Loss: 0.0023375
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0023375 (Pseudo: 0.00%)
[Iter 1780/20000] Loss: 0.0024942 (Best: 0.0020337 @iter1742) ([91m↑1.81%[0m) [0.99% of initial]
[Iter 1780] Gaussian 0 vs 1:
  Original Loss: 0.0026557
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0026557 (Pseudo: 0.00%)
[Iter 1780] Gaussian 1 vs 0:
  Original Loss: 0.0026901
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0026901 (Pseudo: 0.00%)
[Iter 1790/20000] Loss: 0.0021947 (Best: 0.0017947 @iter1789) ([92m↓12.01%[0m) [0.87% of initial]
[Iter 1790] Gaussian 0 vs 1:
  Original Loss: 0.0022580
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0022580 (Pseudo: 0.00%)
[Iter 1790] Gaussian 1 vs 0:
  Original Loss: 0.0021683
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0021683 (Pseudo: 0.00%)
Iter:1799, L1 loss=0.001658, Total loss=0.001923, Time:13
[Iter 1800/20000] Loss: 0.0022046 (Best: 0.0017947 @iter1789) ([91m↑0.45%[0m) [0.88% of initial]
[Iter 1800] Gaussian 0 vs 1:
  Original Loss: 0.0021755
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0021755 (Pseudo: 0.00%)
[Iter 1800] Gaussian 1 vs 0:
  Original Loss: 0.0021331
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0021331 (Pseudo: 0.00%)
[Iter 1810/20000] Loss: 0.0074789 (Best: 0.0017947 @iter1789) ([91m↑239.25%[0m) [2.97% of initial]
[Iter 1810] Gaussian 0 vs 1:
  Original Loss: 0.0066157
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0066157 (Pseudo: 0.00%)
[Iter 1810] Gaussian 1 vs 0:
  Original Loss: 0.0069806
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0069806 (Pseudo: 0.00%)
[Iter 1820/20000] Loss: 0.0043898 (Best: 0.0017947 @iter1789) ([92m↓41.30%[0m) [1.74% of initial]
[Iter 1820] Gaussian 0 vs 1:
  Original Loss: 0.0044454
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0044454 (Pseudo: 0.00%)
[Iter 1820] Gaussian 1 vs 0:
  Original Loss: 0.0046795
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0046795 (Pseudo: 0.00%)
[Iter 1830/20000] Loss: 0.0039609 (Best: 0.0017947 @iter1789) ([92m↓9.77%[0m) [1.57% of initial]
[Iter 1830] Gaussian 0 vs 1:
  Original Loss: 0.0038922
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0038922 (Pseudo: 0.00%)
[Iter 1830] Gaussian 1 vs 0:
  Original Loss: 0.0036706
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0036706 (Pseudo: 0.00%)
[Iter 1840/20000] Loss: 0.0027439 (Best: 0.0017947 @iter1789) ([92m↓30.73%[0m) [1.09% of initial]
[Iter 1840] Gaussian 0 vs 1:
  Original Loss: 0.0023904
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0023904 (Pseudo: 0.00%)
[Iter 1840] Gaussian 1 vs 0:
  Original Loss: 0.0023449
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0023449 (Pseudo: 0.00%)
[Iter 1850/20000] Loss: 0.0026190 (Best: 0.0017947 @iter1789) ([92m↓4.55%[0m) [1.04% of initial]
[Iter 1850] Gaussian 0 vs 1:
  Original Loss: 0.0023982
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0023982 (Pseudo: 0.00%)
[Iter 1850] Gaussian 1 vs 0:
  Original Loss: 0.0023577
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0023577 (Pseudo: 0.00%)
[Iter 1860/20000] Loss: 0.0023776 (Best: 0.0017947 @iter1789) ([92m↓9.22%[0m) [0.94% of initial]
[Iter 1860] Gaussian 0 vs 1:
  Original Loss: 0.0021776
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0021776 (Pseudo: 0.00%)
[Iter 1860] Gaussian 1 vs 0:
  Original Loss: 0.0021057
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0021057 (Pseudo: 0.00%)
[Iter 1870/20000] Loss: 0.0022259 (Best: 0.0017947 @iter1789) ([92m↓6.38%[0m) [0.88% of initial]
[Iter 1870] Gaussian 0 vs 1:
  Original Loss: 0.0020362
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0020362 (Pseudo: 0.00%)
[Iter 1870] Gaussian 1 vs 0:
  Original Loss: 0.0019076
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0019076 (Pseudo: 0.00%)
[Iter 1880/20000] Loss: 0.0021042 (Best: 0.0017947 @iter1789) ([92m↓5.47%[0m) [0.84% of initial]
[Iter 1880] Gaussian 0 vs 1:
  Original Loss: 0.0018354
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0018354 (Pseudo: 0.00%)
[Iter 1880] Gaussian 1 vs 0:
  Original Loss: 0.0017910
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0017910 (Pseudo: 0.00%)
[Iter 1890/20000] Loss: 0.0019212 (Best: 0.0017709 @iter1890) ([92m↓8.70%[0m) [0.76% of initial]
[Iter 1890] Gaussian 0 vs 1:
  Original Loss: 0.0017709
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0017709 (Pseudo: 0.00%)
[Iter 1890] Gaussian 1 vs 0:
  Original Loss: 0.0016648
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0016648 (Pseudo: 0.00%)
Iter:1899, L1 loss=0.001697, Total loss=0.001929, Time:14
[Iter 1900/20000] Loss: 0.0019956 (Best: 0.0016208 @iter1891) ([91m↑3.87%[0m) [0.79% of initial]
[Iter 1900] Gaussian 0 vs 1:
  Original Loss: 0.0018667
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0018667 (Pseudo: 0.00%)
[Iter 1900] Gaussian 1 vs 0:
  Original Loss: 0.0017981
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0017981 (Pseudo: 0.00%)
[Iter 1910/20000] Loss: 0.0020840 (Best: 0.0016208 @iter1891) ([91m↑4.43%[0m) [0.83% of initial]
[Iter 1910] Gaussian 0 vs 1:
  Original Loss: 0.0018534
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0018534 (Pseudo: 0.00%)
[Iter 1910] Gaussian 1 vs 0:
  Original Loss: 0.0017633
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0017633 (Pseudo: 0.00%)
[Iter 1920/20000] Loss: 0.0021427 (Best: 0.0016208 @iter1891) ([91m↑2.81%[0m) [0.85% of initial]
[Iter 1920] Gaussian 0 vs 1:
  Original Loss: 0.0021793
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0021793 (Pseudo: 0.00%)
[Iter 1920] Gaussian 1 vs 0:
  Original Loss: 0.0020639
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0020639 (Pseudo: 0.00%)
[Iter 1930/20000] Loss: 0.0017447 (Best: 0.0015654 @iter1930) ([92m↓18.57%[0m) [0.69% of initial]
[Iter 1930] Gaussian 0 vs 1:
  Original Loss: 0.0015654
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0015654 (Pseudo: 0.00%)
[Iter 1930] Gaussian 1 vs 0:
  Original Loss: 0.0015368
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0015368 (Pseudo: 0.00%)
[Iter 1940/20000] Loss: 0.0019357 (Best: 0.0015609 @iter1939) ([91m↑10.95%[0m) [0.77% of initial]
[Iter 1940] Gaussian 0 vs 1:
  Original Loss: 0.0020470
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0020470 (Pseudo: 0.00%)
[Iter 1940] Gaussian 1 vs 0:
  Original Loss: 0.0019336
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0019336 (Pseudo: 0.00%)
[Iter 1950/20000] Loss: 0.0020685 (Best: 0.0015609 @iter1939) ([91m↑6.86%[0m) [0.82% of initial]
[Iter 1950] Gaussian 0 vs 1:
  Original Loss: 0.0019119
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0019119 (Pseudo: 0.00%)
[Iter 1950] Gaussian 1 vs 0:
  Original Loss: 0.0018449
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0018449 (Pseudo: 0.00%)
[Iter 1960/20000] Loss: 0.0018813 (Best: 0.0015609 @iter1939) ([92m↓9.05%[0m) [0.75% of initial]
[Iter 1960] Gaussian 0 vs 1:
  Original Loss: 0.0017665
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0017665 (Pseudo: 0.00%)
[Iter 1960] Gaussian 1 vs 0:
  Original Loss: 0.0017198
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0017198 (Pseudo: 0.00%)
[Iter 1970/20000] Loss: 0.0017058 (Best: 0.0015387 @iter1963) ([92m↓9.33%[0m) [0.68% of initial]
[Iter 1970] Gaussian 0 vs 1:
  Original Loss: 0.0016120
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0016120 (Pseudo: 0.00%)
[Iter 1970] Gaussian 1 vs 0:
  Original Loss: 0.0015693
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0015693 (Pseudo: 0.00%)
[Iter 1980/20000] Loss: 0.0020655 (Best: 0.0015387 @iter1963) ([91m↑21.09%[0m) [0.82% of initial]
[Iter 1980] Gaussian 0 vs 1:
  Original Loss: 0.0024446
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0024446 (Pseudo: 0.00%)
[Iter 1980] Gaussian 1 vs 0:
  Original Loss: 0.0024964
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0024964 (Pseudo: 0.00%)
[Iter 1990/20000] Loss: 0.0018055 (Best: 0.0015387 @iter1963) ([92m↓12.59%[0m) [0.72% of initial]
[Iter 1990] Gaussian 0 vs 1:
  Original Loss: 0.0016504
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0016504 (Pseudo: 0.00%)
[Iter 1990] Gaussian 1 vs 0:
  Original Loss: 0.0016081
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0016081 (Pseudo: 0.00%)
Iter:1999, L1 loss=0.001507, Total loss=0.001784, Time:14
[Iter 2000/20000] Loss: 0.0019725 (Best: 0.0014723 @iter1996) ([91m↑9.25%[0m) [0.78% of initial]
Testing Speed: 235.2639207362767 fps
Testing Time: 0.21252727508544922 s

[ITER 2000] Evaluating test: SSIM = 0.851223840713501, PSNR = 17.758567562103273
Testing Speed: 269.97901602763534 fps
Testing Time: 0.011111974716186523 s

[ITER 2000] Evaluating train: SSIM = 0.9999455213546753, PSNR = 48.286462148030594
Iter:2000, total_points:42564
[Iter 2000] Gaussian 0 vs 1:
  Original Loss: 0.0021198
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0021198 (Pseudo: 0.00%)
[Iter 2000] Gaussian 1 vs 0:
  Original Loss: 0.0021146
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0021146 (Pseudo: 0.00%)
[Iter 2010/20000] Loss: 0.0068103 (Best: 0.0014723 @iter1996) ([91m↑245.26%[0m) [2.71% of initial]
[Iter 2010] Gaussian 0 vs 1:
  Original Loss: 0.0062222
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0062222 (Pseudo: 0.00%)
[Iter 2010] Gaussian 1 vs 0:
  Original Loss: 0.0060457
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0060457 (Pseudo: 0.00%)
[Iter 2020/20000] Loss: 0.0036691 (Best: 0.0014723 @iter1996) ([92m↓46.12%[0m) [1.46% of initial]
[Iter 2020] Gaussian 0 vs 1:
  Original Loss: 0.0032271
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0032271 (Pseudo: 0.00%)
[Iter 2020] Gaussian 1 vs 0:
  Original Loss: 0.0031352
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0031352 (Pseudo: 0.00%)
[Iter 2030/20000] Loss: 0.0028172 (Best: 0.0014723 @iter1996) ([92m↓23.22%[0m) [1.12% of initial]
[Iter 2030] Gaussian 0 vs 1:
  Original Loss: 0.0025100
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0025100 (Pseudo: 0.00%)
[Iter 2030] Gaussian 1 vs 0:
  Original Loss: 0.0024845
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0024845 (Pseudo: 0.00%)
[Iter 2040/20000] Loss: 0.0024673 (Best: 0.0014723 @iter1996) ([92m↓12.42%[0m) [0.98% of initial]
[Iter 2040] Gaussian 0 vs 1:
  Original Loss: 0.0027615
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0027615 (Pseudo: 0.00%)
[Iter 2040] Gaussian 1 vs 0:
  Original Loss: 0.0027948
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0027948 (Pseudo: 0.00%)
[Iter 2050/20000] Loss: 0.0020627 (Best: 0.0014723 @iter1996) ([92m↓16.40%[0m) [0.82% of initial]
[Iter 2050] Gaussian 0 vs 1:
  Original Loss: 0.0017278
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0017278 (Pseudo: 0.00%)
[Iter 2050] Gaussian 1 vs 0:
  Original Loss: 0.0016704
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0016704 (Pseudo: 0.00%)
[Iter 2060/20000] Loss: 0.0017035 (Best: 0.0014723 @iter1996) ([92m↓17.42%[0m) [0.68% of initial]
[Iter 2060] Gaussian 0 vs 1:
  Original Loss: 0.0015296
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0015296 (Pseudo: 0.00%)
[Iter 2060] Gaussian 1 vs 0:
  Original Loss: 0.0015117
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0015117 (Pseudo: 0.00%)
[Iter 2070/20000] Loss: 0.0019892 (Best: 0.0014723 @iter1996) ([91m↑16.77%[0m) [0.79% of initial]
[Iter 2070] Gaussian 0 vs 1:
  Original Loss: 0.0022155
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0022155 (Pseudo: 0.00%)
[Iter 2070] Gaussian 1 vs 0:
  Original Loss: 0.0022722
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0022722 (Pseudo: 0.00%)
[Iter 2080/20000] Loss: 0.0018927 (Best: 0.0014723 @iter1996) ([92m↓4.85%[0m) [0.75% of initial]
[Iter 2080] Gaussian 0 vs 1:
  Original Loss: 0.0021175
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0021175 (Pseudo: 0.00%)
[Iter 2080] Gaussian 1 vs 0:
  Original Loss: 0.0021208
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0021208 (Pseudo: 0.00%)
[Iter 2090/20000] Loss: 0.0018186 (Best: 0.0013622 @iter2089) ([92m↓3.91%[0m) [0.72% of initial]
[Iter 2090] Gaussian 0 vs 1:
  Original Loss: 0.0020783
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0020783 (Pseudo: 0.00%)
[Iter 2090] Gaussian 1 vs 0:
  Original Loss: 0.0021429
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0021429 (Pseudo: 0.00%)
Iter:2099, L1 loss=0.00155, Total loss=0.001656, Time:15
[Iter 2100/20000] Loss: 0.0016712 (Best: 0.0013622 @iter2089) ([92m↓8.10%[0m) [0.66% of initial]
[Iter 2100] Gaussian 0 vs 1:
  Original Loss: 0.0015762
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0015762 (Pseudo: 0.00%)
[Iter 2100] Gaussian 1 vs 0:
  Original Loss: 0.0015346
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0015346 (Pseudo: 0.00%)
[Iter 2110/20000] Loss: 0.0015717 (Best: 0.0013622 @iter2089) ([92m↓5.96%[0m) [0.62% of initial]
[Iter 2110] Gaussian 0 vs 1:
  Original Loss: 0.0014118
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0014118 (Pseudo: 0.00%)
[Iter 2110] Gaussian 1 vs 0:
  Original Loss: 0.0014029
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0014029 (Pseudo: 0.00%)
[Iter 2120/20000] Loss: 0.0014321 (Best: 0.0013046 @iter2120) ([92m↓8.88%[0m) [0.57% of initial]
[Iter 2120] Gaussian 0 vs 1:
  Original Loss: 0.0013046
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0013046 (Pseudo: 0.00%)
[Iter 2120] Gaussian 1 vs 0:
  Original Loss: 0.0012475
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0012475 (Pseudo: 0.00%)
[Iter 2130/20000] Loss: 0.0015624 (Best: 0.0012257 @iter2125) ([91m↑9.09%[0m) [0.62% of initial]
[Iter 2130] Gaussian 0 vs 1:
  Original Loss: 0.0015977
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0015977 (Pseudo: 0.00%)
[Iter 2130] Gaussian 1 vs 0:
  Original Loss: 0.0014893
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0014893 (Pseudo: 0.00%)
[Iter 2140/20000] Loss: 0.0017188 (Best: 0.0012257 @iter2125) ([91m↑10.01%[0m) [0.68% of initial]
[Iter 2140] Gaussian 0 vs 1:
  Original Loss: 0.0018861
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0018861 (Pseudo: 0.00%)
[Iter 2140] Gaussian 1 vs 0:
  Original Loss: 0.0018611
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0018611 (Pseudo: 0.00%)
[Iter 2150/20000] Loss: 0.0017398 (Best: 0.0012257 @iter2125) ([91m↑1.22%[0m) [0.69% of initial]
[Iter 2150] Gaussian 0 vs 1:
  Original Loss: 0.0014850
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0014850 (Pseudo: 0.00%)
[Iter 2150] Gaussian 1 vs 0:
  Original Loss: 0.0014952
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0014952 (Pseudo: 0.00%)
[Iter 2160/20000] Loss: 0.0015560 (Best: 0.0012257 @iter2125) ([92m↓10.56%[0m) [0.62% of initial]
[Iter 2160] Gaussian 0 vs 1:
  Original Loss: 0.0013665
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0013665 (Pseudo: 0.00%)
[Iter 2160] Gaussian 1 vs 0:
  Original Loss: 0.0013760
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0013760 (Pseudo: 0.00%)
[Iter 2170/20000] Loss: 0.0016294 (Best: 0.0012257 @iter2125) ([91m↑4.71%[0m) [0.65% of initial]
[Iter 2170] Gaussian 0 vs 1:
  Original Loss: 0.0017304
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0017304 (Pseudo: 0.00%)
[Iter 2170] Gaussian 1 vs 0:
  Original Loss: 0.0017111
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0017111 (Pseudo: 0.00%)
[Iter 2180/20000] Loss: 0.0013234 (Best: 0.0012106 @iter2180) ([92m↓18.78%[0m) [0.53% of initial]
[Iter 2180] Gaussian 0 vs 1:
  Original Loss: 0.0012106
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0012106 (Pseudo: 0.00%)
[Iter 2180] Gaussian 1 vs 0:
  Original Loss: 0.0011913
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0011913 (Pseudo: 0.00%)
[Iter 2190/20000] Loss: 0.0016171 (Best: 0.0012106 @iter2180) ([91m↑22.20%[0m) [0.64% of initial]
[Iter 2190] Gaussian 0 vs 1:
  Original Loss: 0.0019078
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0019078 (Pseudo: 0.00%)
[Iter 2190] Gaussian 1 vs 0:
  Original Loss: 0.0019355
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0019355 (Pseudo: 0.00%)
Iter:2199, L1 loss=0.001395, Total loss=0.001526, Time:16
[Iter 2200/20000] Loss: 0.0016219 (Best: 0.0012106 @iter2180) ([91m↑0.30%[0m) [0.64% of initial]
[Iter 2200] Gaussian 0 vs 1:
  Original Loss: 0.0017914
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0017914 (Pseudo: 0.00%)
[Iter 2200] Gaussian 1 vs 0:
  Original Loss: 0.0018305
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0018305 (Pseudo: 0.00%)
[Iter 2210/20000] Loss: 0.0077354 (Best: 0.0012106 @iter2180) ([91m↑376.92%[0m) [3.07% of initial]
[Iter 2210] Gaussian 0 vs 1:
  Original Loss: 0.0075031
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0075031 (Pseudo: 0.00%)
[Iter 2210] Gaussian 1 vs 0:
  Original Loss: 0.0072677
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0072677 (Pseudo: 0.00%)
[Iter 2220/20000] Loss: 0.0042164 (Best: 0.0012106 @iter2180) ([92m↓45.49%[0m) [1.68% of initial]
[Iter 2220] Gaussian 0 vs 1:
  Original Loss: 0.0037695
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0037695 (Pseudo: 0.00%)
[Iter 2220] Gaussian 1 vs 0:
  Original Loss: 0.0037385
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0037385 (Pseudo: 0.00%)
[Iter 2230/20000] Loss: 0.0026705 (Best: 0.0012106 @iter2180) ([92m↓36.66%[0m) [1.06% of initial]
[Iter 2230] Gaussian 0 vs 1:
  Original Loss: 0.0022750
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0022750 (Pseudo: 0.00%)
[Iter 2230] Gaussian 1 vs 0:
  Original Loss: 0.0022484
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0022484 (Pseudo: 0.00%)
[Iter 2240/20000] Loss: 0.0023042 (Best: 0.0012106 @iter2180) ([92m↓13.72%[0m) [0.92% of initial]
[Iter 2240] Gaussian 0 vs 1:
  Original Loss: 0.0021141
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0021141 (Pseudo: 0.00%)
[Iter 2240] Gaussian 1 vs 0:
  Original Loss: 0.0020571
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0020571 (Pseudo: 0.00%)
[Iter 2250/20000] Loss: 0.0021345 (Best: 0.0012106 @iter2180) ([92m↓7.37%[0m) [0.85% of initial]
[Iter 2250] Gaussian 0 vs 1:
  Original Loss: 0.0023109
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0023109 (Pseudo: 0.00%)
[Iter 2250] Gaussian 1 vs 0:
  Original Loss: 0.0022848
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0022848 (Pseudo: 0.00%)
[Iter 2260/20000] Loss: 0.0016966 (Best: 0.0012106 @iter2180) ([92m↓20.51%[0m) [0.67% of initial]
[Iter 2260] Gaussian 0 vs 1:
  Original Loss: 0.0015034
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0015034 (Pseudo: 0.00%)
[Iter 2260] Gaussian 1 vs 0:
  Original Loss: 0.0014815
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0014815 (Pseudo: 0.00%)
[Iter 2270/20000] Loss: 0.0017942 (Best: 0.0012106 @iter2180) ([91m↑5.75%[0m) [0.71% of initial]
[Iter 2270] Gaussian 0 vs 1:
  Original Loss: 0.0020060
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0020060 (Pseudo: 0.00%)
[Iter 2270] Gaussian 1 vs 0:
  Original Loss: 0.0019895
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0019895 (Pseudo: 0.00%)
[Iter 2280/20000] Loss: 0.0014482 (Best: 0.0012106 @iter2180) ([92m↓19.28%[0m) [0.58% of initial]
[Iter 2280] Gaussian 0 vs 1:
  Original Loss: 0.0013402
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0013402 (Pseudo: 0.00%)
[Iter 2280] Gaussian 1 vs 0:
  Original Loss: 0.0013338
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0013338 (Pseudo: 0.00%)
[Iter 2290/20000] Loss: 0.0013923 (Best: 0.0011823 @iter2287) ([92m↓3.86%[0m) [0.55% of initial]
[Iter 2290] Gaussian 0 vs 1:
  Original Loss: 0.0012570
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0012570 (Pseudo: 0.00%)
[Iter 2290] Gaussian 1 vs 0:
  Original Loss: 0.0012375
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0012375 (Pseudo: 0.00%)
Iter:2299, L1 loss=0.001392, Total loss=0.0014, Time:17
[Iter 2300/20000] Loss: 0.0016849 (Best: 0.0011823 @iter2287) ([91m↑21.02%[0m) [0.67% of initial]
[Iter 2300] Gaussian 0 vs 1:
  Original Loss: 0.0018119
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0018119 (Pseudo: 0.00%)
[Iter 2300] Gaussian 1 vs 0:
  Original Loss: 0.0018945
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0018945 (Pseudo: 0.00%)
[Iter 2310/20000] Loss: 0.0015445 (Best: 0.0011823 @iter2287) ([92m↓8.34%[0m) [0.61% of initial]
[Iter 2310] Gaussian 0 vs 1:
  Original Loss: 0.0015288
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0015288 (Pseudo: 0.00%)
[Iter 2310] Gaussian 1 vs 0:
  Original Loss: 0.0014970
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0014970 (Pseudo: 0.00%)
[Iter 2320/20000] Loss: 0.0013208 (Best: 0.0011823 @iter2287) ([92m↓14.48%[0m) [0.52% of initial]
[Iter 2320] Gaussian 0 vs 1:
  Original Loss: 0.0011849
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0011849 (Pseudo: 0.00%)
[Iter 2320] Gaussian 1 vs 0:
  Original Loss: 0.0012192
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0012192 (Pseudo: 0.00%)
[Iter 2330/20000] Loss: 0.0013487 (Best: 0.0011553 @iter2327) ([91m↑2.11%[0m) [0.54% of initial]
[Iter 2330] Gaussian 0 vs 1:
  Original Loss: 0.0013315
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0013315 (Pseudo: 0.00%)
[Iter 2330] Gaussian 1 vs 0:
  Original Loss: 0.0012790
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0012790 (Pseudo: 0.00%)
[Iter 2340/20000] Loss: 0.0013611 (Best: 0.0010960 @iter2338) ([91m↑0.92%[0m) [0.54% of initial]
[Iter 2340] Gaussian 0 vs 1:
  Original Loss: 0.0012546
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0012546 (Pseudo: 0.00%)
[Iter 2340] Gaussian 1 vs 0:
  Original Loss: 0.0012285
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0012285 (Pseudo: 0.00%)
[Iter 2350/20000] Loss: 0.0015030 (Best: 0.0010960 @iter2338) ([91m↑10.42%[0m) [0.60% of initial]
[Iter 2350] Gaussian 0 vs 1:
  Original Loss: 0.0014497
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0014497 (Pseudo: 0.00%)
[Iter 2350] Gaussian 1 vs 0:
  Original Loss: 0.0014125
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0014125 (Pseudo: 0.00%)
[Iter 2360/20000] Loss: 0.0012858 (Best: 0.0010960 @iter2338) ([92m↓14.45%[0m) [0.51% of initial]
[Iter 2360] Gaussian 0 vs 1:
  Original Loss: 0.0013139
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0013139 (Pseudo: 0.00%)
[Iter 2360] Gaussian 1 vs 0:
  Original Loss: 0.0012944
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0012944 (Pseudo: 0.00%)
[Iter 2370/20000] Loss: 0.0014347 (Best: 0.0010960 @iter2338) ([91m↑11.58%[0m) [0.57% of initial]
[Iter 2370] Gaussian 0 vs 1:
  Original Loss: 0.0012622
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0012622 (Pseudo: 0.00%)
[Iter 2370] Gaussian 1 vs 0:
  Original Loss: 0.0011911
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0011911 (Pseudo: 0.00%)
[Iter 2380/20000] Loss: 0.0014792 (Best: 0.0010960 @iter2338) ([91m↑3.10%[0m) [0.59% of initial]
[Iter 2380] Gaussian 0 vs 1:
  Original Loss: 0.0016159
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0016159 (Pseudo: 0.00%)
[Iter 2380] Gaussian 1 vs 0:
  Original Loss: 0.0016105
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0016105 (Pseudo: 0.00%)
[Iter 2390/20000] Loss: 0.0016021 (Best: 0.0010960 @iter2338) ([91m↑8.30%[0m) [0.64% of initial]
[Iter 2390] Gaussian 0 vs 1:
  Original Loss: 0.0018050
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0018050 (Pseudo: 0.00%)
[Iter 2390] Gaussian 1 vs 0:
  Original Loss: 0.0017708
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0017708 (Pseudo: 0.00%)
Iter:2399, L1 loss=0.001198, Total loss=0.001263, Time:17
[Iter 2400/20000] Loss: 0.0013135 (Best: 0.0010960 @iter2338) ([92m↓18.01%[0m) [0.52% of initial]
[Iter 2400] Gaussian 0 vs 1:
  Original Loss: 0.0011973
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0011973 (Pseudo: 0.00%)
[Iter 2400] Gaussian 1 vs 0:
  Original Loss: 0.0013144
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0013144 (Pseudo: 0.00%)
[Iter 2410/20000] Loss: 0.0061524 (Best: 0.0010960 @iter2338) ([91m↑368.39%[0m) [2.44% of initial]
[Iter 2410] Gaussian 0 vs 1:
  Original Loss: 0.0052609
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0052609 (Pseudo: 0.00%)
[Iter 2410] Gaussian 1 vs 0:
  Original Loss: 0.0049266
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0049266 (Pseudo: 0.00%)
[Iter 2420/20000] Loss: 0.0035401 (Best: 0.0010960 @iter2338) ([92m↓42.46%[0m) [1.41% of initial]
[Iter 2420] Gaussian 0 vs 1:
  Original Loss: 0.0029951
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0029951 (Pseudo: 0.00%)
[Iter 2420] Gaussian 1 vs 0:
  Original Loss: 0.0028760
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0028760 (Pseudo: 0.00%)
[Iter 2430/20000] Loss: 0.0025813 (Best: 0.0010960 @iter2338) ([92m↓27.08%[0m) [1.03% of initial]
[Iter 2430] Gaussian 0 vs 1:
  Original Loss: 0.0026477
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0026477 (Pseudo: 0.00%)
[Iter 2430] Gaussian 1 vs 0:
  Original Loss: 0.0028132
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0028132 (Pseudo: 0.00%)
[Iter 2440/20000] Loss: 0.0020488 (Best: 0.0010960 @iter2338) ([92m↓20.63%[0m) [0.81% of initial]
[Iter 2440] Gaussian 0 vs 1:
  Original Loss: 0.0017777
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0017777 (Pseudo: 0.00%)
[Iter 2440] Gaussian 1 vs 0:
  Original Loss: 0.0017036
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0017036 (Pseudo: 0.00%)
[Iter 2450/20000] Loss: 0.0019562 (Best: 0.0010960 @iter2338) ([92m↓4.52%[0m) [0.78% of initial]
[Iter 2450] Gaussian 0 vs 1:
  Original Loss: 0.0020311
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0020311 (Pseudo: 0.00%)
[Iter 2450] Gaussian 1 vs 0:
  Original Loss: 0.0021307
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0021307 (Pseudo: 0.00%)
[Iter 2460/20000] Loss: 0.0016487 (Best: 0.0010960 @iter2338) ([92m↓15.72%[0m) [0.66% of initial]
[Iter 2460] Gaussian 0 vs 1:
  Original Loss: 0.0014915
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0014915 (Pseudo: 0.00%)
[Iter 2460] Gaussian 1 vs 0:
  Original Loss: 0.0015004
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0015004 (Pseudo: 0.00%)
[Iter 2470/20000] Loss: 0.0016327 (Best: 0.0010960 @iter2338) ([92m↓0.97%[0m) [0.65% of initial]
[Iter 2470] Gaussian 0 vs 1:
  Original Loss: 0.0016538
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0016538 (Pseudo: 0.00%)
[Iter 2470] Gaussian 1 vs 0:
  Original Loss: 0.0017107
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0017107 (Pseudo: 0.00%)
[Iter 2480/20000] Loss: 0.0016395 (Best: 0.0010960 @iter2338) ([91m↑0.41%[0m) [0.65% of initial]
[Iter 2480] Gaussian 0 vs 1:
  Original Loss: 0.0014431
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0014431 (Pseudo: 0.00%)
[Iter 2480] Gaussian 1 vs 0:
  Original Loss: 0.0014340
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0014340 (Pseudo: 0.00%)
[Iter 2490/20000] Loss: 0.0014592 (Best: 0.0010960 @iter2338) ([92m↓10.99%[0m) [0.58% of initial]
[Iter 2490] Gaussian 0 vs 1:
  Original Loss: 0.0014358
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0014358 (Pseudo: 0.00%)
[Iter 2490] Gaussian 1 vs 0:
  Original Loss: 0.0014134
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0014134 (Pseudo: 0.00%)
Iter:2499, L1 loss=0.001231, Total loss=0.001227, Time:17
[Iter 2500/20000] Loss: 0.0013106 (Best: 0.0010960 @iter2338) ([92m↓10.19%[0m) [0.52% of initial]
[Iter 2500] Gaussian 0 vs 1:
  Original Loss: 0.0012209
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0012209 (Pseudo: 0.00%)
[Iter 2500] Gaussian 1 vs 0:
  Original Loss: 0.0012528
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0012528 (Pseudo: 0.00%)
[Iter 2510/20000] Loss: 0.0013585 (Best: 0.0010301 @iter2504) ([91m↑3.66%[0m) [0.54% of initial]
[Iter 2510] Gaussian 0 vs 1:
  Original Loss: 0.0015743
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0015743 (Pseudo: 0.00%)
[Iter 2510] Gaussian 1 vs 0:
  Original Loss: 0.0016330
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0016330 (Pseudo: 0.00%)
[Iter 2520/20000] Loss: 0.0012036 (Best: 0.0010111 @iter2519) ([92m↓11.40%[0m) [0.48% of initial]
[Iter 2520] Gaussian 0 vs 1:
  Original Loss: 0.0012162
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0012162 (Pseudo: 0.00%)
[Iter 2520] Gaussian 1 vs 0:
  Original Loss: 0.0012883
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0012883 (Pseudo: 0.00%)
[Iter 2530/20000] Loss: 0.0010594 (Best: 0.0009340 @iter2528) ([92m↓11.98%[0m) [0.42% of initial]
[Iter 2530] Gaussian 0 vs 1:
  Original Loss: 0.0009551
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0009551 (Pseudo: 0.00%)
[Iter 2530] Gaussian 1 vs 0:
  Original Loss: 0.0009538
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0009538 (Pseudo: 0.00%)
[Iter 2540/20000] Loss: 0.0011869 (Best: 0.0009340 @iter2528) ([91m↑12.03%[0m) [0.47% of initial]
[Iter 2540] Gaussian 0 vs 1:
  Original Loss: 0.0011876
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0011876 (Pseudo: 0.00%)
[Iter 2540] Gaussian 1 vs 0:
  Original Loss: 0.0011839
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0011839 (Pseudo: 0.00%)
[Iter 2550/20000] Loss: 0.0013671 (Best: 0.0009340 @iter2528) ([91m↑15.19%[0m) [0.54% of initial]
[Iter 2550] Gaussian 0 vs 1:
  Original Loss: 0.0014048
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0014048 (Pseudo: 0.00%)
[Iter 2550] Gaussian 1 vs 0:
  Original Loss: 0.0014457
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0014457 (Pseudo: 0.00%)
[Iter 2560/20000] Loss: 0.0011793 (Best: 0.0009340 @iter2528) ([92m↓13.74%[0m) [0.47% of initial]
[Iter 2560] Gaussian 0 vs 1:
  Original Loss: 0.0010764
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0010764 (Pseudo: 0.00%)
[Iter 2560] Gaussian 1 vs 0:
  Original Loss: 0.0010463
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0010463 (Pseudo: 0.00%)
[Iter 2570/20000] Loss: 0.0013915 (Best: 0.0009340 @iter2528) ([91m↑17.99%[0m) [0.55% of initial]
[Iter 2570] Gaussian 0 vs 1:
  Original Loss: 0.0013948
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0013948 (Pseudo: 0.00%)
[Iter 2570] Gaussian 1 vs 0:
  Original Loss: 0.0013855
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0013855 (Pseudo: 0.00%)
[Iter 2580/20000] Loss: 0.0012150 (Best: 0.0009128 @iter2578) ([92m↓12.69%[0m) [0.48% of initial]
[Iter 2580] Gaussian 0 vs 1:
  Original Loss: 0.0012350
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0012350 (Pseudo: 0.00%)
[Iter 2580] Gaussian 1 vs 0:
  Original Loss: 0.0012421
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0012421 (Pseudo: 0.00%)
[Iter 2590/20000] Loss: 0.0012754 (Best: 0.0009102 @iter2584) ([91m↑4.97%[0m) [0.51% of initial]
[Iter 2590] Gaussian 0 vs 1:
  Original Loss: 0.0013607
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0013607 (Pseudo: 0.00%)
[Iter 2590] Gaussian 1 vs 0:
  Original Loss: 0.0014092
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0014092 (Pseudo: 0.00%)
Iter:2599, L1 loss=0.001014, Total loss=0.001012, Time:17
[Iter 2600/20000] Loss: 0.0011904 (Best: 0.0009102 @iter2584) ([92m↓6.67%[0m) [0.47% of initial]
[Iter 2600] Gaussian 0 vs 1:
  Original Loss: 0.0013056
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0013056 (Pseudo: 0.00%)
[Iter 2600] Gaussian 1 vs 0:
  Original Loss: 0.0013334
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0013334 (Pseudo: 0.00%)
[Iter 2610/20000] Loss: 0.0060108 (Best: 0.0009102 @iter2584) ([91m↑404.96%[0m) [2.39% of initial]
[Iter 2610] Gaussian 0 vs 1:
  Original Loss: 0.0054474
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0054474 (Pseudo: 0.00%)
[Iter 2610] Gaussian 1 vs 0:
  Original Loss: 0.0052212
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0052212 (Pseudo: 0.00%)
[Iter 2620/20000] Loss: 0.0032401 (Best: 0.0009102 @iter2584) ([92m↓46.10%[0m) [1.29% of initial]
[Iter 2620] Gaussian 0 vs 1:
  Original Loss: 0.0028164
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0028164 (Pseudo: 0.00%)
[Iter 2620] Gaussian 1 vs 0:
  Original Loss: 0.0027265
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0027265 (Pseudo: 0.00%)
[Iter 2630/20000] Loss: 0.0021727 (Best: 0.0009102 @iter2584) ([92m↓32.94%[0m) [0.86% of initial]
[Iter 2630] Gaussian 0 vs 1:
  Original Loss: 0.0021922
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0021922 (Pseudo: 0.00%)
[Iter 2630] Gaussian 1 vs 0:
  Original Loss: 0.0021198
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0021198 (Pseudo: 0.00%)
[Iter 2640/20000] Loss: 0.0017086 (Best: 0.0009102 @iter2584) ([92m↓21.36%[0m) [0.68% of initial]
[Iter 2640] Gaussian 0 vs 1:
  Original Loss: 0.0015578
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0015578 (Pseudo: 0.00%)
[Iter 2640] Gaussian 1 vs 0:
  Original Loss: 0.0015522
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0015522 (Pseudo: 0.00%)
[Iter 2650/20000] Loss: 0.0014132 (Best: 0.0009102 @iter2584) ([92m↓17.29%[0m) [0.56% of initial]
[Iter 2650] Gaussian 0 vs 1:
  Original Loss: 0.0011457
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0011457 (Pseudo: 0.00%)
[Iter 2650] Gaussian 1 vs 0:
  Original Loss: 0.0011131
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0011131 (Pseudo: 0.00%)
[Iter 2660/20000] Loss: 0.0016634 (Best: 0.0009102 @iter2584) ([91m↑17.70%[0m) [0.66% of initial]
[Iter 2660] Gaussian 0 vs 1:
  Original Loss: 0.0014413
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0014413 (Pseudo: 0.00%)
[Iter 2660] Gaussian 1 vs 0:
  Original Loss: 0.0014032
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0014032 (Pseudo: 0.00%)
[Iter 2670/20000] Loss: 0.0016491 (Best: 0.0009102 @iter2584) ([92m↓0.86%[0m) [0.66% of initial]
[Iter 2670] Gaussian 0 vs 1:
  Original Loss: 0.0017100
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0017100 (Pseudo: 0.00%)
[Iter 2670] Gaussian 1 vs 0:
  Original Loss: 0.0016796
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0016796 (Pseudo: 0.00%)
[Iter 2680/20000] Loss: 0.0012779 (Best: 0.0009102 @iter2584) ([92m↓22.51%[0m) [0.51% of initial]
[Iter 2680] Gaussian 0 vs 1:
  Original Loss: 0.0010734
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0010734 (Pseudo: 0.00%)
[Iter 2680] Gaussian 1 vs 0:
  Original Loss: 0.0010524
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0010524 (Pseudo: 0.00%)
[Iter 2690/20000] Loss: 0.0012013 (Best: 0.0009102 @iter2584) ([92m↓5.99%[0m) [0.48% of initial]
[Iter 2690] Gaussian 0 vs 1:
  Original Loss: 0.0010383
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0010383 (Pseudo: 0.00%)
[Iter 2690] Gaussian 1 vs 0:
  Original Loss: 0.0009678
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0009678 (Pseudo: 0.00%)
Iter:2699, L1 loss=0.001111, Total loss=0.001177, Time:18
[Iter 2700/20000] Loss: 0.0014546 (Best: 0.0009102 @iter2584) ([91m↑21.09%[0m) [0.58% of initial]
[Iter 2700] Gaussian 0 vs 1:
  Original Loss: 0.0017281
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0017281 (Pseudo: 0.00%)
[Iter 2700] Gaussian 1 vs 0:
  Original Loss: 0.0017053
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0017053 (Pseudo: 0.00%)
[Iter 2710/20000] Loss: 0.0012435 (Best: 0.0009102 @iter2584) ([92m↓14.51%[0m) [0.49% of initial]
[Iter 2710] Gaussian 0 vs 1:
  Original Loss: 0.0012896
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0012896 (Pseudo: 0.00%)
[Iter 2710] Gaussian 1 vs 0:
  Original Loss: 0.0012699
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0012699 (Pseudo: 0.00%)
[Iter 2720/20000] Loss: 0.0010938 (Best: 0.0009102 @iter2584) ([92m↓12.04%[0m) [0.43% of initial]
[Iter 2720] Gaussian 0 vs 1:
  Original Loss: 0.0010369
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0010369 (Pseudo: 0.00%)
[Iter 2720] Gaussian 1 vs 0:
  Original Loss: 0.0010570
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0010570 (Pseudo: 0.00%)
[Iter 2730/20000] Loss: 0.0009833 (Best: 0.0008248 @iter2725) ([92m↓10.11%[0m) [0.39% of initial]
[Iter 2730] Gaussian 0 vs 1:
  Original Loss: 0.0009075
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0009075 (Pseudo: 0.00%)
[Iter 2730] Gaussian 1 vs 0:
  Original Loss: 0.0009161
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0009161 (Pseudo: 0.00%)
[Iter 2740/20000] Loss: 0.0008567 (Best: 0.0007618 @iter2740) ([92m↓12.87%[0m) [0.34% of initial]
[Iter 2740] Gaussian 0 vs 1:
  Original Loss: 0.0007618
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0007618 (Pseudo: 0.00%)
[Iter 2740] Gaussian 1 vs 0:
  Original Loss: 0.0007701
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0007701 (Pseudo: 0.00%)
[Iter 2750/20000] Loss: 0.0011315 (Best: 0.0007618 @iter2740) ([91m↑32.07%[0m) [0.45% of initial]
[Iter 2750] Gaussian 0 vs 1:
  Original Loss: 0.0013268
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0013268 (Pseudo: 0.00%)
[Iter 2750] Gaussian 1 vs 0:
  Original Loss: 0.0013694
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0013694 (Pseudo: 0.00%)
[Iter 2760/20000] Loss: 0.0011649 (Best: 0.0007618 @iter2740) ([91m↑2.96%[0m) [0.46% of initial]
[Iter 2760] Gaussian 0 vs 1:
  Original Loss: 0.0012803
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0012803 (Pseudo: 0.00%)
[Iter 2760] Gaussian 1 vs 0:
  Original Loss: 0.0014127
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0014127 (Pseudo: 0.00%)
[Iter 2770/20000] Loss: 0.0013065 (Best: 0.0007618 @iter2740) ([91m↑12.15%[0m) [0.52% of initial]
[Iter 2770] Gaussian 0 vs 1:
  Original Loss: 0.0012820
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0012820 (Pseudo: 0.00%)
[Iter 2770] Gaussian 1 vs 0:
  Original Loss: 0.0013043
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0013043 (Pseudo: 0.00%)
[Iter 2780/20000] Loss: 0.0010920 (Best: 0.0007618 @iter2740) ([92m↓16.41%[0m) [0.43% of initial]
[Iter 2780] Gaussian 0 vs 1:
  Original Loss: 0.0012728
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0012728 (Pseudo: 0.00%)
[Iter 2780] Gaussian 1 vs 0:
  Original Loss: 0.0012630
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0012630 (Pseudo: 0.00%)
[Iter 2790/20000] Loss: 0.0011261 (Best: 0.0007618 @iter2740) ([91m↑3.12%[0m) [0.45% of initial]
[Iter 2790] Gaussian 0 vs 1:
  Original Loss: 0.0011587
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0011587 (Pseudo: 0.00%)
[Iter 2790] Gaussian 1 vs 0:
  Original Loss: 0.0011701
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0011701 (Pseudo: 0.00%)
Iter:2799, L1 loss=0.001261, Total loss=0.001296, Time:17
[Iter 2800/20000] Loss: 0.0011152 (Best: 0.0007618 @iter2740) ([92m↓0.97%[0m) [0.44% of initial]
[Iter 2800] Gaussian 0 vs 1:
  Original Loss: 0.0009635
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0009635 (Pseudo: 0.00%)
[Iter 2800] Gaussian 1 vs 0:
  Original Loss: 0.0009860
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0009860 (Pseudo: 0.00%)
[Iter 2810/20000] Loss: 0.0049535 (Best: 0.0007618 @iter2740) ([91m↑344.18%[0m) [1.97% of initial]
[Iter 2810] Gaussian 0 vs 1:
  Original Loss: 0.0040058
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0040058 (Pseudo: 0.00%)
[Iter 2810] Gaussian 1 vs 0:
  Original Loss: 0.0039270
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0039270 (Pseudo: 0.00%)
[Iter 2820/20000] Loss: 0.0026826 (Best: 0.0007618 @iter2740) ([92m↓45.84%[0m) [1.07% of initial]
[Iter 2820] Gaussian 0 vs 1:
  Original Loss: 0.0024844
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0024844 (Pseudo: 0.00%)
[Iter 2820] Gaussian 1 vs 0:
  Original Loss: 0.0024523
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0024523 (Pseudo: 0.00%)
[Iter 2830/20000] Loss: 0.0017814 (Best: 0.0007618 @iter2740) ([92m↓33.59%[0m) [0.71% of initial]
[Iter 2830] Gaussian 0 vs 1:
  Original Loss: 0.0015890
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0015890 (Pseudo: 0.00%)
[Iter 2830] Gaussian 1 vs 0:
  Original Loss: 0.0015108
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0015108 (Pseudo: 0.00%)
[Iter 2840/20000] Loss: 0.0015067 (Best: 0.0007618 @iter2740) ([92m↓15.42%[0m) [0.60% of initial]
[Iter 2840] Gaussian 0 vs 1:
  Original Loss: 0.0015821
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0015821 (Pseudo: 0.00%)
[Iter 2840] Gaussian 1 vs 0:
  Original Loss: 0.0015879
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0015879 (Pseudo: 0.00%)
[Iter 2850/20000] Loss: 0.0013013 (Best: 0.0007618 @iter2740) ([92m↓13.63%[0m) [0.52% of initial]
[Iter 2850] Gaussian 0 vs 1:
  Original Loss: 0.0012960
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0012960 (Pseudo: 0.00%)
[Iter 2850] Gaussian 1 vs 0:
  Original Loss: 0.0012764
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0012764 (Pseudo: 0.00%)
[Iter 2860/20000] Loss: 0.0014410 (Best: 0.0007618 @iter2740) ([91m↑10.73%[0m) [0.57% of initial]
[Iter 2860] Gaussian 0 vs 1:
  Original Loss: 0.0014827
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0014827 (Pseudo: 0.00%)
[Iter 2860] Gaussian 1 vs 0:
  Original Loss: 0.0014960
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0014960 (Pseudo: 0.00%)
[Iter 2870/20000] Loss: 0.0012020 (Best: 0.0007618 @iter2740) ([92m↓16.59%[0m) [0.48% of initial]
[Iter 2870] Gaussian 0 vs 1:
  Original Loss: 0.0011101
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0011101 (Pseudo: 0.00%)
[Iter 2870] Gaussian 1 vs 0:
  Original Loss: 0.0011716
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0011716 (Pseudo: 0.00%)
[Iter 2880/20000] Loss: 0.0011502 (Best: 0.0007618 @iter2740) ([92m↓4.31%[0m) [0.46% of initial]
[Iter 2880] Gaussian 0 vs 1:
  Original Loss: 0.0011625
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0011625 (Pseudo: 0.00%)
[Iter 2880] Gaussian 1 vs 0:
  Original Loss: 0.0011148
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0011148 (Pseudo: 0.00%)
[Iter 2890/20000] Loss: 0.0011142 (Best: 0.0007618 @iter2740) ([92m↓3.13%[0m) [0.44% of initial]
[Iter 2890] Gaussian 0 vs 1:
  Original Loss: 0.0011306
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0011306 (Pseudo: 0.00%)
[Iter 2890] Gaussian 1 vs 0:
  Original Loss: 0.0011609
  Raw Pseudo Loss: 0.0000000
  Scaled Pseudo Loss: 0.0000000 (scale: 1.000)
  Ratio to Original: 0.00%
  Total Loss: 0.0011609 (Pseudo: 0.00%)
Optimizing 
Training parameters: {'iterations': 20000, 'position_lr_init': 0.00019, 'position_lr_final': 1.9e-06, 'position_lr_delay_mult': 0.01, 'position_lr_max_steps': 20000, 'feature_lr': 0.002, 'opacity_lr': 0.008, 'scaling_lr': 0.005, 'rotation_lr': 0.001, 'percent_dense': 0.01, 'lambda_dssim': 0.2, 'densification_interval': 200, 'opacity_reset_interval': 4000, 'densify_from_iter': 500, 'densify_until_iter': 8000, 'densify_grad_threshold': 2.6e-05, 'random_background': False, 'sample_pseudo_interval': 1, 'start_sample_pseudo': 0, 'end_sample_pseudo': 30000}
Create gaussians1
GsDict.keys() is dict_keys(['gs0', 'gs1'])

Starting training...
Total iterations: 20000
==================================================
[Iter 10/20000] Loss: 0.2179127 (Best: 0.2126908 @iter10) [100.00% of initial]
[Iter 20/20000] Loss: 0.1746702 (Best: 0.1693033 @iter20) ([92m↓19.84%[0m) [69.39% of initial]
[Iter 30/20000] Loss: 0.1374909 (Best: 0.1327883 @iter30) ([92m↓21.29%[0m) [54.62% of initial]
[Iter 40/20000] Loss: 0.1123912 (Best: 0.1098365 @iter40) ([92m↓18.26%[0m) [44.65% of initial]
[Iter 50/20000] Loss: 0.0993451 (Best: 0.0965456 @iter49) ([92m↓11.61%[0m) [39.47% of initial]
[Iter 60/20000] Loss: 0.0936794 (Best: 0.0908578 @iter59) ([92m↓5.70%[0m) [37.22% of initial]
[Iter 70/20000] Loss: 0.0884512 (Best: 0.0869382 @iter70) ([92m↓5.58%[0m) [35.14% of initial]
[Iter 80/20000] Loss: 0.0851907 (Best: 0.0831084 @iter80) ([92m↓3.69%[0m) [33.85% of initial]
[Iter 90/20000] Loss: 0.0824118 (Best: 0.0801456 @iter88) ([92m↓3.26%[0m) [32.74% of initial]
Iter:99, L1 loss=0.05723, Total loss=0.07876, Time:21
[Iter 100/20000] Loss: 0.0786623 (Best: 0.0766275 @iter97) ([92m↓4.55%[0m) [31.25% of initial]
[Iter 110/20000] Loss: 0.0753225 (Best: 0.0731347 @iter106) ([92m↓4.25%[0m) [29.92% of initial]
[Iter 120/20000] Loss: 0.0714331 (Best: 0.0685495 @iter118) ([92m↓5.16%[0m) [28.38% of initial]
[Iter 130/20000] Loss: 0.0666932 (Best: 0.0641878 @iter130) ([92m↓6.64%[0m) [26.50% of initial]
[Iter 140/20000] Loss: 0.0635410 (Best: 0.0613000 @iter140) ([92m↓4.73%[0m) [25.24% of initial]
[Iter 150/20000] Loss: 0.0612668 (Best: 0.0583680 @iter148) ([92m↓3.58%[0m) [24.34% of initial]
[Iter 160/20000] Loss: 0.0590264 (Best: 0.0559365 @iter157) ([92m↓3.66%[0m) [23.45% of initial]
[Iter 170/20000] Loss: 0.0563525 (Best: 0.0535271 @iter167) ([92m↓4.53%[0m) [22.39% of initial]
[Iter 180/20000] Loss: 0.0522929 (Best: 0.0499450 @iter179) ([92m↓7.20%[0m) [20.78% of initial]
[Iter 190/20000] Loss: 0.0495094 (Best: 0.0477999 @iter188) ([92m↓5.32%[0m) [19.67% of initial]
Iter:199, L1 loss=0.03443, Total loss=0.04965, Time:16
[Iter 200/20000] Loss: 0.0477494 (Best: 0.0455610 @iter198) ([92m↓3.55%[0m) [18.97% of initial]
Optimizing 
Training parameters: {'iterations': 20000, 'position_lr_init': 0.00019, 'position_lr_final': 1.9e-06, 'position_lr_delay_mult': 0.01, 'position_lr_max_steps': 20000, 'feature_lr': 0.002, 'opacity_lr': 0.008, 'scaling_lr': 0.005, 'rotation_lr': 0.001, 'percent_dense': 0.01, 'lambda_dssim': 0.2, 'densification_interval': 200, 'opacity_reset_interval': 4000, 'densify_from_iter': 500, 'densify_until_iter': 8000, 'densify_grad_threshold': 2.6e-05, 'random_background': False, 'sample_pseudo_interval': 1, 'start_sample_pseudo': 0, 'end_sample_pseudo': 30000}
Create gaussians1
GsDict.keys() is dict_keys(['gs0', 'gs1'])

Starting training...
Total iterations: 20000
==================================================
[Iter 10/20000] Loss: 0.2179127 (Best: 0.2126908 @iter10) [100.00% of initial]
[Iter 20/20000] Loss: 0.1746701 (Best: 0.1693030 @iter20) ([92m↓19.84%[0m) [69.39% of initial]
[Iter 30/20000] Loss: 0.1374909 (Best: 0.1327888 @iter30) ([92m↓21.29%[0m) [54.62% of initial]
[Iter 40/20000] Loss: 0.1123924 (Best: 0.1098380 @iter40) ([92m↓18.25%[0m) [44.65% of initial]
[Iter 50/20000] Loss: 0.0993465 (Best: 0.0965469 @iter49) ([92m↓11.61%[0m) [39.47% of initial]
[Iter 60/20000] Loss: 0.0936745 (Best: 0.0908466 @iter59) ([92m↓5.71%[0m) [37.22% of initial]
[Iter 70/20000] Loss: 0.0884501 (Best: 0.0869346 @iter70) ([92m↓5.58%[0m) [35.14% of initial]
[Iter 80/20000] Loss: 0.0851832 (Best: 0.0830954 @iter80) ([92m↓3.69%[0m) [33.84% of initial]
[Iter 90/20000] Loss: 0.0824056 (Best: 0.0801445 @iter88) ([92m↓3.26%[0m) [32.74% of initial]
Iter:99, L1 loss=0.05724, Total loss=0.07876, Time:21
[Iter 100/20000] Loss: 0.0786614 (Best: 0.0766117 @iter97) ([92m↓4.54%[0m) [31.25% of initial]
[Iter 110/20000] Loss: 0.0753191 (Best: 0.0731338 @iter106) ([92m↓4.25%[0m) [29.92% of initial]
[Iter 120/20000] Loss: 0.0714308 (Best: 0.0685448 @iter118) ([92m↓5.16%[0m) [28.38% of initial]
[Iter 130/20000] Loss: 0.0667027 (Best: 0.0641905 @iter130) ([92m↓6.62%[0m) [26.50% of initial]
[Iter 140/20000] Loss: 0.0635355 (Best: 0.0612855 @iter140) ([92m↓4.75%[0m) [25.24% of initial]
[Iter 150/20000] Loss: 0.0612953 (Best: 0.0584232 @iter148) ([92m↓3.53%[0m) [24.35% of initial]
[Iter 160/20000] Loss: 0.0590676 (Best: 0.0559612 @iter157) ([92m↓3.63%[0m) [23.47% of initial]
[Iter 170/20000] Loss: 0.0563757 (Best: 0.0535411 @iter167) ([92m↓4.56%[0m) [22.40% of initial]
[Iter 180/20000] Loss: 0.0523700 (Best: 0.0500822 @iter179) ([92m↓7.11%[0m) [20.81% of initial]
[Iter 190/20000] Loss: 0.0495711 (Best: 0.0478573 @iter188) ([92m↓5.34%[0m) [19.69% of initial]
Iter:199, L1 loss=0.03448, Total loss=0.04979, Time:18
[Iter 200/20000] Loss: 0.0477961 (Best: 0.0457590 @iter198) ([92m↓3.58%[0m) [18.99% of initial]
[Iter 210/20000] Loss: 0.0450427 (Best: 0.0427748 @iter209) ([92m↓5.76%[0m) [17.90% of initial]
[Iter 220/20000] Loss: 0.0440523 (Best: 0.0411728 @iter219) ([92m↓2.20%[0m) [17.50% of initial]
[Iter 230/20000] Loss: 0.0423533 (Best: 0.0398691 @iter227) ([92m↓3.86%[0m) [16.83% of initial]
[Iter 240/20000] Loss: 0.0402689 (Best: 0.0379520 @iter238) ([92m↓4.92%[0m) [16.00% of initial]
[Iter 250/20000] Loss: 0.0380066 (Best: 0.0362114 @iter248) ([92m↓5.62%[0m) [15.10% of initial]
[Iter 260/20000] Loss: 0.0358424 (Best: 0.0342580 @iter260) ([92m↓5.69%[0m) [14.24% of initial]
[Iter 270/20000] Loss: 0.0349260 (Best: 0.0328095 @iter269) ([92m↓2.56%[0m) [13.88% of initial]
[Iter 280/20000] Loss: 0.0347145 (Best: 0.0317726 @iter277) ([92m↓0.61%[0m) [13.79% of initial]
[Iter 290/20000] Loss: 0.0330622 (Best: 0.0304665 @iter287) ([92m↓4.76%[0m) [13.14% of initial]
Iter:299, L1 loss=0.02216, Total loss=0.03338, Time:17
[Iter 300/20000] Loss: 0.0308207 (Best: 0.0289778 @iter300) ([92m↓6.78%[0m) [12.24% of initial]
[Iter 310/20000] Loss: 0.0293137 (Best: 0.0273058 @iter310) ([92m↓4.89%[0m) [11.65% of initial]
[Iter 320/20000] Loss: 0.0278540 (Best: 0.0263919 @iter320) ([92m↓4.98%[0m) [11.07% of initial]
[Iter 330/20000] Loss: 0.0275400 (Best: 0.0256397 @iter330) ([92m↓1.13%[0m) [10.94% of initial]
[Iter 340/20000] Loss: 0.0253861 (Best: 0.0242630 @iter340) ([92m↓7.82%[0m) [10.09% of initial]
[Iter 350/20000] Loss: 0.0261197 (Best: 0.0234571 @iter349) ([91m↑2.89%[0m) [10.38% of initial]
[Iter 360/20000] Loss: 0.0247774 (Best: 0.0228342 @iter358) ([92m↓5.14%[0m) [9.84% of initial]
[Iter 370/20000] Loss: 0.0245670 (Best: 0.0222024 @iter368) ([92m↓0.85%[0m) [9.76% of initial]
[Iter 380/20000] Loss: 0.0222958 (Best: 0.0211353 @iter379) ([92m↓9.24%[0m) [8.86% of initial]
[Iter 390/20000] Loss: 0.0217796 (Best: 0.0203159 @iter385) ([92m↓2.32%[0m) [8.65% of initial]
Iter:399, L1 loss=0.01353, Total loss=0.02133, Time:18
[Iter 400/20000] Loss: 0.0206350 (Best: 0.0192143 @iter400) ([92m↓5.26%[0m) [8.20% of initial]
[Iter 410/20000] Loss: 0.0194687 (Best: 0.0183569 @iter410) ([92m↓5.65%[0m) [7.73% of initial]
[Iter 420/20000] Loss: 0.0197469 (Best: 0.0177098 @iter418) ([91m↑1.43%[0m) [7.85% of initial]
[Iter 430/20000] Loss: 0.0179078 (Best: 0.0172186 @iter430) ([92m↓9.31%[0m) [7.11% of initial]
[Iter 440/20000] Loss: 0.0184554 (Best: 0.0167350 @iter438) ([91m↑3.06%[0m) [7.33% of initial]
[Iter 450/20000] Loss: 0.0174292 (Best: 0.0156436 @iter449) ([92m↓5.56%[0m) [6.92% of initial]
[Iter 460/20000] Loss: 0.0166013 (Best: 0.0149313 @iter458) ([92m↓4.75%[0m) [6.60% of initial]
[Iter 470/20000] Loss: 0.0151157 (Best: 0.0141822 @iter470) ([92m↓8.95%[0m) [6.01% of initial]
[Iter 480/20000] Loss: 0.0147565 (Best: 0.0132992 @iter479) ([92m↓2.38%[0m) [5.86% of initial]
[Iter 490/20000] Loss: 0.0137483 (Best: 0.0127463 @iter490) ([92m↓6.83%[0m) [5.46% of initial]
Iter:499, L1 loss=0.008123, Total loss=0.01425, Time:18
[Iter 500/20000] Loss: 0.0135548 (Best: 0.0126694 @iter493) ([92m↓1.41%[0m) [5.39% of initial]
[Iter 510/20000] Loss: 0.0135199 (Best: 0.0120787 @iter508) ([92m↓0.26%[0m) [5.37% of initial]
[Iter 520/20000] Loss: 0.0127953 (Best: 0.0117691 @iter511) ([92m↓5.36%[0m) [5.08% of initial]
[Iter 530/20000] Loss: 0.0123573 (Best: 0.0111778 @iter529) ([92m↓3.42%[0m) [4.91% of initial]
[Iter 540/20000] Loss: 0.0125711 (Best: 0.0111778 @iter529) ([91m↑1.73%[0m) [4.99% of initial]
[Iter 550/20000] Loss: 0.0120336 (Best: 0.0110438 @iter545) ([92m↓4.28%[0m) [4.78% of initial]
[Iter 560/20000] Loss: 0.0122784 (Best: 0.0106725 @iter556) ([91m↑2.03%[0m) [4.88% of initial]
[Iter 570/20000] Loss: 0.0118497 (Best: 0.0104502 @iter569) ([92m↓3.49%[0m) [4.71% of initial]
[Iter 580/20000] Loss: 0.0114676 (Best: 0.0104502 @iter569) ([92m↓3.22%[0m) [4.56% of initial]
[Iter 590/20000] Loss: 0.0113879 (Best: 0.0102013 @iter583) ([92m↓0.69%[0m) [4.52% of initial]
Iter:599, L1 loss=0.007168, Total loss=0.01179, Time:18
[Iter 600/20000] Loss: 0.0110137 (Best: 0.0101286 @iter591) ([92m↓3.29%[0m) [4.38% of initial]
[Iter 610/20000] Loss: 0.0213721 (Best: 0.0101286 @iter591) ([91m↑94.05%[0m) [8.49% of initial]
[Iter 620/20000] Loss: 0.0139540 (Best: 0.0101286 @iter591) ([92m↓34.71%[0m) [5.54% of initial]
[Iter 630/20000] Loss: 0.0117599 (Best: 0.0101286 @iter591) ([92m↓15.72%[0m) [4.67% of initial]
[Iter 640/20000] Loss: 0.0100886 (Best: 0.0090897 @iter640) ([92m↓14.21%[0m) [4.01% of initial]
[Iter 650/20000] Loss: 0.0105900 (Best: 0.0090224 @iter646) ([91m↑4.97%[0m) [4.21% of initial]
[Iter 660/20000] Loss: 0.0099560 (Best: 0.0086686 @iter655) ([92m↓5.99%[0m) [3.96% of initial]
[Iter 670/20000] Loss: 0.0095573 (Best: 0.0082746 @iter667) ([92m↓4.00%[0m) [3.80% of initial]
[Iter 680/20000] Loss: 0.0088036 (Best: 0.0081489 @iter680) ([92m↓7.89%[0m) [3.50% of initial]
[Iter 690/20000] Loss: 0.0090871 (Best: 0.0077659 @iter685) ([91m↑3.22%[0m) [3.61% of initial]
Iter:699, L1 loss=0.005697, Total loss=0.009633, Time:18
[Iter 700/20000] Loss: 0.0089681 (Best: 0.0077659 @iter685) ([92m↓1.31%[0m) [3.56% of initial]
[Iter 710/20000] Loss: 0.0083568 (Best: 0.0075818 @iter703) ([92m↓6.82%[0m) [3.32% of initial]
[Iter 720/20000] Loss: 0.0084263 (Best: 0.0075750 @iter715) ([91m↑0.83%[0m) [3.35% of initial]
[Iter 730/20000] Loss: 0.0084545 (Best: 0.0072810 @iter727) ([91m↑0.33%[0m) [3.36% of initial]
[Iter 740/20000] Loss: 0.0085990 (Best: 0.0071907 @iter733) ([91m↑1.71%[0m) [3.42% of initial]
[Iter 750/20000] Loss: 0.0081885 (Best: 0.0071907 @iter733) ([92m↓4.77%[0m) [3.25% of initial]
[Iter 760/20000] Loss: 0.0074114 (Best: 0.0069256 @iter754) ([92m↓9.49%[0m) [2.94% of initial]
[Iter 770/20000] Loss: 0.0075560 (Best: 0.0069256 @iter754) ([91m↑1.95%[0m) [3.00% of initial]
[Iter 780/20000] Loss: 0.0078130 (Best: 0.0067757 @iter778) ([91m↑3.40%[0m) [3.10% of initial]
[Iter 790/20000] Loss: 0.0075804 (Best: 0.0065905 @iter787) ([92m↓2.98%[0m) [3.01% of initial]
Iter:799, L1 loss=0.005006, Total loss=0.008092, Time:18
[Iter 800/20000] Loss: 0.0073647 (Best: 0.0065699 @iter794) ([92m↓2.85%[0m) [2.93% of initial]
[Iter 810/20000] Loss: 0.0154988 (Best: 0.0065699 @iter794) ([91m↑110.45%[0m) [6.16% of initial]
[Iter 820/20000] Loss: 0.0109214 (Best: 0.0065699 @iter794) ([92m↓29.53%[0m) [4.34% of initial]
[Iter 830/20000] Loss: 0.0090186 (Best: 0.0065699 @iter794) ([92m↓17.42%[0m) [3.58% of initial]
[Iter 840/20000] Loss: 0.0082094 (Best: 0.0065699 @iter794) ([92m↓8.97%[0m) [3.26% of initial]
[Iter 850/20000] Loss: 0.0074978 (Best: 0.0065699 @iter794) ([92m↓8.67%[0m) [2.98% of initial]
[Iter 860/20000] Loss: 0.0070423 (Best: 0.0062987 @iter859) ([92m↓6.07%[0m) [2.80% of initial]
[Iter 870/20000] Loss: 0.0066952 (Best: 0.0061127 @iter862) ([92m↓4.93%[0m) [2.66% of initial]
[Iter 880/20000] Loss: 0.0066766 (Best: 0.0059547 @iter875) ([92m↓0.28%[0m) [2.65% of initial]
[Iter 890/20000] Loss: 0.0062916 (Best: 0.0057352 @iter884) ([92m↓5.77%[0m) [2.50% of initial]
Iter:899, L1 loss=0.003697, Total loss=0.005645, Time:14
[Iter 900/20000] Loss: 0.0064825 (Best: 0.0056453 @iter899) ([91m↑3.03%[0m) [2.58% of initial]
[Iter 910/20000] Loss: 0.0066310 (Best: 0.0055385 @iter907) ([91m↑2.29%[0m) [2.63% of initial]
[Iter 920/20000] Loss: 0.0060062 (Best: 0.0054099 @iter913) ([92m↓9.42%[0m) [2.39% of initial]
[Iter 930/20000] Loss: 0.0062460 (Best: 0.0053537 @iter925) ([91m↑3.99%[0m) [2.48% of initial]
[Iter 940/20000] Loss: 0.0062910 (Best: 0.0052154 @iter938) ([91m↑0.72%[0m) [2.50% of initial]
[Iter 950/20000] Loss: 0.0058857 (Best: 0.0052154 @iter938) ([92m↓6.44%[0m) [2.34% of initial]
[Iter 960/20000] Loss: 0.0060818 (Best: 0.0052154 @iter938) ([91m↑3.33%[0m) [2.42% of initial]
[Iter 970/20000] Loss: 0.0059414 (Best: 0.0052101 @iter964) ([92m↓2.31%[0m) [2.36% of initial]
[Iter 980/20000] Loss: 0.0063186 (Best: 0.0052101 @iter964) ([91m↑6.35%[0m) [2.51% of initial]
[Iter 990/20000] Loss: 0.0061374 (Best: 0.0052078 @iter988) ([92m↓2.87%[0m) [2.44% of initial]
Iter:999, L1 loss=0.004446, Total loss=0.006789, Time:17
[Iter 1000/20000] Loss: 0.0064049 (Best: 0.0052078 @iter988) ([91m↑4.36%[0m) [2.54% of initial]
Pruning 997 points (7.2%) from gaussian0 at iteration 1000
Pruning 1024 points (7.5%) from gaussian1 at iteration 1000
[Iter 1010/20000] Loss: 0.0136676 (Best: 0.0052078 @iter988) ([91m↑113.39%[0m) [5.43% of initial]
[Iter 1020/20000] Loss: 0.0097405 (Best: 0.0052078 @iter988) ([92m↓28.73%[0m) [3.87% of initial]
[Iter 1030/20000] Loss: 0.0078874 (Best: 0.0052078 @iter988) ([92m↓19.02%[0m) [3.13% of initial]
[Iter 1040/20000] Loss: 0.0071045 (Best: 0.0052078 @iter988) ([92m↓9.93%[0m) [2.82% of initial]
[Iter 1050/20000] Loss: 0.0067992 (Best: 0.0052078 @iter988) ([92m↓4.30%[0m) [2.70% of initial]
[Iter 1060/20000] Loss: 0.0065283 (Best: 0.0052078 @iter988) ([92m↓3.98%[0m) [2.59% of initial]
[Iter 1070/20000] Loss: 0.0063595 (Best: 0.0052078 @iter988) ([92m↓2.59%[0m) [2.53% of initial]
[Iter 1080/20000] Loss: 0.0060951 (Best: 0.0052078 @iter988) ([92m↓4.16%[0m) [2.42% of initial]
[Iter 1090/20000] Loss: 0.0059933 (Best: 0.0052078 @iter988) ([92m↓1.67%[0m) [2.38% of initial]
Iter:1099, L1 loss=0.003805, Total loss=0.005927, Time:18
[Iter 1100/20000] Loss: 0.0057804 (Best: 0.0052078 @iter988) ([92m↓3.55%[0m) [2.30% of initial]
[Iter 1110/20000] Loss: 0.0058143 (Best: 0.0052078 @iter988) ([91m↑0.59%[0m) [2.31% of initial]
[Iter 1120/20000] Loss: 0.0057839 (Best: 0.0050055 @iter1117) ([92m↓0.52%[0m) [2.30% of initial]
[Iter 1130/20000] Loss: 0.0058946 (Best: 0.0050055 @iter1117) ([91m↑1.92%[0m) [2.34% of initial]
[Iter 1140/20000] Loss: 0.0055643 (Best: 0.0049841 @iter1135) ([92m↓5.60%[0m) [2.21% of initial]
[Iter 1150/20000] Loss: 0.0052458 (Best: 0.0048780 @iter1145) ([92m↓5.72%[0m) [2.08% of initial]
[Iter 1160/20000] Loss: 0.0057627 (Best: 0.0048325 @iter1156) ([91m↑9.85%[0m) [2.29% of initial]
[Iter 1170/20000] Loss: 0.0054423 (Best: 0.0048231 @iter1166) ([92m↓5.56%[0m) [2.16% of initial]
[Iter 1180/20000] Loss: 0.0050938 (Best: 0.0047607 @iter1180) ([92m↓6.40%[0m) [2.02% of initial]
[Iter 1190/20000] Loss: 0.0053946 (Best: 0.0047381 @iter1186) ([91m↑5.91%[0m) [2.14% of initial]
Iter:1199, L1 loss=0.00373, Total loss=0.005736, Time:17
[Iter 1200/20000] Loss: 0.0054311 (Best: 0.0047089 @iter1195) ([91m↑0.68%[0m) [2.16% of initial]
[Iter 1210/20000] Loss: 0.0123840 (Best: 0.0047089 @iter1195) ([91m↑128.02%[0m) [4.92% of initial]
[Iter 1220/20000] Loss: 0.0081281 (Best: 0.0047089 @iter1195) ([92m↓34.37%[0m) [3.23% of initial]
[Iter 1230/20000] Loss: 0.0067012 (Best: 0.0047089 @iter1195) ([92m↓17.56%[0m) [2.66% of initial]
[Iter 1240/20000] Loss: 0.0061042 (Best: 0.0047089 @iter1195) ([92m↓8.91%[0m) [2.43% of initial]
[Iter 1250/20000] Loss: 0.0054067 (Best: 0.0047089 @iter1195) ([92m↓11.43%[0m) [2.15% of initial]
[Iter 1260/20000] Loss: 0.0051951 (Best: 0.0044716 @iter1258) ([92m↓3.91%[0m) [2.06% of initial]
[Iter 1270/20000] Loss: 0.0047987 (Best: 0.0044625 @iter1269) ([92m↓7.63%[0m) [1.91% of initial]
[Iter 1280/20000] Loss: 0.0049754 (Best: 0.0041316 @iter1273) ([91m↑3.68%[0m) [1.98% of initial]
[Iter 1290/20000] Loss: 0.0049100 (Best: 0.0040692 @iter1288) ([92m↓1.32%[0m) [1.95% of initial]
Iter:1299, L1 loss=0.002991, Total loss=0.004384, Time:19
[Iter 1300/20000] Loss: 0.0047313 (Best: 0.0040692 @iter1288) ([92m↓3.64%[0m) [1.88% of initial]
[Iter 1310/20000] Loss: 0.0048043 (Best: 0.0040692 @iter1288) ([91m↑1.54%[0m) [1.91% of initial]
[Iter 1320/20000] Loss: 0.0045331 (Best: 0.0038643 @iter1319) ([92m↓5.65%[0m) [1.80% of initial]
[Iter 1330/20000] Loss: 0.0044820 (Best: 0.0037796 @iter1321) ([92m↓1.13%[0m) [1.78% of initial]
[Iter 1340/20000] Loss: 0.0043336 (Best: 0.0037796 @iter1321) ([92m↓3.31%[0m) [1.72% of initial]
[Iter 1350/20000] Loss: 0.0043177 (Best: 0.0037796 @iter1321) ([92m↓0.37%[0m) [1.72% of initial]
[Iter 1360/20000] Loss: 0.0043546 (Best: 0.0037796 @iter1321) ([91m↑0.85%[0m) [1.73% of initial]
[Iter 1370/20000] Loss: 0.0042020 (Best: 0.0037796 @iter1321) ([92m↓3.50%[0m) [1.67% of initial]
[Iter 1380/20000] Loss: 0.0043393 (Best: 0.0036630 @iter1375) ([91m↑3.27%[0m) [1.72% of initial]
[Iter 1390/20000] Loss: 0.0041833 (Best: 0.0036630 @iter1375) ([92m↓3.59%[0m) [1.66% of initial]
Iter:1399, L1 loss=0.002531, Total loss=0.00347, Time:14
[Iter 1400/20000] Loss: 0.0039262 (Best: 0.0034698 @iter1399) ([92m↓6.15%[0m) [1.56% of initial]
[Iter 1410/20000] Loss: 0.0098935 (Best: 0.0034698 @iter1399) ([91m↑151.99%[0m) [3.93% of initial]
[Iter 1420/20000] Loss: 0.0068206 (Best: 0.0034698 @iter1399) ([92m↓31.06%[0m) [2.71% of initial]
[Iter 1430/20000] Loss: 0.0054939 (Best: 0.0034698 @iter1399) ([92m↓19.45%[0m) [2.18% of initial]
[Iter 1440/20000] Loss: 0.0049253 (Best: 0.0034698 @iter1399) ([92m↓10.35%[0m) [1.96% of initial]
[Iter 1450/20000] Loss: 0.0040622 (Best: 0.0034698 @iter1399) ([92m↓17.52%[0m) [1.61% of initial]
[Iter 1460/20000] Loss: 0.0041290 (Best: 0.0034698 @iter1399) ([91m↑1.64%[0m) [1.64% of initial]
[Iter 1470/20000] Loss: 0.0039509 (Best: 0.0034698 @iter1399) ([92m↓4.31%[0m) [1.57% of initial]
[Iter 1480/20000] Loss: 0.0037362 (Best: 0.0032498 @iter1480) ([92m↓5.43%[0m) [1.48% of initial]
[Iter 1490/20000] Loss: 0.0037250 (Best: 0.0032498 @iter1480) ([92m↓0.30%[0m) [1.48% of initial]
Iter:1499, L1 loss=0.002868, Total loss=0.003854, Time:19
[Iter 1500/20000] Loss: 0.0036762 (Best: 0.0032498 @iter1480) ([92m↓1.31%[0m) [1.46% of initial]
Pruning 659 points (2.7%) from gaussian0 at iteration 1500
Pruning 737 points (3.0%) from gaussian1 at iteration 1500
[Iter 1510/20000] Loss: 0.0048843 (Best: 0.0032498 @iter1480) ([91m↑32.86%[0m) [1.94% of initial]
[Iter 1520/20000] Loss: 0.0042176 (Best: 0.0032498 @iter1480) ([92m↓13.65%[0m) [1.68% of initial]
[Iter 1530/20000] Loss: 0.0039439 (Best: 0.0032498 @iter1480) ([92m↓6.49%[0m) [1.57% of initial]
[Iter 1540/20000] Loss: 0.0037935 (Best: 0.0032498 @iter1480) ([92m↓3.81%[0m) [1.51% of initial]
[Iter 1550/20000] Loss: 0.0034878 (Best: 0.0032249 @iter1543) ([92m↓8.06%[0m) [1.39% of initial]
[Iter 1560/20000] Loss: 0.0036592 (Best: 0.0030060 @iter1558) ([91m↑4.92%[0m) [1.45% of initial]
[Iter 1570/20000] Loss: 0.0032923 (Best: 0.0029810 @iter1569) ([92m↓10.03%[0m) [1.31% of initial]
[Iter 1580/20000] Loss: 0.0032870 (Best: 0.0028057 @iter1573) ([92m↓0.16%[0m) [1.31% of initial]
[Iter 1590/20000] Loss: 0.0032354 (Best: 0.0028057 @iter1573) ([92m↓1.57%[0m) [1.29% of initial]
Iter:1599, L1 loss=0.002943, Total loss=0.00381, Time:22
[Iter 1600/20000] Loss: 0.0035155 (Best: 0.0028057 @iter1573) ([91m↑8.66%[0m) [1.40% of initial]
[Iter 1610/20000] Loss: 0.0098386 (Best: 0.0028057 @iter1573) ([91m↑179.87%[0m) [3.91% of initial]
[Iter 1620/20000] Loss: 0.0066054 (Best: 0.0028057 @iter1573) ([92m↓32.86%[0m) [2.62% of initial]
[Iter 1630/20000] Loss: 0.0048198 (Best: 0.0028057 @iter1573) ([92m↓27.03%[0m) [1.91% of initial]
[Iter 1640/20000] Loss: 0.0043656 (Best: 0.0028057 @iter1573) ([92m↓9.42%[0m) [1.73% of initial]
[Iter 1650/20000] Loss: 0.0040435 (Best: 0.0028057 @iter1573) ([92m↓7.38%[0m) [1.61% of initial]
[Iter 1660/20000] Loss: 0.0035095 (Best: 0.0028057 @iter1573) ([92m↓13.21%[0m) [1.39% of initial]
[Iter 1670/20000] Loss: 0.0033057 (Best: 0.0028057 @iter1573) ([92m↓5.81%[0m) [1.31% of initial]
[Iter 1680/20000] Loss: 0.0034018 (Best: 0.0028057 @iter1573) ([91m↑2.91%[0m) [1.35% of initial]
[Iter 1690/20000] Loss: 0.0034474 (Best: 0.0028057 @iter1573) ([91m↑1.34%[0m) [1.37% of initial]
Iter:1699, L1 loss=0.002662, Total loss=0.003404, Time:20
[Iter 1700/20000] Loss: 0.0031770 (Best: 0.0028057 @iter1573) ([92m↓7.84%[0m) [1.26% of initial]
[Iter 1710/20000] Loss: 0.0034265 (Best: 0.0028031 @iter1705) ([91m↑7.85%[0m) [1.36% of initial]
[Iter 1720/20000] Loss: 0.0029587 (Best: 0.0027939 @iter1720) ([92m↓13.65%[0m) [1.18% of initial]
[Iter 1730/20000] Loss: 0.0029716 (Best: 0.0027350 @iter1730) ([91m↑0.44%[0m) [1.18% of initial]
[Iter 1740/20000] Loss: 0.0030054 (Best: 0.0027014 @iter1738) ([91m↑1.14%[0m) [1.19% of initial]
[Iter 1750/20000] Loss: 0.0026987 (Best: 0.0024756 @iter1750) ([92m↓10.20%[0m) [1.07% of initial]
[Iter 1760/20000] Loss: 0.0029375 (Best: 0.0024756 @iter1750) ([91m↑8.85%[0m) [1.17% of initial]
[Iter 1770/20000] Loss: 0.0027859 (Best: 0.0024756 @iter1750) ([92m↓5.16%[0m) [1.11% of initial]
[Iter 1780/20000] Loss: 0.0028084 (Best: 0.0024675 @iter1771) ([91m↑0.81%[0m) [1.12% of initial]
[Iter 1790/20000] Loss: 0.0025347 (Best: 0.0021930 @iter1789) ([92m↓9.74%[0m) [1.01% of initial]
Iter:1799, L1 loss=0.001906, Total loss=0.002381, Time:18
[Iter 1800/20000] Loss: 0.0026024 (Best: 0.0021930 @iter1789) ([91m↑2.67%[0m) [1.03% of initial]
[Iter 1810/20000] Loss: 0.0091911 (Best: 0.0021930 @iter1789) ([91m↑253.17%[0m) [3.65% of initial]
[Iter 1820/20000] Loss: 0.0053204 (Best: 0.0021930 @iter1789) ([92m↓42.11%[0m) [2.11% of initial]
[Iter 1830/20000] Loss: 0.0044098 (Best: 0.0021930 @iter1789) ([92m↓17.12%[0m) [1.75% of initial]
[Iter 1840/20000] Loss: 0.0032407 (Best: 0.0021930 @iter1789) ([92m↓26.51%[0m) [1.29% of initial]
[Iter 1850/20000] Loss: 0.0030095 (Best: 0.0021930 @iter1789) ([92m↓7.13%[0m) [1.20% of initial]
[Iter 1860/20000] Loss: 0.0027869 (Best: 0.0021930 @iter1789) ([92m↓7.40%[0m) [1.11% of initial]
[Iter 1870/20000] Loss: 0.0025866 (Best: 0.0021930 @iter1789) ([92m↓7.19%[0m) [1.03% of initial]
[Iter 1880/20000] Loss: 0.0024574 (Best: 0.0021930 @iter1789) ([92m↓5.00%[0m) [0.98% of initial]
[Iter 1890/20000] Loss: 0.0022584 (Best: 0.0020950 @iter1890) ([92m↓8.10%[0m) [0.90% of initial]
Iter:1899, L1 loss=0.001931, Total loss=0.002243, Time:23
[Iter 1900/20000] Loss: 0.0023464 (Best: 0.0019723 @iter1891) ([91m↑3.90%[0m) [0.93% of initial]
[Iter 1910/20000] Loss: 0.0023694 (Best: 0.0019329 @iter1903) ([91m↑0.98%[0m) [0.94% of initial]
[Iter 1920/20000] Loss: 0.0024219 (Best: 0.0019329 @iter1903) ([91m↑2.22%[0m) [0.96% of initial]
[Iter 1930/20000] Loss: 0.0020688 (Best: 0.0018956 @iter1930) ([92m↓14.58%[0m) [0.82% of initial]
[Iter 1940/20000] Loss: 0.0021894 (Best: 0.0018865 @iter1939) ([91m↑5.83%[0m) [0.87% of initial]
[Iter 1950/20000] Loss: 0.0024752 (Best: 0.0018865 @iter1939) ([91m↑13.05%[0m) [0.98% of initial]
[Iter 1960/20000] Loss: 0.0022177 (Best: 0.0018865 @iter1939) ([92m↓10.40%[0m) [0.88% of initial]
[Iter 1970/20000] Loss: 0.0020408 (Best: 0.0018785 @iter1963) ([92m↓7.98%[0m) [0.81% of initial]
[Iter 1980/20000] Loss: 0.0023498 (Best: 0.0018785 @iter1963) ([91m↑15.14%[0m) [0.93% of initial]
[Iter 1990/20000] Loss: 0.0021064 (Best: 0.0018785 @iter1963) ([92m↓10.36%[0m) [0.84% of initial]
Iter:1999, L1 loss=0.001655, Total loss=0.00194, Time:22
[Iter 2000/20000] Loss: 0.0021905 (Best: 0.0017209 @iter1996) ([91m↑3.99%[0m) [0.87% of initial]
Testing Speed: 119.6850861048096 fps
Testing Time: 0.41776299476623535 s

[ITER 2000] Evaluating test: SSIM = 0.8639336740970611, PSNR = 17.609300174713134
Testing Speed: 130.96150123333436 fps
Testing Time: 0.022907495498657227 s

[ITER 2000] Evaluating train: SSIM = 0.9999516407648722, PSNR = 48.570456186930336
Iter:2000, total_points:42623
Pruning 665 points (1.2%) from gaussian0 at iteration 2000
Pruning 730 points (1.3%) from gaussian1 at iteration 2000
[Iter 2010/20000] Loss: 0.0080007 (Best: 0.0017209 @iter1996) ([91m↑265.25%[0m) [3.18% of initial]
[Iter 2020/20000] Loss: 0.0050709 (Best: 0.0017209 @iter1996) ([92m↓36.62%[0m) [2.01% of initial]
[Iter 2030/20000] Loss: 0.0037194 (Best: 0.0017209 @iter1996) ([92m↓26.65%[0m) [1.48% of initial]
[Iter 2040/20000] Loss: 0.0031932 (Best: 0.0017209 @iter1996) ([92m↓14.15%[0m) [1.27% of initial]
[Iter 2050/20000] Loss: 0.0027622 (Best: 0.0017209 @iter1996) ([92m↓13.50%[0m) [1.10% of initial]
[Iter 2060/20000] Loss: 0.0023987 (Best: 0.0017209 @iter1996) ([92m↓13.16%[0m) [0.95% of initial]
[Iter 2070/20000] Loss: 0.0025588 (Best: 0.0017209 @iter1996) ([91m↑6.67%[0m) [1.02% of initial]
[Iter 2080/20000] Loss: 0.0024370 (Best: 0.0017209 @iter1996) ([92m↓4.76%[0m) [0.97% of initial]
[Iter 2090/20000] Loss: 0.0023824 (Best: 0.0017209 @iter1996) ([92m↓2.24%[0m) [0.95% of initial]
Iter:2099, L1 loss=0.001964, Total loss=0.002275, Time:22
[Iter 2100/20000] Loss: 0.0022520 (Best: 0.0017209 @iter1996) ([92m↓5.47%[0m) [0.89% of initial]
[Iter 2110/20000] Loss: 0.0021570 (Best: 0.0017209 @iter1996) ([92m↓4.22%[0m) [0.86% of initial]
[Iter 2120/20000] Loss: 0.0019419 (Best: 0.0017209 @iter1996) ([92m↓9.97%[0m) [0.77% of initial]
[Iter 2130/20000] Loss: 0.0021159 (Best: 0.0017209 @iter1996) ([91m↑8.96%[0m) [0.84% of initial]
[Iter 2140/20000] Loss: 0.0022173 (Best: 0.0017209 @iter1996) ([91m↑4.79%[0m) [0.88% of initial]
[Iter 2150/20000] Loss: 0.0022493 (Best: 0.0017209 @iter1996) ([91m↑1.44%[0m) [0.89% of initial]
[Iter 2160/20000] Loss: 0.0021010 (Best: 0.0017209 @iter1996) ([92m↓6.59%[0m) [0.83% of initial]
[Iter 2170/20000] Loss: 0.0021218 (Best: 0.0017209 @iter1996) ([91m↑0.99%[0m) [0.84% of initial]
[Iter 2180/20000] Loss: 0.0018205 (Best: 0.0016963 @iter2180) ([92m↓14.20%[0m) [0.72% of initial]
[Iter 2190/20000] Loss: 0.0021048 (Best: 0.0016963 @iter2180) ([91m↑15.62%[0m) [0.84% of initial]
Iter:2199, L1 loss=0.001813, Total loss=0.002014, Time:26
[Iter 2200/20000] Loss: 0.0020828 (Best: 0.0016766 @iter2191) ([92m↓1.05%[0m) [0.83% of initial]
[Iter 2210/20000] Loss: 0.0087117 (Best: 0.0016766 @iter2191) ([91m↑318.27%[0m) [3.46% of initial]
[Iter 2220/20000] Loss: 0.0049476 (Best: 0.0016766 @iter2191) ([92m↓43.21%[0m) [1.97% of initial]
[Iter 2230/20000] Loss: 0.0031843 (Best: 0.0016766 @iter2191) ([92m↓35.64%[0m) [1.27% of initial]
[Iter 2240/20000] Loss: 0.0027000 (Best: 0.0016766 @iter2191) ([92m↓15.21%[0m) [1.07% of initial]
[Iter 2250/20000] Loss: 0.0025452 (Best: 0.0016766 @iter2191) ([92m↓5.73%[0m) [1.01% of initial]
[Iter 2260/20000] Loss: 0.0021475 (Best: 0.0016766 @iter2191) ([92m↓15.63%[0m) [0.85% of initial]
[Iter 2270/20000] Loss: 0.0021857 (Best: 0.0016766 @iter2191) ([91m↑1.78%[0m) [0.87% of initial]
[Iter 2280/20000] Loss: 0.0018488 (Best: 0.0016766 @iter2191) ([92m↓15.41%[0m) [0.73% of initial]
[Iter 2290/20000] Loss: 0.0017884 (Best: 0.0015869 @iter2287) ([92m↓3.27%[0m) [0.71% of initial]
Iter:2299, L1 loss=0.001587, Total loss=0.001715, Time:24
[Iter 2300/20000] Loss: 0.0020434 (Best: 0.0015869 @iter2287) ([91m↑14.26%[0m) [0.81% of initial]
[Iter 2310/20000] Loss: 0.0019283 (Best: 0.0015869 @iter2287) ([92m↓5.64%[0m) [0.77% of initial]
[Iter 2320/20000] Loss: 0.0017015 (Best: 0.0015595 @iter2320) ([92m↓11.76%[0m) [0.68% of initial]
[Iter 2330/20000] Loss: 0.0016928 (Best: 0.0015242 @iter2324) ([92m↓0.51%[0m) [0.67% of initial]
[Iter 2340/20000] Loss: 0.0017468 (Best: 0.0014926 @iter2338) ([91m↑3.19%[0m) [0.69% of initial]
[Iter 2350/20000] Loss: 0.0018307 (Best: 0.0014809 @iter2342) ([91m↑4.80%[0m) [0.73% of initial]
[Iter 2360/20000] Loss: 0.0016343 (Best: 0.0014269 @iter2359) ([92m↓10.73%[0m) [0.65% of initial]
[Iter 2370/20000] Loss: 0.0017584 (Best: 0.0014269 @iter2359) ([91m↑7.59%[0m) [0.70% of initial]
[Iter 2380/20000] Loss: 0.0018236 (Best: 0.0014269 @iter2359) ([91m↑3.70%[0m) [0.72% of initial]
[Iter 2390/20000] Loss: 0.0019783 (Best: 0.0014269 @iter2359) ([91m↑8.48%[0m) [0.79% of initial]
Iter:2399, L1 loss=0.001468, Total loss=0.001522, Time:26
[Iter 2400/20000] Loss: 0.0016624 (Best: 0.0014269 @iter2359) ([92m↓15.97%[0m) [0.66% of initial]
[Iter 2410/20000] Loss: 0.0067322 (Best: 0.0014269 @iter2359) ([91m↑304.96%[0m) [2.67% of initial]
[Iter 2420/20000] Loss: 0.0039196 (Best: 0.0014269 @iter2359) ([92m↓41.78%[0m) [1.56% of initial]
[Iter 2430/20000] Loss: 0.0028883 (Best: 0.0014269 @iter2359) ([92m↓26.31%[0m) [1.15% of initial]
[Iter 2440/20000] Loss: 0.0023718 (Best: 0.0014269 @iter2359) ([92m↓17.88%[0m) [0.94% of initial]
[Iter 2450/20000] Loss: 0.0022596 (Best: 0.0014269 @iter2359) ([92m↓4.73%[0m) [0.90% of initial]
[Iter 2460/20000] Loss: 0.0019537 (Best: 0.0014269 @iter2359) ([92m↓13.54%[0m) [0.78% of initial]
[Iter 2470/20000] Loss: 0.0018921 (Best: 0.0014269 @iter2359) ([92m↓3.15%[0m) [0.75% of initial]
[Iter 2480/20000] Loss: 0.0018869 (Best: 0.0014269 @iter2359) ([92m↓0.27%[0m) [0.75% of initial]
[Iter 2490/20000] Loss: 0.0017019 (Best: 0.0014269 @iter2359) ([92m↓9.81%[0m) [0.68% of initial]
Iter:2499, L1 loss=0.001391, Total loss=0.00148, Time:25
[Iter 2500/20000] Loss: 0.0015505 (Best: 0.0014269 @iter2359) ([92m↓8.89%[0m) [0.62% of initial]
Pruning 497 points (0.6%) from gaussian0 at iteration 2500
Pruning 537 points (0.7%) from gaussian1 at iteration 2500
[Iter 2510/20000] Loss: 0.0033516 (Best: 0.0014269 @iter2359) ([91m↑116.16%[0m) [1.33% of initial]
[Iter 2520/20000] Loss: 0.0022367 (Best: 0.0014269 @iter2359) ([92m↓33.26%[0m) [0.89% of initial]
[Iter 2530/20000] Loss: 0.0016884 (Best: 0.0014269 @iter2359) ([92m↓24.51%[0m) [0.67% of initial]
[Iter 2540/20000] Loss: 0.0015706 (Best: 0.0014116 @iter2533) ([92m↓6.98%[0m) [0.62% of initial]
[Iter 2550/20000] Loss: 0.0017020 (Best: 0.0013079 @iter2548) ([91m↑8.36%[0m) [0.68% of initial]
[Iter 2560/20000] Loss: 0.0014533 (Best: 0.0012437 @iter2557) ([92m↓14.61%[0m) [0.58% of initial]
[Iter 2570/20000] Loss: 0.0016381 (Best: 0.0012437 @iter2557) ([91m↑12.72%[0m) [0.65% of initial]
[Iter 2580/20000] Loss: 0.0014639 (Best: 0.0011622 @iter2578) ([92m↓10.63%[0m) [0.58% of initial]
[Iter 2590/20000] Loss: 0.0015619 (Best: 0.0011509 @iter2584) ([91m↑6.69%[0m) [0.62% of initial]
Iter:2599, L1 loss=0.001216, Total loss=0.001253, Time:26
[Iter 2600/20000] Loss: 0.0014667 (Best: 0.0011509 @iter2584) ([92m↓6.09%[0m) [0.58% of initial]
[Iter 2610/20000] Loss: 0.0067031 (Best: 0.0011509 @iter2584) ([91m↑357.01%[0m) [2.66% of initial]
[Iter 2620/20000] Loss: 0.0038604 (Best: 0.0011509 @iter2584) ([92m↓42.41%[0m) [1.53% of initial]
[Iter 2630/20000] Loss: 0.0025211 (Best: 0.0011509 @iter2584) ([92m↓34.69%[0m) [1.00% of initial]
[Iter 2640/20000] Loss: 0.0020215 (Best: 0.0011509 @iter2584) ([92m↓19.82%[0m) [0.80% of initial]
[Iter 2650/20000] Loss: 0.0016893 (Best: 0.0011509 @iter2584) ([92m↓16.44%[0m) [0.67% of initial]
[Iter 2660/20000] Loss: 0.0018997 (Best: 0.0011509 @iter2584) ([91m↑12.46%[0m) [0.75% of initial]
[Iter 2670/20000] Loss: 0.0017992 (Best: 0.0011509 @iter2584) ([92m↓5.29%[0m) [0.71% of initial]
[Iter 2680/20000] Loss: 0.0014482 (Best: 0.0011509 @iter2584) ([92m↓19.51%[0m) [0.58% of initial]
[Iter 2690/20000] Loss: 0.0014088 (Best: 0.0011509 @iter2584) ([92m↓2.72%[0m) [0.56% of initial]
Iter:2699, L1 loss=0.001363, Total loss=0.001393, Time:26
[Iter 2700/20000] Loss: 0.0017159 (Best: 0.0011509 @iter2584) ([91m↑21.80%[0m) [0.68% of initial]
[Iter 2710/20000] Loss: 0.0014514 (Best: 0.0011509 @iter2584) ([92m↓15.42%[0m) [0.58% of initial]
[Iter 2720/20000] Loss: 0.0013113 (Best: 0.0011509 @iter2584) ([92m↓9.66%[0m) [0.52% of initial]
[Iter 2730/20000] Loss: 0.0012070 (Best: 0.0010613 @iter2725) ([92m↓7.95%[0m) [0.48% of initial]
[Iter 2740/20000] Loss: 0.0010728 (Best: 0.0009589 @iter2740) ([92m↓11.12%[0m) [0.43% of initial]
[Iter 2750/20000] Loss: 0.0013804 (Best: 0.0009589 @iter2740) ([91m↑28.67%[0m) [0.55% of initial]
[Iter 2760/20000] Loss: 0.0015101 (Best: 0.0009589 @iter2740) ([91m↑9.39%[0m) [0.60% of initial]
[Iter 2770/20000] Loss: 0.0016229 (Best: 0.0009589 @iter2740) ([91m↑7.48%[0m) [0.64% of initial]
[Iter 2780/20000] Loss: 0.0013307 (Best: 0.0009589 @iter2740) ([92m↓18.00%[0m) [0.53% of initial]
[Iter 2790/20000] Loss: 0.0013819 (Best: 0.0009589 @iter2740) ([91m↑3.84%[0m) [0.55% of initial]
Iter:2799, L1 loss=0.001414, Total loss=0.001547, Time:27
[Iter 2800/20000] Loss: 0.0013924 (Best: 0.0009589 @iter2740) ([91m↑0.76%[0m) [0.55% of initial]
[Iter 2810/20000] Loss: 0.0056787 (Best: 0.0009589 @iter2740) ([91m↑307.83%[0m) [2.26% of initial]
[Iter 2820/20000] Loss: 0.0030386 (Best: 0.0009589 @iter2740) ([92m↓46.49%[0m) [1.21% of initial]
[Iter 2830/20000] Loss: 0.0019869 (Best: 0.0009589 @iter2740) ([92m↓34.61%[0m) [0.79% of initial]
[Iter 2840/20000] Loss: 0.0017137 (Best: 0.0009589 @iter2740) ([92m↓13.75%[0m) [0.68% of initial]
[Iter 2850/20000] Loss: 0.0015058 (Best: 0.0009589 @iter2740) ([92m↓12.13%[0m) [0.60% of initial]
[Iter 2860/20000] Loss: 0.0016210 (Best: 0.0009589 @iter2740) ([91m↑7.65%[0m) [0.64% of initial]
[Iter 2870/20000] Loss: 0.0013906 (Best: 0.0009589 @iter2740) ([92m↓14.21%[0m) [0.55% of initial]
[Iter 2880/20000] Loss: 0.0013447 (Best: 0.0009589 @iter2740) ([92m↓3.30%[0m) [0.53% of initial]
[Iter 2890/20000] Loss: 0.0012779 (Best: 0.0009589 @iter2740) ([92m↓4.97%[0m) [0.51% of initial]
Iter:2899, L1 loss=0.001008, Total loss=0.001024, Time:25
[Iter 2900/20000] Loss: 0.0012220 (Best: 0.0009589 @iter2740) ([92m↓4.38%[0m) [0.49% of initial]
[Iter 2910/20000] Loss: 0.0013570 (Best: 0.0009589 @iter2740) ([91m↑11.05%[0m) [0.54% of initial]
[Iter 2920/20000] Loss: 0.0014170 (Best: 0.0009589 @iter2740) ([91m↑4.43%[0m) [0.56% of initial]
[Iter 2930/20000] Loss: 0.0013201 (Best: 0.0009589 @iter2740) ([92m↓6.84%[0m) [0.52% of initial]
[Iter 2940/20000] Loss: 0.0011453 (Best: 0.0009589 @iter2740) ([92m↓13.24%[0m) [0.46% of initial]
[Iter 2950/20000] Loss: 0.0010667 (Best: 0.0009117 @iter2950) ([92m↓6.87%[0m) [0.42% of initial]
[Iter 2960/20000] Loss: 0.0011750 (Best: 0.0009117 @iter2950) ([91m↑10.16%[0m) [0.47% of initial]
[Iter 2970/20000] Loss: 0.0010597 (Best: 0.0008533 @iter2969) ([92m↓9.81%[0m) [0.42% of initial]
[Iter 2980/20000] Loss: 0.0010040 (Best: 0.0008533 @iter2969) ([92m↓5.26%[0m) [0.40% of initial]
[Iter 2990/20000] Loss: 0.0010378 (Best: 0.0007953 @iter2983) ([91m↑3.36%[0m) [0.41% of initial]
Iter:2999, L1 loss=0.0008053, Total loss=0.0007963, Time:27
[Iter 3000/20000] Loss: 0.0010182 (Best: 0.0007953 @iter2983) ([92m↓1.88%[0m) [0.40% of initial]
Pruning 380 points (0.3%) from gaussian0 at iteration 3000
Pruning 368 points (0.3%) from gaussian1 at iteration 3000
[Iter 3010/20000] Loss: 0.0056973 (Best: 0.0007953 @iter2983) ([91m↑459.53%[0m) [2.26% of initial]
[Iter 3020/20000] Loss: 0.0034144 (Best: 0.0007953 @iter2983) ([92m↓40.07%[0m) [1.36% of initial]
[Iter 3030/20000] Loss: 0.0024079 (Best: 0.0007953 @iter2983) ([92m↓29.48%[0m) [0.96% of initial]
[Iter 3040/20000] Loss: 0.0018695 (Best: 0.0007953 @iter2983) ([92m↓22.36%[0m) [0.74% of initial]
[Iter 3050/20000] Loss: 0.0016989 (Best: 0.0007953 @iter2983) ([92m↓9.12%[0m) [0.67% of initial]
[Iter 3060/20000] Loss: 0.0015909 (Best: 0.0007953 @iter2983) ([92m↓6.36%[0m) [0.63% of initial]
[Iter 3070/20000] Loss: 0.0014103 (Best: 0.0007953 @iter2983) ([92m↓11.35%[0m) [0.56% of initial]
[Iter 3080/20000] Loss: 0.0014145 (Best: 0.0007953 @iter2983) ([91m↑0.29%[0m) [0.56% of initial]
[Iter 3090/20000] Loss: 0.0013549 (Best: 0.0007953 @iter2983) ([92m↓4.21%[0m) [0.54% of initial]
Iter:3099, L1 loss=0.001102, Total loss=0.001145, Time:30
[Iter 3100/20000] Loss: 0.0012914 (Best: 0.0007953 @iter2983) ([92m↓4.69%[0m) [0.51% of initial]
[Iter 3110/20000] Loss: 0.0014303 (Best: 0.0007953 @iter2983) ([91m↑10.76%[0m) [0.57% of initial]
[Iter 3120/20000] Loss: 0.0013664 (Best: 0.0007953 @iter2983) ([92m↓4.47%[0m) [0.54% of initial]
[Iter 3130/20000] Loss: 0.0011487 (Best: 0.0007953 @iter2983) ([92m↓15.94%[0m) [0.46% of initial]
[Iter 3140/20000] Loss: 0.0010810 (Best: 0.0007953 @iter2983) ([92m↓5.89%[0m) [0.43% of initial]
[Iter 3150/20000] Loss: 0.0011638 (Best: 0.0007953 @iter2983) ([91m↑7.66%[0m) [0.46% of initial]
[Iter 3160/20000] Loss: 0.0010790 (Best: 0.0007953 @iter2983) ([92m↓7.29%[0m) [0.43% of initial]
[Iter 3170/20000] Loss: 0.0010831 (Best: 0.0007953 @iter2983) ([91m↑0.38%[0m) [0.43% of initial]
[Iter 3180/20000] Loss: 0.0011463 (Best: 0.0007953 @iter2983) ([91m↑5.84%[0m) [0.46% of initial]
[Iter 3190/20000] Loss: 0.0011184 (Best: 0.0007953 @iter2983) ([92m↓2.44%[0m) [0.44% of initial]
Iter:3199, L1 loss=0.001017, Total loss=0.001013, Time:32
[Iter 3200/20000] Loss: 0.0010731 (Best: 0.0007953 @iter2983) ([92m↓4.06%[0m) [0.43% of initial]
[Iter 3210/20000] Loss: 0.0056278 (Best: 0.0007953 @iter2983) ([91m↑424.46%[0m) [2.24% of initial]
[Iter 3220/20000] Loss: 0.0032222 (Best: 0.0007953 @iter2983) ([92m↓42.75%[0m) [1.28% of initial]
[Iter 3230/20000] Loss: 0.0020420 (Best: 0.0007953 @iter2983) ([92m↓36.62%[0m) [0.81% of initial]
[Iter 3240/20000] Loss: 0.0017980 (Best: 0.0007953 @iter2983) ([92m↓11.95%[0m) [0.71% of initial]
[Iter 3250/20000] Loss: 0.0013573 (Best: 0.0007953 @iter2983) ([92m↓24.51%[0m) [0.54% of initial]
[Iter 3260/20000] Loss: 0.0012147 (Best: 0.0007953 @iter2983) ([92m↓10.50%[0m) [0.48% of initial]
[Iter 3270/20000] Loss: 0.0012604 (Best: 0.0007953 @iter2983) ([91m↑3.76%[0m) [0.50% of initial]
[Iter 3280/20000] Loss: 0.0013297 (Best: 0.0007953 @iter2983) ([91m↑5.49%[0m) [0.53% of initial]
[Iter 3290/20000] Loss: 0.0010059 (Best: 0.0007953 @iter2983) ([92m↓24.35%[0m) [0.40% of initial]
Iter:3299, L1 loss=0.001448, Total loss=0.001618, Time:32
[Iter 3300/20000] Loss: 0.0013588 (Best: 0.0007953 @iter2983) ([91m↑35.09%[0m) [0.54% of initial]
[Iter 3310/20000] Loss: 0.0010207 (Best: 0.0007953 @iter2983) ([92m↓24.88%[0m) [0.41% of initial]
[Iter 3320/20000] Loss: 0.0011656 (Best: 0.0007953 @iter2983) ([91m↑14.20%[0m) [0.46% of initial]
[Iter 3330/20000] Loss: 0.0012299 (Best: 0.0007953 @iter2983) ([91m↑5.51%[0m) [0.49% of initial]
[Iter 3340/20000] Loss: 0.0013256 (Best: 0.0007953 @iter2983) ([91m↑7.79%[0m) [0.53% of initial]
[Iter 3350/20000] Loss: 0.0010960 (Best: 0.0007953 @iter2983) ([92m↓17.32%[0m) [0.44% of initial]
[Iter 3360/20000] Loss: 0.0013563 (Best: 0.0007953 @iter2983) ([91m↑23.74%[0m) [0.54% of initial]
[Iter 3370/20000] Loss: 0.0009855 (Best: 0.0007953 @iter2983) ([92m↓27.34%[0m) [0.39% of initial]
[Iter 3380/20000] Loss: 0.0009665 (Best: 0.0007933 @iter3379) ([92m↓1.93%[0m) [0.38% of initial]
[Iter 3390/20000] Loss: 0.0012471 (Best: 0.0007933 @iter3379) ([91m↑29.03%[0m) [0.50% of initial]
Iter:3399, L1 loss=0.001536, Total loss=0.001564, Time:33
[Iter 3400/20000] Loss: 0.0012967 (Best: 0.0007933 @iter3379) ([91m↑3.98%[0m) [0.52% of initial]
[Iter 3410/20000] Loss: 0.0051829 (Best: 0.0007933 @iter3379) ([91m↑299.70%[0m) [2.06% of initial]
[Iter 3420/20000] Loss: 0.0025602 (Best: 0.0007933 @iter3379) ([92m↓50.60%[0m) [1.02% of initial]
[Iter 3430/20000] Loss: 0.0016643 (Best: 0.0007933 @iter3379) ([92m↓34.99%[0m) [0.66% of initial]
[Iter 3440/20000] Loss: 0.0014805 (Best: 0.0007933 @iter3379) ([92m↓11.05%[0m) [0.59% of initial]
[Iter 3450/20000] Loss: 0.0014251 (Best: 0.0007933 @iter3379) ([92m↓3.74%[0m) [0.57% of initial]
[Iter 3460/20000] Loss: 0.0012874 (Best: 0.0007933 @iter3379) ([92m↓9.67%[0m) [0.51% of initial]
[Iter 3470/20000] Loss: 0.0012022 (Best: 0.0007933 @iter3379) ([92m↓6.62%[0m) [0.48% of initial]
[Iter 3480/20000] Loss: 0.0010966 (Best: 0.0007933 @iter3379) ([92m↓8.78%[0m) [0.44% of initial]
[Iter 3490/20000] Loss: 0.0010460 (Best: 0.0007933 @iter3379) ([92m↓4.62%[0m) [0.42% of initial]
Iter:3499, L1 loss=0.0007809, Total loss=0.0007549, Time:36
[Iter 3500/20000] Loss: 0.0008266 (Best: 0.0007549 @iter3499) ([92m↓20.97%[0m) [0.33% of initial]
Pruning 307 points (0.2%) from gaussian0 at iteration 3500
Pruning 317 points (0.2%) from gaussian1 at iteration 3500
[Iter 3510/20000] Loss: 0.0025120 (Best: 0.0007549 @iter3499) ([91m↑203.88%[0m) [1.00% of initial]
[Iter 3520/20000] Loss: 0.0016366 (Best: 0.0007549 @iter3499) ([92m↓34.85%[0m) [0.65% of initial]
[Iter 3530/20000] Loss: 0.0012273 (Best: 0.0007549 @iter3499) ([92m↓25.01%[0m) [0.49% of initial]
[Iter 3540/20000] Loss: 0.0013741 (Best: 0.0007549 @iter3499) ([91m↑11.96%[0m) [0.55% of initial]
[Iter 3550/20000] Loss: 0.0012333 (Best: 0.0007549 @iter3499) ([92m↓10.24%[0m) [0.49% of initial]
[Iter 3560/20000] Loss: 0.0010850 (Best: 0.0007549 @iter3499) ([92m↓12.02%[0m) [0.43% of initial]
[Iter 3570/20000] Loss: 0.0011408 (Best: 0.0007549 @iter3499) ([91m↑5.14%[0m) [0.45% of initial]
[Iter 3580/20000] Loss: 0.0008809 (Best: 0.0007549 @iter3499) ([92m↓22.78%[0m) [0.35% of initial]
[Iter 3590/20000] Loss: 0.0008914 (Best: 0.0007549 @iter3499) ([91m↑1.19%[0m) [0.35% of initial]
Iter:3599, L1 loss=0.000775, Total loss=0.0007452, Time:33
[Iter 3600/20000] Loss: 0.0008637 (Best: 0.0007077 @iter3598) ([92m↓3.10%[0m) [0.34% of initial]
[Iter 3610/20000] Loss: 0.0045684 (Best: 0.0007077 @iter3598) ([91m↑428.91%[0m) [1.81% of initial]
[Iter 3620/20000] Loss: 0.0028420 (Best: 0.0007077 @iter3598) ([92m↓37.79%[0m) [1.13% of initial]
[Iter 3630/20000] Loss: 0.0017420 (Best: 0.0007077 @iter3598) ([92m↓38.70%[0m) [0.69% of initial]
[Iter 3640/20000] Loss: 0.0013530 (Best: 0.0007077 @iter3598) ([92m↓22.33%[0m) [0.54% of initial]
[Iter 3650/20000] Loss: 0.0013367 (Best: 0.0007077 @iter3598) ([92m↓1.20%[0m) [0.53% of initial]
[Iter 3660/20000] Loss: 0.0011080 (Best: 0.0007077 @iter3598) ([92m↓17.11%[0m) [0.44% of initial]
[Iter 3670/20000] Loss: 0.0009545 (Best: 0.0007077 @iter3598) ([92m↓13.86%[0m) [0.38% of initial]
[Iter 3680/20000] Loss: 0.0012083 (Best: 0.0007077 @iter3598) ([91m↑26.60%[0m) [0.48% of initial]
[Iter 3690/20000] Loss: 0.0014083 (Best: 0.0007077 @iter3598) ([91m↑16.55%[0m) [0.56% of initial]
Iter:3699, L1 loss=0.001215, Total loss=0.001219, Time:35
[Iter 3700/20000] Loss: 0.0012080 (Best: 0.0007077 @iter3598) ([92m↓14.22%[0m) [0.48% of initial]
[Iter 3710/20000] Loss: 0.0009975 (Best: 0.0007077 @iter3598) ([92m↓17.43%[0m) [0.40% of initial]
[Iter 3720/20000] Loss: 0.0010551 (Best: 0.0007077 @iter3598) ([91m↑5.78%[0m) [0.42% of initial]
[Iter 3730/20000] Loss: 0.0009057 (Best: 0.0007077 @iter3598) ([92m↓14.16%[0m) [0.36% of initial]
[Iter 3740/20000] Loss: 0.0009345 (Best: 0.0007077 @iter3598) ([91m↑3.18%[0m) [0.37% of initial]
[Iter 3750/20000] Loss: 0.0009682 (Best: 0.0007077 @iter3598) ([91m↑3.61%[0m) [0.38% of initial]
[Iter 3760/20000] Loss: 0.0009446 (Best: 0.0007077 @iter3598) ([92m↓2.44%[0m) [0.38% of initial]
[Iter 3770/20000] Loss: 0.0008982 (Best: 0.0007077 @iter3598) ([92m↓4.91%[0m) [0.36% of initial]
[Iter 3780/20000] Loss: 0.0008435 (Best: 0.0006579 @iter3775) ([92m↓6.09%[0m) [0.34% of initial]
[Iter 3790/20000] Loss: 0.0006933 (Best: 0.0006098 @iter3790) ([92m↓17.81%[0m) [0.28% of initial]
Iter:3799, L1 loss=0.0008631, Total loss=0.0008777, Time:28
[Iter 3800/20000] Loss: 0.0008611 (Best: 0.0006098 @iter3790) ([91m↑24.20%[0m) [0.34% of initial]
[Iter 3810/20000] Loss: 0.0042488 (Best: 0.0006098 @iter3790) ([91m↑393.41%[0m) [1.69% of initial]
[Iter 3820/20000] Loss: 0.0023342 (Best: 0.0006098 @iter3790) ([92m↓45.06%[0m) [0.93% of initial]
[Iter 3830/20000] Loss: 0.0014282 (Best: 0.0006098 @iter3790) ([92m↓38.81%[0m) [0.57% of initial]
[Iter 3840/20000] Loss: 0.0015191 (Best: 0.0006098 @iter3790) ([91m↑6.37%[0m) [0.60% of initial]
[Iter 3850/20000] Loss: 0.0011519 (Best: 0.0006098 @iter3790) ([92m↓24.17%[0m) [0.46% of initial]
[Iter 3860/20000] Loss: 0.0011215 (Best: 0.0006098 @iter3790) ([92m↓2.64%[0m) [0.45% of initial]
[Iter 3870/20000] Loss: 0.0009037 (Best: 0.0006098 @iter3790) ([92m↓19.42%[0m) [0.36% of initial]
[Iter 3880/20000] Loss: 0.0009424 (Best: 0.0006098 @iter3790) ([91m↑4.28%[0m) [0.37% of initial]
[Iter 3890/20000] Loss: 0.0007566 (Best: 0.0006098 @iter3790) ([92m↓19.71%[0m) [0.30% of initial]
Iter:3899, L1 loss=0.0008442, Total loss=0.0008249, Time:37
[Iter 3900/20000] Loss: 0.0007970 (Best: 0.0005893 @iter3898) ([91m↑5.33%[0m) [0.32% of initial]
[Iter 3910/20000] Loss: 0.0009577 (Best: 0.0005893 @iter3898) ([91m↑20.16%[0m) [0.38% of initial]
[Iter 3920/20000] Loss: 0.0009955 (Best: 0.0005893 @iter3898) ([91m↑3.95%[0m) [0.40% of initial]
[Iter 3930/20000] Loss: 0.0009716 (Best: 0.0005893 @iter3898) ([92m↓2.40%[0m) [0.39% of initial]
[Iter 3940/20000] Loss: 0.0007991 (Best: 0.0005893 @iter3898) ([92m↓17.76%[0m) [0.32% of initial]
[Iter 3950/20000] Loss: 0.0008867 (Best: 0.0005893 @iter3898) ([91m↑10.96%[0m) [0.35% of initial]
[Iter 3960/20000] Loss: 0.0008975 (Best: 0.0005893 @iter3898) ([91m↑1.22%[0m) [0.36% of initial]
[Iter 3970/20000] Loss: 0.0008014 (Best: 0.0005893 @iter3898) ([92m↓10.71%[0m) [0.32% of initial]
[Iter 3980/20000] Loss: 0.0011550 (Best: 0.0005893 @iter3898) ([91m↑44.11%[0m) [0.46% of initial]
[Iter 3990/20000] Loss: 0.0008777 (Best: 0.0005893 @iter3898) ([92m↓24.01%[0m) [0.35% of initial]
Iter:3999, L1 loss=0.0009605, Total loss=0.0009579, Time:36
[Iter 4000/20000] Loss: 0.0008713 (Best: 0.0005893 @iter3898) ([92m↓0.72%[0m) [0.35% of initial]
Pruning 293 points (0.2%) from gaussian0 at iteration 4000
Pruning 257 points (0.2%) from gaussian1 at iteration 4000
[Iter 4010/20000] Loss: 0.1509534 (Best: 0.0005893 @iter3898) ([91m↑17224.50%[0m) [59.97% of initial]
[Iter 4020/20000] Loss: 0.1042555 (Best: 0.0005893 @iter3898) ([92m↓30.94%[0m) [41.42% of initial]
[Iter 4030/20000] Loss: 0.0666630 (Best: 0.0005893 @iter3898) ([92m↓36.06%[0m) [26.48% of initial]
[Iter 4040/20000] Loss: 0.0372397 (Best: 0.0005893 @iter3898) ([92m↓44.14%[0m) [14.79% of initial]
[Iter 4050/20000] Loss: 0.0171762 (Best: 0.0005893 @iter3898) ([92m↓53.88%[0m) [6.82% of initial]
[Iter 4060/20000] Loss: 0.0080140 (Best: 0.0005893 @iter3898) ([92m↓53.34%[0m) [3.18% of initial]
[Iter 4070/20000] Loss: 0.0049723 (Best: 0.0005893 @iter3898) ([92m↓37.95%[0m) [1.98% of initial]
[Iter 4080/20000] Loss: 0.0037523 (Best: 0.0005893 @iter3898) ([92m↓24.54%[0m) [1.49% of initial]
[Iter 4090/20000] Loss: 0.0027871 (Best: 0.0005893 @iter3898) ([92m↓25.72%[0m) [1.11% of initial]
Iter:4099, L1 loss=0.002193, Total loss=0.002335, Time:47
[Iter 4100/20000] Loss: 0.0023528 (Best: 0.0005893 @iter3898) ([92m↓15.58%[0m) [0.93% of initial]
[Iter 4110/20000] Loss: 0.0020927 (Best: 0.0005893 @iter3898) ([92m↓11.05%[0m) [0.83% of initial]
[Iter 4120/20000] Loss: 0.0018296 (Best: 0.0005893 @iter3898) ([92m↓12.57%[0m) [0.73% of initial]
[Iter 4130/20000] Loss: 0.0018131 (Best: 0.0005893 @iter3898) ([92m↓0.90%[0m) [0.72% of initial]
[Iter 4140/20000] Loss: 0.0016524 (Best: 0.0005893 @iter3898) ([92m↓8.86%[0m) [0.66% of initial]
[Iter 4150/20000] Loss: 0.0014847 (Best: 0.0005893 @iter3898) ([92m↓10.15%[0m) [0.59% of initial]
[Iter 4160/20000] Loss: 0.0015548 (Best: 0.0005893 @iter3898) ([91m↑4.72%[0m) [0.62% of initial]
[Iter 4170/20000] Loss: 0.0014670 (Best: 0.0005893 @iter3898) ([92m↓5.65%[0m) [0.58% of initial]
[Iter 4180/20000] Loss: 0.0014532 (Best: 0.0005893 @iter3898) ([92m↓0.94%[0m) [0.58% of initial]
[Iter 4190/20000] Loss: 0.0012832 (Best: 0.0005893 @iter3898) ([92m↓11.70%[0m) [0.51% of initial]
Iter:4199, L1 loss=0.001243, Total loss=0.001257, Time:45
[Iter 4200/20000] Loss: 0.0013494 (Best: 0.0005893 @iter3898) ([91m↑5.16%[0m) [0.54% of initial]
[Iter 4210/20000] Loss: 0.0034420 (Best: 0.0005893 @iter3898) ([91m↑155.08%[0m) [1.37% of initial]
[Iter 4220/20000] Loss: 0.0022966 (Best: 0.0005893 @iter3898) ([92m↓33.28%[0m) [0.91% of initial]
[Iter 4230/20000] Loss: 0.0015670 (Best: 0.0005893 @iter3898) ([92m↓31.77%[0m) [0.62% of initial]
[Iter 4240/20000] Loss: 0.0013685 (Best: 0.0005893 @iter3898) ([92m↓12.67%[0m) [0.54% of initial]
[Iter 4250/20000] Loss: 0.0013491 (Best: 0.0005893 @iter3898) ([92m↓1.42%[0m) [0.54% of initial]
[Iter 4260/20000] Loss: 0.0013991 (Best: 0.0005893 @iter3898) ([91m↑3.71%[0m) [0.56% of initial]
[Iter 4270/20000] Loss: 0.0013169 (Best: 0.0005893 @iter3898) ([92m↓5.88%[0m) [0.52% of initial]
[Iter 4280/20000] Loss: 0.0010863 (Best: 0.0005893 @iter3898) ([92m↓17.51%[0m) [0.43% of initial]
[Iter 4290/20000] Loss: 0.0010954 (Best: 0.0005893 @iter3898) ([91m↑0.84%[0m) [0.44% of initial]
Iter:4299, L1 loss=0.001303, Total loss=0.001249, Time:47
[Iter 4300/20000] Loss: 0.0010554 (Best: 0.0005893 @iter3898) ([92m↓3.65%[0m) [0.42% of initial]
[Iter 4310/20000] Loss: 0.0010129 (Best: 0.0005893 @iter3898) ([92m↓4.03%[0m) [0.40% of initial]
[Iter 4320/20000] Loss: 0.0011667 (Best: 0.0005893 @iter3898) ([91m↑15.18%[0m) [0.46% of initial]
[Iter 4330/20000] Loss: 0.0010109 (Best: 0.0005893 @iter3898) ([92m↓13.35%[0m) [0.40% of initial]
[Iter 4340/20000] Loss: 0.0010034 (Best: 0.0005893 @iter3898) ([92m↓0.74%[0m) [0.40% of initial]
[Iter 4350/20000] Loss: 0.0009884 (Best: 0.0005893 @iter3898) ([92m↓1.49%[0m) [0.39% of initial]
[Iter 4360/20000] Loss: 0.0009497 (Best: 0.0005893 @iter3898) ([92m↓3.92%[0m) [0.38% of initial]
[Iter 4370/20000] Loss: 0.0009979 (Best: 0.0005893 @iter3898) ([91m↑5.07%[0m) [0.40% of initial]
[Iter 4380/20000] Loss: 0.0010230 (Best: 0.0005893 @iter3898) ([91m↑2.52%[0m) [0.41% of initial]
[Iter 4390/20000] Loss: 0.0009515 (Best: 0.0005893 @iter3898) ([92m↓6.99%[0m) [0.38% of initial]
Iter:4399, L1 loss=0.0008477, Total loss=0.0008333, Time:45
[Iter 4400/20000] Loss: 0.0009355 (Best: 0.0005893 @iter3898) ([92m↓1.68%[0m) [0.37% of initial]
[Iter 4410/20000] Loss: 0.0022363 (Best: 0.0005893 @iter3898) ([91m↑139.04%[0m) [0.89% of initial]
[Iter 4420/20000] Loss: 0.0014923 (Best: 0.0005893 @iter3898) ([92m↓33.27%[0m) [0.59% of initial]
[Iter 4430/20000] Loss: 0.0012770 (Best: 0.0005893 @iter3898) ([92m↓14.42%[0m) [0.51% of initial]
[Iter 4440/20000] Loss: 0.0010866 (Best: 0.0005893 @iter3898) ([92m↓14.91%[0m) [0.43% of initial]
[Iter 4450/20000] Loss: 0.0009849 (Best: 0.0005893 @iter3898) ([92m↓9.35%[0m) [0.39% of initial]
[Iter 4460/20000] Loss: 0.0009724 (Best: 0.0005893 @iter3898) ([92m↓1.27%[0m) [0.39% of initial]
[Iter 4470/20000] Loss: 0.0010854 (Best: 0.0005893 @iter3898) ([91m↑11.62%[0m) [0.43% of initial]
[Iter 4480/20000] Loss: 0.0010389 (Best: 0.0005893 @iter3898) ([92m↓4.29%[0m) [0.41% of initial]
[Iter 4490/20000] Loss: 0.0010052 (Best: 0.0005893 @iter3898) ([92m↓3.24%[0m) [0.40% of initial]
Iter:4499, L1 loss=0.001083, Total loss=0.001031, Time:47
[Iter 4500/20000] Loss: 0.0011270 (Best: 0.0005893 @iter3898) ([91m↑12.12%[0m) [0.45% of initial]
Pruning 305 points (0.2%) from gaussian0 at iteration 4500
Pruning 340 points (0.2%) from gaussian1 at iteration 4500
[Iter 4510/20000] Loss: 0.0018741 (Best: 0.0005893 @iter3898) ([91m↑66.29%[0m) [0.74% of initial]
[Iter 4520/20000] Loss: 0.0015112 (Best: 0.0005893 @iter3898) ([92m↓19.36%[0m) [0.60% of initial]
[Iter 4530/20000] Loss: 0.0014454 (Best: 0.0005893 @iter3898) ([92m↓4.35%[0m) [0.57% of initial]
[Iter 4540/20000] Loss: 0.0011763 (Best: 0.0005893 @iter3898) ([92m↓18.62%[0m) [0.47% of initial]
[Iter 4550/20000] Loss: 0.0010320 (Best: 0.0005893 @iter3898) ([92m↓12.26%[0m) [0.41% of initial]
[Iter 4560/20000] Loss: 0.0010342 (Best: 0.0005893 @iter3898) ([91m↑0.21%[0m) [0.41% of initial]
[Iter 4570/20000] Loss: 0.0009000 (Best: 0.0005893 @iter3898) ([92m↓12.98%[0m) [0.36% of initial]
[Iter 4580/20000] Loss: 0.0008892 (Best: 0.0005893 @iter3898) ([92m↓1.19%[0m) [0.35% of initial]
[Iter 4590/20000] Loss: 0.0009775 (Best: 0.0005893 @iter3898) ([91m↑9.92%[0m) [0.39% of initial]
Iter:4599, L1 loss=0.0009708, Total loss=0.0009429, Time:45
[Iter 4600/20000] Loss: 0.0009146 (Best: 0.0005893 @iter3898) ([92m↓6.43%[0m) [0.36% of initial]
[Iter 4610/20000] Loss: 0.0024649 (Best: 0.0005893 @iter3898) ([91m↑169.51%[0m) [0.98% of initial]
[Iter 4620/20000] Loss: 0.0017129 (Best: 0.0005893 @iter3898) ([92m↓30.51%[0m) [0.68% of initial]
[Iter 4630/20000] Loss: 0.0012168 (Best: 0.0005893 @iter3898) ([92m↓28.96%[0m) [0.48% of initial]
[Iter 4640/20000] Loss: 0.0010630 (Best: 0.0005893 @iter3898) ([92m↓12.65%[0m) [0.42% of initial]
[Iter 4650/20000] Loss: 0.0009742 (Best: 0.0005893 @iter3898) ([92m↓8.35%[0m) [0.39% of initial]
[Iter 4660/20000] Loss: 0.0008604 (Best: 0.0005893 @iter3898) ([92m↓11.68%[0m) [0.34% of initial]
[Iter 4670/20000] Loss: 0.0008477 (Best: 0.0005893 @iter3898) ([92m↓1.48%[0m) [0.34% of initial]
[Iter 4680/20000] Loss: 0.0008326 (Best: 0.0005893 @iter3898) ([92m↓1.78%[0m) [0.33% of initial]
[Iter 4690/20000] Loss: 0.0008157 (Best: 0.0005893 @iter3898) ([92m↓2.03%[0m) [0.32% of initial]
Iter:4699, L1 loss=0.0009038, Total loss=0.0008452, Time:48
[Iter 4700/20000] Loss: 0.0008767 (Best: 0.0005893 @iter3898) ([91m↑7.48%[0m) [0.35% of initial]
[Iter 4710/20000] Loss: 0.0007878 (Best: 0.0005893 @iter3898) ([92m↓10.13%[0m) [0.31% of initial]
[Iter 4720/20000] Loss: 0.0008575 (Best: 0.0005893 @iter3898) ([91m↑8.84%[0m) [0.34% of initial]
[Iter 4730/20000] Loss: 0.0008398 (Best: 0.0005893 @iter3898) ([92m↓2.06%[0m) [0.33% of initial]
[Iter 4740/20000] Loss: 0.0009479 (Best: 0.0005893 @iter3898) ([91m↑12.87%[0m) [0.38% of initial]
[Iter 4750/20000] Loss: 0.0009089 (Best: 0.0005893 @iter3898) ([92m↓4.12%[0m) [0.36% of initial]
[Iter 4760/20000] Loss: 0.0008425 (Best: 0.0005893 @iter3898) ([92m↓7.30%[0m) [0.33% of initial]
[Iter 4770/20000] Loss: 0.0008905 (Best: 0.0005893 @iter3898) ([91m↑5.69%[0m) [0.35% of initial]
[Iter 4780/20000] Loss: 0.0009489 (Best: 0.0005893 @iter3898) ([91m↑6.56%[0m) [0.38% of initial]
[Iter 4790/20000] Loss: 0.0008507 (Best: 0.0005893 @iter3898) ([92m↓10.34%[0m) [0.34% of initial]
Iter:4799, L1 loss=0.0008695, Total loss=0.0008421, Time:45
[Iter 4800/20000] Loss: 0.0009692 (Best: 0.0005893 @iter3898) ([91m↑13.93%[0m) [0.39% of initial]
[Iter 4810/20000] Loss: 0.0019592 (Best: 0.0005893 @iter3898) ([91m↑102.14%[0m) [0.78% of initial]
[Iter 4820/20000] Loss: 0.0015034 (Best: 0.0005893 @iter3898) ([92m↓23.27%[0m) [0.60% of initial]
[Iter 4830/20000] Loss: 0.0012207 (Best: 0.0005893 @iter3898) ([92m↓18.80%[0m) [0.48% of initial]
[Iter 4840/20000] Loss: 0.0009605 (Best: 0.0005893 @iter3898) ([92m↓21.32%[0m) [0.38% of initial]
[Iter 4850/20000] Loss: 0.0008235 (Best: 0.0005893 @iter3898) ([92m↓14.27%[0m) [0.33% of initial]
[Iter 4860/20000] Loss: 0.0008835 (Best: 0.0005893 @iter3898) ([91m↑7.29%[0m) [0.35% of initial]
[Iter 4870/20000] Loss: 0.0007905 (Best: 0.0005893 @iter3898) ([92m↓10.52%[0m) [0.31% of initial]
[Iter 4880/20000] Loss: 0.0008480 (Best: 0.0005893 @iter3898) ([91m↑7.27%[0m) [0.34% of initial]
[Iter 4890/20000] Loss: 0.0007971 (Best: 0.0005893 @iter3898) ([92m↓6.01%[0m) [0.32% of initial]
Iter:4899, L1 loss=0.0008861, Total loss=0.0008623, Time:45
[Iter 4900/20000] Loss: 0.0007813 (Best: 0.0005893 @iter3898) ([92m↓1.98%[0m) [0.31% of initial]
[Iter 4910/20000] Loss: 0.0009760 (Best: 0.0005893 @iter3898) ([91m↑24.93%[0m) [0.39% of initial]
[Iter 4920/20000] Loss: 0.0008369 (Best: 0.0005893 @iter3898) ([92m↓14.25%[0m) [0.33% of initial]
[Iter 4930/20000] Loss: 0.0007474 (Best: 0.0005893 @iter3898) ([92m↓10.69%[0m) [0.30% of initial]
[Iter 4940/20000] Loss: 0.0007870 (Best: 0.0005893 @iter3898) ([91m↑5.30%[0m) [0.31% of initial]
[Iter 4950/20000] Loss: 0.0006894 (Best: 0.0005893 @iter3898) ([92m↓12.40%[0m) [0.27% of initial]
[Iter 4960/20000] Loss: 0.0007212 (Best: 0.0005893 @iter3898) ([91m↑4.62%[0m) [0.29% of initial]
[Iter 4970/20000] Loss: 0.0007611 (Best: 0.0005893 @iter3898) ([91m↑5.53%[0m) [0.30% of initial]
[Iter 4980/20000] Loss: 0.0007873 (Best: 0.0005893 @iter3898) ([91m↑3.44%[0m) [0.31% of initial]
[Iter 4990/20000] Loss: 0.0007197 (Best: 0.0005893 @iter3898) ([92m↓8.59%[0m) [0.29% of initial]
Iter:4999, L1 loss=0.0007129, Total loss=0.00068, Time:47
[Iter 5000/20000] Loss: 0.0006755 (Best: 0.0005893 @iter3898) ([92m↓6.14%[0m) [0.27% of initial]
Pruning 169 points (0.1%) from gaussian0 at iteration 5000
Pruning 173 points (0.1%) from gaussian1 at iteration 5000
[Iter 5010/20000] Loss: 0.0022434 (Best: 0.0005893 @iter3898) ([91m↑232.11%[0m) [0.89% of initial]
[Iter 5020/20000] Loss: 0.0015752 (Best: 0.0005893 @iter3898) ([92m↓29.79%[0m) [0.63% of initial]
[Iter 5030/20000] Loss: 0.0011085 (Best: 0.0005893 @iter3898) ([92m↓29.63%[0m) [0.44% of initial]
[Iter 5040/20000] Loss: 0.0010944 (Best: 0.0005893 @iter3898) ([92m↓1.27%[0m) [0.43% of initial]
[Iter 5050/20000] Loss: 0.0009482 (Best: 0.0005893 @iter3898) ([92m↓13.35%[0m) [0.38% of initial]
[Iter 5060/20000] Loss: 0.0007753 (Best: 0.0005893 @iter3898) ([92m↓18.24%[0m) [0.31% of initial]
[Iter 5070/20000] Loss: 0.0008414 (Best: 0.0005893 @iter3898) ([91m↑8.53%[0m) [0.33% of initial]
[Iter 5080/20000] Loss: 0.0007346 (Best: 0.0005893 @iter3898) ([92m↓12.69%[0m) [0.29% of initial]
[Iter 5090/20000] Loss: 0.0007773 (Best: 0.0005893 @iter3898) ([91m↑5.81%[0m) [0.31% of initial]
Iter:5099, L1 loss=0.0008231, Total loss=0.0007675, Time:53
[Iter 5100/20000] Loss: 0.0008280 (Best: 0.0005893 @iter3898) ([91m↑6.52%[0m) [0.33% of initial]
[Iter 5110/20000] Loss: 0.0008013 (Best: 0.0005893 @iter3898) ([92m↓3.23%[0m) [0.32% of initial]
[Iter 5120/20000] Loss: 0.0008036 (Best: 0.0005893 @iter3898) ([91m↑0.29%[0m) [0.32% of initial]
[Iter 5130/20000] Loss: 0.0008183 (Best: 0.0005893 @iter3898) ([91m↑1.83%[0m) [0.33% of initial]
[Iter 5140/20000] Loss: 0.0007095 (Best: 0.0005893 @iter3898) ([92m↓13.29%[0m) [0.28% of initial]
[Iter 5150/20000] Loss: 0.0007272 (Best: 0.0005893 @iter3898) ([91m↑2.49%[0m) [0.29% of initial]
[Iter 5160/20000] Loss: 0.0007218 (Best: 0.0005893 @iter3898) ([92m↓0.74%[0m) [0.29% of initial]
[Iter 5170/20000] Loss: 0.0007717 (Best: 0.0005893 @iter3898) ([91m↑6.91%[0m) [0.31% of initial]
[Iter 5180/20000] Loss: 0.0007168 (Best: 0.0005893 @iter3898) ([92m↓7.12%[0m) [0.28% of initial]
[Iter 5190/20000] Loss: 0.0007328 (Best: 0.0005893 @iter3898) ([91m↑2.24%[0m) [0.29% of initial]
Iter:5199, L1 loss=0.0007879, Total loss=0.0007599, Time:50
[Iter 5200/20000] Loss: 0.0007058 (Best: 0.0005893 @iter3898) ([92m↓3.70%[0m) [0.28% of initial]
[Iter 5210/20000] Loss: 0.0019077 (Best: 0.0005893 @iter3898) ([91m↑170.30%[0m) [0.76% of initial]
[Iter 5220/20000] Loss: 0.0013919 (Best: 0.0005893 @iter3898) ([92m↓27.04%[0m) [0.55% of initial]
[Iter 5230/20000] Loss: 0.0010224 (Best: 0.0005893 @iter3898) ([92m↓26.55%[0m) [0.41% of initial]
[Iter 5240/20000] Loss: 0.0008042 (Best: 0.0005893 @iter3898) ([92m↓21.34%[0m) [0.32% of initial]
[Iter 5250/20000] Loss: 0.0011098 (Best: 0.0005893 @iter3898) ([91m↑38.01%[0m) [0.44% of initial]
[Iter 5260/20000] Loss: 0.0008808 (Best: 0.0005893 @iter3898) ([92m↓20.63%[0m) [0.35% of initial]
[Iter 5270/20000] Loss: 0.0008038 (Best: 0.0005893 @iter3898) ([92m↓8.74%[0m) [0.32% of initial]
[Iter 5280/20000] Loss: 0.0008816 (Best: 0.0005893 @iter3898) ([91m↑9.67%[0m) [0.35% of initial]
[Iter 5290/20000] Loss: 0.0008300 (Best: 0.0005893 @iter3898) ([92m↓5.84%[0m) [0.33% of initial]
Iter:5299, L1 loss=0.0008955, Total loss=0.0008469, Time:46
[Iter 5300/20000] Loss: 0.0009130 (Best: 0.0005893 @iter3898) ([91m↑10.00%[0m) [0.36% of initial]
[Iter 5310/20000] Loss: 0.0009025 (Best: 0.0005893 @iter3898) ([92m↓1.15%[0m) [0.36% of initial]
[Iter 5320/20000] Loss: 0.0007976 (Best: 0.0005893 @iter3898) ([92m↓11.63%[0m) [0.32% of initial]
[Iter 5330/20000] Loss: 0.0006619 (Best: 0.0005893 @iter3898) ([92m↓17.01%[0m) [0.26% of initial]
[Iter 5340/20000] Loss: 0.0007039 (Best: 0.0005893 @iter3898) ([91m↑6.35%[0m) [0.28% of initial]
[Iter 5350/20000] Loss: 0.0007277 (Best: 0.0005669 @iter5344) ([91m↑3.37%[0m) [0.29% of initial]
[Iter 5360/20000] Loss: 0.0007229 (Best: 0.0005669 @iter5344) ([92m↓0.66%[0m) [0.29% of initial]
[Iter 5370/20000] Loss: 0.0007575 (Best: 0.0005669 @iter5344) ([91m↑4.80%[0m) [0.30% of initial]
[Iter 5380/20000] Loss: 0.0007367 (Best: 0.0005669 @iter5344) ([92m↓2.75%[0m) [0.29% of initial]
[Iter 5390/20000] Loss: 0.0007408 (Best: 0.0005669 @iter5344) ([91m↑0.56%[0m) [0.29% of initial]
Iter:5399, L1 loss=0.0006828, Total loss=0.0006596, Time:44
[Iter 5400/20000] Loss: 0.0007412 (Best: 0.0005669 @iter5344) ([91m↑0.05%[0m) [0.29% of initial]
[Iter 5410/20000] Loss: 0.0015317 (Best: 0.0005669 @iter5344) ([91m↑106.66%[0m) [0.61% of initial]
[Iter 5420/20000] Loss: 0.0012944 (Best: 0.0005669 @iter5344) ([92m↓15.49%[0m) [0.51% of initial]
[Iter 5430/20000] Loss: 0.0008874 (Best: 0.0005669 @iter5344) ([92m↓31.44%[0m) [0.35% of initial]
[Iter 5440/20000] Loss: 0.0008234 (Best: 0.0005669 @iter5344) ([92m↓7.21%[0m) [0.33% of initial]
[Iter 5450/20000] Loss: 0.0007029 (Best: 0.0005669 @iter5344) ([92m↓14.63%[0m) [0.28% of initial]
[Iter 5460/20000] Loss: 0.0007784 (Best: 0.0005669 @iter5344) ([91m↑10.73%[0m) [0.31% of initial]
[Iter 5470/20000] Loss: 0.0006740 (Best: 0.0005669 @iter5344) ([92m↓13.41%[0m) [0.27% of initial]
[Iter 5480/20000] Loss: 0.0006394 (Best: 0.0005411 @iter5476) ([92m↓5.14%[0m) [0.25% of initial]
[Iter 5490/20000] Loss: 0.0006649 (Best: 0.0005411 @iter5476) ([91m↑3.99%[0m) [0.26% of initial]
Iter:5499, L1 loss=0.000712, Total loss=0.0006783, Time:49
[Iter 5500/20000] Loss: 0.0006256 (Best: 0.0005411 @iter5476) ([92m↓5.91%[0m) [0.25% of initial]
Pruning 90 points (0.0%) from gaussian0 at iteration 5500
Pruning 111 points (0.1%) from gaussian1 at iteration 5500
[Iter 5510/20000] Loss: 0.0011625 (Best: 0.0005411 @iter5476) ([91m↑85.83%[0m) [0.46% of initial]
[Iter 5520/20000] Loss: 0.0008885 (Best: 0.0005411 @iter5476) ([92m↓23.57%[0m) [0.35% of initial]
[Iter 5530/20000] Loss: 0.0008250 (Best: 0.0005411 @iter5476) ([92m↓7.15%[0m) [0.33% of initial]
[Iter 5540/20000] Loss: 0.0007630 (Best: 0.0005411 @iter5476) ([92m↓7.52%[0m) [0.30% of initial]
[Iter 5550/20000] Loss: 0.0007237 (Best: 0.0005411 @iter5476) ([92m↓5.15%[0m) [0.29% of initial]
[Iter 5560/20000] Loss: 0.0006744 (Best: 0.0005411 @iter5476) ([92m↓6.81%[0m) [0.27% of initial]
[Iter 5570/20000] Loss: 0.0006707 (Best: 0.0005411 @iter5476) ([92m↓0.56%[0m) [0.27% of initial]
[Iter 5580/20000] Loss: 0.0007783 (Best: 0.0005411 @iter5476) ([91m↑16.04%[0m) [0.31% of initial]
[Iter 5590/20000] Loss: 0.0008205 (Best: 0.0005411 @iter5476) ([91m↑5.42%[0m) [0.33% of initial]
Iter:5599, L1 loss=0.0006535, Total loss=0.0006218, Time:45
[Iter 5600/20000] Loss: 0.0006420 (Best: 0.0005411 @iter5476) ([92m↓21.76%[0m) [0.26% of initial]
[Iter 5610/20000] Loss: 0.0017242 (Best: 0.0005411 @iter5476) ([91m↑168.59%[0m) [0.69% of initial]
[Iter 5620/20000] Loss: 0.0010798 (Best: 0.0005411 @iter5476) ([92m↓37.37%[0m) [0.43% of initial]
[Iter 5630/20000] Loss: 0.0008919 (Best: 0.0005411 @iter5476) ([92m↓17.40%[0m) [0.35% of initial]
[Iter 5640/20000] Loss: 0.0008263 (Best: 0.0005411 @iter5476) ([92m↓7.35%[0m) [0.33% of initial]
[Iter 5650/20000] Loss: 0.0007010 (Best: 0.0005411 @iter5476) ([92m↓15.17%[0m) [0.28% of initial]
[Iter 5660/20000] Loss: 0.0006971 (Best: 0.0005411 @iter5476) ([92m↓0.55%[0m) [0.28% of initial]
[Iter 5670/20000] Loss: 0.0007007 (Best: 0.0005411 @iter5476) ([91m↑0.51%[0m) [0.28% of initial]
[Iter 5680/20000] Loss: 0.0006421 (Best: 0.0005411 @iter5476) ([92m↓8.37%[0m) [0.26% of initial]
[Iter 5690/20000] Loss: 0.0006927 (Best: 0.0005411 @iter5476) ([91m↑7.89%[0m) [0.28% of initial]
Iter:5699, L1 loss=0.0006601, Total loss=0.0006109, Time:49
[Iter 5700/20000] Loss: 0.0005998 (Best: 0.0005411 @iter5476) ([92m↓13.42%[0m) [0.24% of initial]
[Iter 5710/20000] Loss: 0.0007573 (Best: 0.0005273 @iter5701) ([91m↑26.27%[0m) [0.30% of initial]
[Iter 5720/20000] Loss: 0.0006786 (Best: 0.0005273 @iter5701) ([92m↓10.39%[0m) [0.27% of initial]
[Iter 5730/20000] Loss: 0.0006995 (Best: 0.0005273 @iter5701) ([91m↑3.08%[0m) [0.28% of initial]
[Iter 5740/20000] Loss: 0.0005895 (Best: 0.0005273 @iter5701) ([92m↓15.73%[0m) [0.23% of initial]
[Iter 5750/20000] Loss: 0.0006043 (Best: 0.0005273 @iter5701) ([91m↑2.50%[0m) [0.24% of initial]
[Iter 5760/20000] Loss: 0.0006093 (Best: 0.0005273 @iter5701) ([91m↑0.83%[0m) [0.24% of initial]
[Iter 5770/20000] Loss: 0.0005415 (Best: 0.0004907 @iter5770) ([92m↓11.13%[0m) [0.22% of initial]
[Iter 5780/20000] Loss: 0.0005630 (Best: 0.0004907 @iter5770) ([91m↑3.97%[0m) [0.22% of initial]
[Iter 5790/20000] Loss: 0.0006044 (Best: 0.0004907 @iter5770) ([91m↑7.36%[0m) [0.24% of initial]
Iter:5799, L1 loss=0.0006049, Total loss=0.0005769, Time:46
[Iter 5800/20000] Loss: 0.0005710 (Best: 0.0004636 @iter5797) ([92m↓5.53%[0m) [0.23% of initial]
[Iter 5810/20000] Loss: 0.0012870 (Best: 0.0004636 @iter5797) ([91m↑125.41%[0m) [0.51% of initial]
[Iter 5820/20000] Loss: 0.0010322 (Best: 0.0004636 @iter5797) ([92m↓19.80%[0m) [0.41% of initial]
[Iter 5830/20000] Loss: 0.0008007 (Best: 0.0004636 @iter5797) ([92m↓22.43%[0m) [0.32% of initial]
[Iter 5840/20000] Loss: 0.0007060 (Best: 0.0004636 @iter5797) ([92m↓11.82%[0m) [0.28% of initial]
[Iter 5850/20000] Loss: 0.0007289 (Best: 0.0004636 @iter5797) ([91m↑3.24%[0m) [0.29% of initial]
[Iter 5860/20000] Loss: 0.0006050 (Best: 0.0004636 @iter5797) ([92m↓17.01%[0m) [0.24% of initial]
[Iter 5870/20000] Loss: 0.0006936 (Best: 0.0004636 @iter5797) ([91m↑14.66%[0m) [0.28% of initial]
[Iter 5880/20000] Loss: 0.0006495 (Best: 0.0004636 @iter5797) ([92m↓6.36%[0m) [0.26% of initial]
[Iter 5890/20000] Loss: 0.0006081 (Best: 0.0004636 @iter5797) ([92m↓6.38%[0m) [0.24% of initial]
Iter:5899, L1 loss=0.000615, Total loss=0.0005772, Time:63
[Iter 5900/20000] Loss: 0.0005901 (Best: 0.0004636 @iter5797) ([92m↓2.96%[0m) [0.23% of initial]
[Iter 5910/20000] Loss: 0.0006280 (Best: 0.0004636 @iter5797) ([91m↑6.43%[0m) [0.25% of initial]
[Iter 5920/20000] Loss: 0.0005960 (Best: 0.0004636 @iter5797) ([92m↓5.10%[0m) [0.24% of initial]
[Iter 5930/20000] Loss: 0.0005069 (Best: 0.0004636 @iter5797) ([92m↓14.94%[0m) [0.20% of initial]
[Iter 5940/20000] Loss: 0.0006668 (Best: 0.0004636 @iter5797) ([91m↑31.54%[0m) [0.26% of initial]
[Iter 5950/20000] Loss: 0.0005495 (Best: 0.0004636 @iter5797) ([92m↓17.59%[0m) [0.22% of initial]
[Iter 5960/20000] Loss: 0.0005904 (Best: 0.0004636 @iter5797) ([91m↑7.45%[0m) [0.23% of initial]
[Iter 5970/20000] Loss: 0.0006967 (Best: 0.0004636 @iter5797) ([91m↑17.99%[0m) [0.28% of initial]
[Iter 5980/20000] Loss: 0.0006166 (Best: 0.0004636 @iter5797) ([92m↓11.50%[0m) [0.24% of initial]
[Iter 5990/20000] Loss: 0.0006217 (Best: 0.0004636 @iter5797) ([91m↑0.83%[0m) [0.25% of initial]
Iter:5999, L1 loss=0.0006657, Total loss=0.0006314, Time:45
[Iter 6000/20000] Loss: 0.0006280 (Best: 0.0004636 @iter5797) ([91m↑1.01%[0m) [0.25% of initial]
Pruning 87 points (0.0%) from gaussian0 at iteration 6000
Pruning 75 points (0.0%) from gaussian1 at iteration 6000
[Iter 6010/20000] Loss: 0.0019937 (Best: 0.0004636 @iter5797) ([91m↑217.46%[0m) [0.79% of initial]
[Iter 6020/20000] Loss: 0.0012910 (Best: 0.0004636 @iter5797) ([92m↓35.25%[0m) [0.51% of initial]
[Iter 6030/20000] Loss: 0.0008820 (Best: 0.0004636 @iter5797) ([92m↓31.69%[0m) [0.35% of initial]
[Iter 6040/20000] Loss: 0.0006916 (Best: 0.0004636 @iter5797) ([92m↓21.58%[0m) [0.27% of initial]
[Iter 6050/20000] Loss: 0.0006242 (Best: 0.0004636 @iter5797) ([92m↓9.74%[0m) [0.25% of initial]
[Iter 6060/20000] Loss: 0.0006624 (Best: 0.0004636 @iter5797) ([91m↑6.11%[0m) [0.26% of initial]
[Iter 6070/20000] Loss: 0.0006450 (Best: 0.0004636 @iter5797) ([92m↓2.63%[0m) [0.26% of initial]
[Iter 6080/20000] Loss: 0.0006430 (Best: 0.0004636 @iter5797) ([92m↓0.30%[0m) [0.26% of initial]
[Iter 6090/20000] Loss: 0.0005856 (Best: 0.0004636 @iter5797) ([92m↓8.93%[0m) [0.23% of initial]
Iter:6099, L1 loss=0.0005932, Total loss=0.0005514, Time:47
[Iter 6100/20000] Loss: 0.0005850 (Best: 0.0004636 @iter5797) ([92m↓0.09%[0m) [0.23% of initial]
[Iter 6110/20000] Loss: 0.0005718 (Best: 0.0004636 @iter5797) ([92m↓2.26%[0m) [0.23% of initial]
[Iter 6120/20000] Loss: 0.0006419 (Best: 0.0004636 @iter5797) ([91m↑12.26%[0m) [0.26% of initial]
[Iter 6130/20000] Loss: 0.0006138 (Best: 0.0004636 @iter5797) ([92m↓4.37%[0m) [0.24% of initial]
[Iter 6140/20000] Loss: 0.0005259 (Best: 0.0004636 @iter5797) ([92m↓14.33%[0m) [0.21% of initial]
[Iter 6150/20000] Loss: 0.0005527 (Best: 0.0004636 @iter5797) ([91m↑5.11%[0m) [0.22% of initial]
[Iter 6160/20000] Loss: 0.0005346 (Best: 0.0004636 @iter5797) ([92m↓3.27%[0m) [0.21% of initial]
[Iter 6170/20000] Loss: 0.0005613 (Best: 0.0004636 @iter5797) ([91m↑4.99%[0m) [0.22% of initial]
[Iter 6180/20000] Loss: 0.0006293 (Best: 0.0004636 @iter5797) ([91m↑12.11%[0m) [0.25% of initial]
[Iter 6190/20000] Loss: 0.0005344 (Best: 0.0004636 @iter5797) ([92m↓15.08%[0m) [0.21% of initial]
Iter:6199, L1 loss=0.0005271, Total loss=0.0004854, Time:48
[Iter 6200/20000] Loss: 0.0005400 (Best: 0.0004636 @iter5797) ([91m↑1.05%[0m) [0.21% of initial]
[Iter 6210/20000] Loss: 0.0012952 (Best: 0.0004636 @iter5797) ([91m↑139.86%[0m) [0.51% of initial]
[Iter 6220/20000] Loss: 0.0009411 (Best: 0.0004636 @iter5797) ([92m↓27.34%[0m) [0.37% of initial]
[Iter 6230/20000] Loss: 0.0008393 (Best: 0.0004636 @iter5797) ([92m↓10.81%[0m) [0.33% of initial]
[Iter 6240/20000] Loss: 0.0006827 (Best: 0.0004636 @iter5797) ([92m↓18.66%[0m) [0.27% of initial]
[Iter 6250/20000] Loss: 0.0006082 (Best: 0.0004636 @iter5797) ([92m↓10.90%[0m) [0.24% of initial]
[Iter 6260/20000] Loss: 0.0007019 (Best: 0.0004636 @iter5797) ([91m↑15.40%[0m) [0.28% of initial]
[Iter 6270/20000] Loss: 0.0006226 (Best: 0.0004636 @iter5797) ([92m↓11.30%[0m) [0.25% of initial]
[Iter 6280/20000] Loss: 0.0006626 (Best: 0.0004636 @iter5797) ([91m↑6.42%[0m) [0.26% of initial]
[Iter 6290/20000] Loss: 0.0005930 (Best: 0.0004636 @iter5797) ([92m↓10.50%[0m) [0.24% of initial]
Iter:6299, L1 loss=0.0007357, Total loss=0.0006753, Time:47
[Iter 6300/20000] Loss: 0.0007885 (Best: 0.0004636 @iter5797) ([91m↑32.97%[0m) [0.31% of initial]
[Iter 6310/20000] Loss: 0.0006745 (Best: 0.0004636 @iter5797) ([92m↓14.46%[0m) [0.27% of initial]
[Iter 6320/20000] Loss: 0.0006674 (Best: 0.0004636 @iter5797) ([92m↓1.05%[0m) [0.27% of initial]
[Iter 6330/20000] Loss: 0.0006346 (Best: 0.0004636 @iter5797) ([92m↓4.92%[0m) [0.25% of initial]
[Iter 6340/20000] Loss: 0.0005132 (Best: 0.0004580 @iter6340) ([92m↓19.13%[0m) [0.20% of initial]
[Iter 6350/20000] Loss: 0.0005234 (Best: 0.0004580 @iter6340) ([91m↑1.99%[0m) [0.21% of initial]
[Iter 6360/20000] Loss: 0.0005989 (Best: 0.0004580 @iter6340) ([91m↑14.43%[0m) [0.24% of initial]
[Iter 6370/20000] Loss: 0.0005790 (Best: 0.0004571 @iter6364) ([92m↓3.33%[0m) [0.23% of initial]
[Iter 6380/20000] Loss: 0.0005245 (Best: 0.0004571 @iter6364) ([92m↓9.40%[0m) [0.21% of initial]
[Iter 6390/20000] Loss: 0.0005524 (Best: 0.0004525 @iter6382) ([91m↑5.32%[0m) [0.22% of initial]
Iter:6399, L1 loss=0.0005685, Total loss=0.000524, Time:50
[Iter 6400/20000] Loss: 0.0005780 (Best: 0.0004525 @iter6382) ([91m↑4.63%[0m) [0.23% of initial]
[Iter 6410/20000] Loss: 0.0012825 (Best: 0.0004525 @iter6382) ([91m↑121.89%[0m) [0.51% of initial]
[Iter 6420/20000] Loss: 0.0010541 (Best: 0.0004525 @iter6382) ([92m↓17.81%[0m) [0.42% of initial]
[Iter 6430/20000] Loss: 0.0009096 (Best: 0.0004525 @iter6382) ([92m↓13.71%[0m) [0.36% of initial]
[Iter 6440/20000] Loss: 0.0007543 (Best: 0.0004525 @iter6382) ([92m↓17.08%[0m) [0.30% of initial]
[Iter 6450/20000] Loss: 0.0006857 (Best: 0.0004525 @iter6382) ([92m↓9.09%[0m) [0.27% of initial]
[Iter 6460/20000] Loss: 0.0005871 (Best: 0.0004525 @iter6382) ([92m↓14.38%[0m) [0.23% of initial]
[Iter 6470/20000] Loss: 0.0005759 (Best: 0.0004525 @iter6382) ([92m↓1.91%[0m) [0.23% of initial]
[Iter 6480/20000] Loss: 0.0005767 (Best: 0.0004525 @iter6382) ([91m↑0.13%[0m) [0.23% of initial]
[Iter 6490/20000] Loss: 0.0005271 (Best: 0.0004525 @iter6382) ([92m↓8.59%[0m) [0.21% of initial]
Iter:6499, L1 loss=0.0006453, Total loss=0.0006404, Time:47
[Iter 6500/20000] Loss: 0.0006148 (Best: 0.0004525 @iter6382) ([91m↑16.63%[0m) [0.24% of initial]
Pruning 65 points (0.0%) from gaussian0 at iteration 6500
Pruning 77 points (0.0%) from gaussian1 at iteration 6500
[Iter 6510/20000] Loss: 0.0013307 (Best: 0.0004525 @iter6382) ([91m↑116.45%[0m) [0.53% of initial]
[Iter 6520/20000] Loss: 0.0008475 (Best: 0.0004525 @iter6382) ([92m↓36.31%[0m) [0.34% of initial]
[Iter 6530/20000] Loss: 0.0007727 (Best: 0.0004525 @iter6382) ([92m↓8.83%[0m) [0.31% of initial]
[Iter 6540/20000] Loss: 0.0006198 (Best: 0.0004525 @iter6382) ([92m↓19.79%[0m) [0.25% of initial]
[Iter 6550/20000] Loss: 0.0005630 (Best: 0.0004525 @iter6382) ([92m↓9.16%[0m) [0.22% of initial]
[Iter 6560/20000] Loss: 0.0004760 (Best: 0.0004479 @iter6559) ([92m↓15.45%[0m) [0.19% of initial]
[Iter 6570/20000] Loss: 0.0005741 (Best: 0.0004291 @iter6568) ([91m↑20.60%[0m) [0.23% of initial]
[Iter 6580/20000] Loss: 0.0005719 (Best: 0.0004291 @iter6568) ([92m↓0.37%[0m) [0.23% of initial]
[Iter 6590/20000] Loss: 0.0004833 (Best: 0.0004291 @iter6568) ([92m↓15.50%[0m) [0.19% of initial]
Iter:6599, L1 loss=0.0005443, Total loss=0.0004968, Time:48
[Iter 6600/20000] Loss: 0.0005183 (Best: 0.0004291 @iter6568) ([91m↑7.25%[0m) [0.21% of initial]
[Iter 6610/20000] Loss: 0.0012198 (Best: 0.0004291 @iter6568) ([91m↑135.32%[0m) [0.48% of initial]
[Iter 6620/20000] Loss: 0.0008631 (Best: 0.0004291 @iter6568) ([92m↓29.24%[0m) [0.34% of initial]
[Iter 6630/20000] Loss: 0.0006561 (Best: 0.0004291 @iter6568) ([92m↓23.99%[0m) [0.26% of initial]
[Iter 6640/20000] Loss: 0.0005486 (Best: 0.0004291 @iter6568) ([92m↓16.38%[0m) [0.22% of initial]
[Iter 6650/20000] Loss: 0.0005326 (Best: 0.0004291 @iter6568) ([92m↓2.92%[0m) [0.21% of initial]
[Iter 6660/20000] Loss: 0.0005998 (Best: 0.0004291 @iter6568) ([91m↑12.63%[0m) [0.24% of initial]
[Iter 6670/20000] Loss: 0.0005783 (Best: 0.0004291 @iter6568) ([92m↓3.59%[0m) [0.23% of initial]
[Iter 6680/20000] Loss: 0.0006007 (Best: 0.0004291 @iter6568) ([91m↑3.88%[0m) [0.24% of initial]
[Iter 6690/20000] Loss: 0.0005541 (Best: 0.0004291 @iter6568) ([92m↓7.76%[0m) [0.22% of initial]
Iter:6699, L1 loss=0.0006127, Total loss=0.0005619, Time:50
[Iter 6700/20000] Loss: 0.0005268 (Best: 0.0004291 @iter6568) ([92m↓4.93%[0m) [0.21% of initial]
[Iter 6710/20000] Loss: 0.0005757 (Best: 0.0004291 @iter6568) ([91m↑9.28%[0m) [0.23% of initial]
[Iter 6720/20000] Loss: 0.0005211 (Best: 0.0004291 @iter6568) ([92m↓9.48%[0m) [0.21% of initial]
[Iter 6730/20000] Loss: 0.0004686 (Best: 0.0004281 @iter6724) ([92m↓10.07%[0m) [0.19% of initial]
[Iter 6740/20000] Loss: 0.0004995 (Best: 0.0004267 @iter6731) ([91m↑6.58%[0m) [0.20% of initial]
[Iter 6750/20000] Loss: 0.0005180 (Best: 0.0004267 @iter6731) ([91m↑3.72%[0m) [0.21% of initial]
[Iter 6760/20000] Loss: 0.0004921 (Best: 0.0004267 @iter6731) ([92m↓5.01%[0m) [0.20% of initial]
[Iter 6770/20000] Loss: 0.0004797 (Best: 0.0004267 @iter6731) ([92m↓2.52%[0m) [0.19% of initial]
[Iter 6780/20000] Loss: 0.0004461 (Best: 0.0004217 @iter6778) ([92m↓7.00%[0m) [0.18% of initial]
[Iter 6790/20000] Loss: 0.0005169 (Best: 0.0003938 @iter6781) ([91m↑15.89%[0m) [0.21% of initial]
Iter:6799, L1 loss=0.000459, Total loss=0.0004292, Time:50
[Iter 6800/20000] Loss: 0.0004511 (Best: 0.0003938 @iter6781) ([92m↓12.75%[0m) [0.18% of initial]
[Iter 6810/20000] Loss: 0.0010815 (Best: 0.0003938 @iter6781) ([91m↑139.77%[0m) [0.43% of initial]
[Iter 6820/20000] Loss: 0.0008902 (Best: 0.0003938 @iter6781) ([92m↓17.69%[0m) [0.35% of initial]
[Iter 6830/20000] Loss: 0.0006628 (Best: 0.0003938 @iter6781) ([92m↓25.54%[0m) [0.26% of initial]
[Iter 6840/20000] Loss: 0.0005799 (Best: 0.0003938 @iter6781) ([92m↓12.52%[0m) [0.23% of initial]
[Iter 6850/20000] Loss: 0.0005601 (Best: 0.0003938 @iter6781) ([92m↓3.41%[0m) [0.22% of initial]
[Iter 6860/20000] Loss: 0.0005178 (Best: 0.0003938 @iter6781) ([92m↓7.56%[0m) [0.21% of initial]
[Iter 6870/20000] Loss: 0.0005035 (Best: 0.0003938 @iter6781) ([92m↓2.76%[0m) [0.20% of initial]
[Iter 6880/20000] Loss: 0.0004751 (Best: 0.0003938 @iter6781) ([92m↓5.64%[0m) [0.19% of initial]
[Iter 6890/20000] Loss: 0.0005606 (Best: 0.0003938 @iter6781) ([91m↑18.00%[0m) [0.22% of initial]
Iter:6899, L1 loss=0.0006201, Total loss=0.0005535, Time:53
[Iter 6900/20000] Loss: 0.0005544 (Best: 0.0003938 @iter6781) ([92m↓1.11%[0m) [0.22% of initial]
[Iter 6910/20000] Loss: 0.0005599 (Best: 0.0003938 @iter6781) ([91m↑1.00%[0m) [0.22% of initial]
[Iter 6920/20000] Loss: 0.0005061 (Best: 0.0003938 @iter6781) ([92m↓9.62%[0m) [0.20% of initial]
[Iter 6930/20000] Loss: 0.0005178 (Best: 0.0003938 @iter6781) ([91m↑2.32%[0m) [0.21% of initial]
[Iter 6940/20000] Loss: 0.0004580 (Best: 0.0003938 @iter6781) ([92m↓11.56%[0m) [0.18% of initial]
[Iter 6950/20000] Loss: 0.0004607 (Best: 0.0003938 @iter6781) ([91m↑0.59%[0m) [0.18% of initial]
[Iter 6960/20000] Loss: 0.0004383 (Best: 0.0003938 @iter6781) ([92m↓4.86%[0m) [0.17% of initial]
[Iter 6970/20000] Loss: 0.0005168 (Best: 0.0003869 @iter6961) ([91m↑17.89%[0m) [0.21% of initial]
[Iter 6980/20000] Loss: 0.0005227 (Best: 0.0003869 @iter6961) ([91m↑1.15%[0m) [0.21% of initial]
[Iter 6990/20000] Loss: 0.0004772 (Best: 0.0003869 @iter6961) ([92m↓8.70%[0m) [0.19% of initial]
Iter:6999, L1 loss=0.0005963, Total loss=0.0005725, Time:46
[Iter 7000/20000] Loss: 0.0004809 (Best: 0.0003869 @iter6961) ([91m↑0.78%[0m) [0.19% of initial]
Pruning 50 points (0.0%) from gaussian0 at iteration 7000
Pruning 71 points (0.0%) from gaussian1 at iteration 7000
[Iter 7010/20000] Loss: 0.0013936 (Best: 0.0003869 @iter6961) ([91m↑189.78%[0m) [0.55% of initial]
[Iter 7020/20000] Loss: 0.0009466 (Best: 0.0003869 @iter6961) ([92m↓32.08%[0m) [0.38% of initial]
[Iter 7030/20000] Loss: 0.0006798 (Best: 0.0003869 @iter6961) ([92m↓28.18%[0m) [0.27% of initial]
[Iter 7040/20000] Loss: 0.0005720 (Best: 0.0003869 @iter6961) ([92m↓15.86%[0m) [0.23% of initial]
[Iter 7050/20000] Loss: 0.0005390 (Best: 0.0003869 @iter6961) ([92m↓5.77%[0m) [0.21% of initial]
[Iter 7060/20000] Loss: 0.0005704 (Best: 0.0003869 @iter6961) ([91m↑5.82%[0m) [0.23% of initial]
[Iter 7070/20000] Loss: 0.0005135 (Best: 0.0003869 @iter6961) ([92m↓9.97%[0m) [0.20% of initial]
[Iter 7080/20000] Loss: 0.0005249 (Best: 0.0003869 @iter6961) ([91m↑2.22%[0m) [0.21% of initial]
[Iter 7090/20000] Loss: 0.0005500 (Best: 0.0003869 @iter6961) ([91m↑4.77%[0m) [0.22% of initial]
Iter:7099, L1 loss=0.0005262, Total loss=0.0004664, Time:46
[Iter 7100/20000] Loss: 0.0005001 (Best: 0.0003869 @iter6961) ([92m↓9.06%[0m) [0.20% of initial]
[Iter 7110/20000] Loss: 0.0005138 (Best: 0.0003869 @iter6961) ([91m↑2.72%[0m) [0.20% of initial]
[Iter 7120/20000] Loss: 0.0004745 (Best: 0.0003869 @iter6961) ([92m↓7.65%[0m) [0.19% of initial]
[Iter 7130/20000] Loss: 0.0004574 (Best: 0.0003869 @iter6961) ([92m↓3.59%[0m) [0.18% of initial]
[Iter 7140/20000] Loss: 0.0005380 (Best: 0.0003869 @iter6961) ([91m↑17.62%[0m) [0.21% of initial]
[Iter 7150/20000] Loss: 0.0004866 (Best: 0.0003869 @iter6961) ([92m↓9.56%[0m) [0.19% of initial]
[Iter 7160/20000] Loss: 0.0004872 (Best: 0.0003869 @iter6961) ([91m↑0.14%[0m) [0.19% of initial]
[Iter 7170/20000] Loss: 0.0005008 (Best: 0.0003869 @iter6961) ([91m↑2.78%[0m) [0.20% of initial]
[Iter 7180/20000] Loss: 0.0005095 (Best: 0.0003869 @iter6961) ([91m↑1.75%[0m) [0.20% of initial]
[Iter 7190/20000] Loss: 0.0004401 (Best: 0.0003869 @iter6961) ([92m↓13.62%[0m) [0.17% of initial]
Iter:7199, L1 loss=0.0004851, Total loss=0.0004594, Time:46
[Iter 7200/20000] Loss: 0.0004657 (Best: 0.0003869 @iter6961) ([91m↑5.82%[0m) [0.19% of initial]
[Iter 7210/20000] Loss: 0.0011658 (Best: 0.0003869 @iter6961) ([91m↑150.32%[0m) [0.46% of initial]
[Iter 7220/20000] Loss: 0.0009502 (Best: 0.0003869 @iter6961) ([92m↓18.50%[0m) [0.38% of initial]
[Iter 7230/20000] Loss: 0.0007286 (Best: 0.0003869 @iter6961) ([92m↓23.31%[0m) [0.29% of initial]
[Iter 7240/20000] Loss: 0.0006093 (Best: 0.0003869 @iter6961) ([92m↓16.37%[0m) [0.24% of initial]
[Iter 7250/20000] Loss: 0.0005233 (Best: 0.0003869 @iter6961) ([92m↓14.13%[0m) [0.21% of initial]
[Iter 7260/20000] Loss: 0.0004909 (Best: 0.0003869 @iter6961) ([92m↓6.18%[0m) [0.20% of initial]
[Iter 7270/20000] Loss: 0.0004530 (Best: 0.0003869 @iter6961) ([92m↓7.72%[0m) [0.18% of initial]
[Iter 7280/20000] Loss: 0.0004391 (Best: 0.0003838 @iter7276) ([92m↓3.07%[0m) [0.17% of initial]
[Iter 7290/20000] Loss: 0.0004642 (Best: 0.0003838 @iter7276) ([91m↑5.71%[0m) [0.18% of initial]
Iter:7299, L1 loss=0.000556, Total loss=0.0005305, Time:46
[Iter 7300/20000] Loss: 0.0004824 (Best: 0.0003838 @iter7276) ([91m↑3.92%[0m) [0.19% of initial]
[Iter 7310/20000] Loss: 0.0004689 (Best: 0.0003838 @iter7276) ([92m↓2.80%[0m) [0.19% of initial]
[Iter 7320/20000] Loss: 0.0005472 (Best: 0.0003838 @iter7276) ([91m↑16.70%[0m) [0.22% of initial]
[Iter 7330/20000] Loss: 0.0005120 (Best: 0.0003838 @iter7276) ([92m↓6.43%[0m) [0.20% of initial]
[Iter 7340/20000] Loss: 0.0004697 (Best: 0.0003838 @iter7276) ([92m↓8.26%[0m) [0.19% of initial]
[Iter 7350/20000] Loss: 0.0005179 (Best: 0.0003838 @iter7276) ([91m↑10.24%[0m) [0.21% of initial]
[Iter 7360/20000] Loss: 0.0005206 (Best: 0.0003838 @iter7276) ([91m↑0.53%[0m) [0.21% of initial]
[Iter 7370/20000] Loss: 0.0004584 (Best: 0.0003838 @iter7276) ([92m↓11.96%[0m) [0.18% of initial]
[Iter 7380/20000] Loss: 0.0005869 (Best: 0.0003838 @iter7276) ([91m↑28.05%[0m) [0.23% of initial]
[Iter 7390/20000] Loss: 0.0006195 (Best: 0.0003838 @iter7276) ([91m↑5.54%[0m) [0.25% of initial]
Iter:7399, L1 loss=0.0006252, Total loss=0.0005755, Time:48
[Iter 7400/20000] Loss: 0.0006135 (Best: 0.0003838 @iter7276) ([92m↓0.97%[0m) [0.24% of initial]
[Iter 7410/20000] Loss: 0.0012206 (Best: 0.0003838 @iter7276) ([91m↑98.95%[0m) [0.48% of initial]
[Iter 7420/20000] Loss: 0.0008264 (Best: 0.0003838 @iter7276) ([92m↓32.30%[0m) [0.33% of initial]
[Iter 7430/20000] Loss: 0.0006898 (Best: 0.0003838 @iter7276) ([92m↓16.52%[0m) [0.27% of initial]
[Iter 7440/20000] Loss: 0.0006338 (Best: 0.0003838 @iter7276) ([92m↓8.12%[0m) [0.25% of initial]
[Iter 7450/20000] Loss: 0.0005729 (Best: 0.0003838 @iter7276) ([92m↓9.62%[0m) [0.23% of initial]
[Iter 7460/20000] Loss: 0.0005401 (Best: 0.0003838 @iter7276) ([92m↓5.72%[0m) [0.21% of initial]
[Iter 7470/20000] Loss: 0.0004990 (Best: 0.0003838 @iter7276) ([92m↓7.61%[0m) [0.20% of initial]
[Iter 7480/20000] Loss: 0.0005019 (Best: 0.0003838 @iter7276) ([91m↑0.57%[0m) [0.20% of initial]
[Iter 7490/20000] Loss: 0.0004336 (Best: 0.0003838 @iter7276) ([92m↓13.60%[0m) [0.17% of initial]
Iter:7499, L1 loss=0.0004853, Total loss=0.0004622, Time:51
[Iter 7500/20000] Loss: 0.0004835 (Best: 0.0003838 @iter7276) ([91m↑11.51%[0m) [0.19% of initial]
Pruning 45 points (0.0%) from gaussian0 at iteration 7500
Pruning 75 points (0.0%) from gaussian1 at iteration 7500
[Iter 7510/20000] Loss: 0.0011664 (Best: 0.0003838 @iter7276) ([91m↑141.21%[0m) [0.46% of initial]
[Iter 7520/20000] Loss: 0.0007618 (Best: 0.0003838 @iter7276) ([92m↓34.68%[0m) [0.30% of initial]
[Iter 7530/20000] Loss: 0.0005886 (Best: 0.0003838 @iter7276) ([92m↓22.74%[0m) [0.23% of initial]
[Iter 7540/20000] Loss: 0.0004357 (Best: 0.0003838 @iter7276) ([92m↓25.99%[0m) [0.17% of initial]
[Iter 7550/20000] Loss: 0.0004381 (Best: 0.0003838 @iter7276) ([91m↑0.57%[0m) [0.17% of initial]
[Iter 7560/20000] Loss: 0.0004262 (Best: 0.0003704 @iter7555) ([92m↓2.73%[0m) [0.17% of initial]
[Iter 7570/20000] Loss: 0.0004453 (Best: 0.0003704 @iter7555) ([91m↑4.48%[0m) [0.18% of initial]
[Iter 7580/20000] Loss: 0.0004769 (Best: 0.0003704 @iter7555) ([91m↑7.10%[0m) [0.19% of initial]
[Iter 7590/20000] Loss: 0.0004823 (Best: 0.0003704 @iter7555) ([91m↑1.13%[0m) [0.19% of initial]
Iter:7599, L1 loss=0.0004666, Total loss=0.0004223, Time:49
[Iter 7600/20000] Loss: 0.0004311 (Best: 0.0003704 @iter7555) ([92m↓10.61%[0m) [0.17% of initial]
[Iter 7610/20000] Loss: 0.0014313 (Best: 0.0003704 @iter7555) ([91m↑232.00%[0m) [0.57% of initial]
[Iter 7620/20000] Loss: 0.0009060 (Best: 0.0003704 @iter7555) ([92m↓36.71%[0m) [0.36% of initial]
[Iter 7630/20000] Loss: 0.0007027 (Best: 0.0003704 @iter7555) ([92m↓22.43%[0m) [0.28% of initial]
[Iter 7640/20000] Loss: 0.0005748 (Best: 0.0003704 @iter7555) ([92m↓18.20%[0m) [0.23% of initial]
[Iter 7650/20000] Loss: 0.0005068 (Best: 0.0003704 @iter7555) ([92m↓11.83%[0m) [0.20% of initial]
[Iter 7660/20000] Loss: 0.0004232 (Best: 0.0003704 @iter7555) ([92m↓16.49%[0m) [0.17% of initial]
[Iter 7670/20000] Loss: 0.0003929 (Best: 0.0003704 @iter7555) ([92m↓7.17%[0m) [0.16% of initial]
[Iter 7680/20000] Loss: 0.0004299 (Best: 0.0003704 @iter7555) ([91m↑9.43%[0m) [0.17% of initial]
[Iter 7690/20000] Loss: 0.0004022 (Best: 0.0003643 @iter7690) ([92m↓6.44%[0m) [0.16% of initial]
Iter:7699, L1 loss=0.0004246, Total loss=0.0003872, Time:50
[Iter 7700/20000] Loss: 0.0003947 (Best: 0.0003643 @iter7690) ([92m↓1.87%[0m) [0.16% of initial]
[Iter 7710/20000] Loss: 0.0003857 (Best: 0.0003643 @iter7690) ([92m↓2.30%[0m) [0.15% of initial]
[Iter 7720/20000] Loss: 0.0003821 (Best: 0.0003385 @iter7714) ([92m↓0.93%[0m) [0.15% of initial]
[Iter 7730/20000] Loss: 0.0003823 (Best: 0.0003369 @iter7729) ([91m↑0.06%[0m) [0.15% of initial]
[Iter 7740/20000] Loss: 0.0003895 (Best: 0.0003369 @iter7729) ([91m↑1.88%[0m) [0.15% of initial]
[Iter 7750/20000] Loss: 0.0004223 (Best: 0.0003369 @iter7729) ([91m↑8.42%[0m) [0.17% of initial]
[Iter 7760/20000] Loss: 0.0004638 (Best: 0.0003369 @iter7729) ([91m↑9.83%[0m) [0.18% of initial]
[Iter 7770/20000] Loss: 0.0004515 (Best: 0.0003369 @iter7729) ([92m↓2.66%[0m) [0.18% of initial]
[Iter 7780/20000] Loss: 0.0004377 (Best: 0.0003369 @iter7729) ([92m↓3.05%[0m) [0.17% of initial]
[Iter 7790/20000] Loss: 0.0004517 (Best: 0.0003369 @iter7729) ([91m↑3.19%[0m) [0.18% of initial]
Iter:7799, L1 loss=0.0004463, Total loss=0.0003852, Time:45
[Iter 7800/20000] Loss: 0.0004465 (Best: 0.0003369 @iter7729) ([92m↓1.15%[0m) [0.18% of initial]
[Iter 7810/20000] Loss: 0.0009706 (Best: 0.0003369 @iter7729) ([91m↑117.40%[0m) [0.39% of initial]
[Iter 7820/20000] Loss: 0.0007290 (Best: 0.0003369 @iter7729) ([92m↓24.89%[0m) [0.29% of initial]
[Iter 7830/20000] Loss: 0.0006404 (Best: 0.0003369 @iter7729) ([92m↓12.15%[0m) [0.25% of initial]
[Iter 7840/20000] Loss: 0.0005472 (Best: 0.0003369 @iter7729) ([92m↓14.55%[0m) [0.22% of initial]
[Iter 7850/20000] Loss: 0.0005384 (Best: 0.0003369 @iter7729) ([92m↓1.61%[0m) [0.21% of initial]
[Iter 7860/20000] Loss: 0.0005445 (Best: 0.0003369 @iter7729) ([91m↑1.13%[0m) [0.22% of initial]
[Iter 7870/20000] Loss: 0.0005006 (Best: 0.0003369 @iter7729) ([92m↓8.06%[0m) [0.20% of initial]
[Iter 7880/20000] Loss: 0.0004410 (Best: 0.0003369 @iter7729) ([92m↓11.90%[0m) [0.18% of initial]
[Iter 7890/20000] Loss: 0.0005398 (Best: 0.0003369 @iter7729) ([91m↑22.40%[0m) [0.21% of initial]
Iter:7899, L1 loss=0.0004942, Total loss=0.0004555, Time:49
[Iter 7900/20000] Loss: 0.0004654 (Best: 0.0003369 @iter7729) ([92m↓13.79%[0m) [0.18% of initial]
[Iter 7910/20000] Loss: 0.0004650 (Best: 0.0003369 @iter7729) ([92m↓0.07%[0m) [0.18% of initial]
[Iter 7920/20000] Loss: 0.0005602 (Best: 0.0003369 @iter7729) ([91m↑20.47%[0m) [0.22% of initial]
[Iter 7930/20000] Loss: 0.0004661 (Best: 0.0003369 @iter7729) ([92m↓16.80%[0m) [0.19% of initial]
[Iter 7940/20000] Loss: 0.0004182 (Best: 0.0003369 @iter7729) ([92m↓10.27%[0m) [0.17% of initial]
[Iter 7950/20000] Loss: 0.0004031 (Best: 0.0003369 @iter7729) ([92m↓3.62%[0m) [0.16% of initial]
[Iter 7960/20000] Loss: 0.0004773 (Best: 0.0003369 @iter7729) ([91m↑18.40%[0m) [0.19% of initial]
[Iter 7970/20000] Loss: 0.0004626 (Best: 0.0003369 @iter7729) ([92m↓3.07%[0m) [0.18% of initial]
[Iter 7980/20000] Loss: 0.0004631 (Best: 0.0003369 @iter7729) ([91m↑0.11%[0m) [0.18% of initial]
[Iter 7990/20000] Loss: 0.0004134 (Best: 0.0003369 @iter7729) ([92m↓10.74%[0m) [0.16% of initial]
Iter:7999, L1 loss=0.0004691, Total loss=0.0004044, Time:47
[Iter 8000/20000] Loss: 0.0004103 (Best: 0.0003369 @iter7729) ([92m↓0.73%[0m) [0.16% of initial]
Pruning 30 points (0.0%) from gaussian0 at iteration 8000
Pruning 42 points (0.0%) from gaussian1 at iteration 8000
[Iter 8010/20000] Loss: 0.0414877 (Best: 0.0003369 @iter7729) ([91m↑10010.91%[0m) [16.48% of initial]
[Iter 8020/20000] Loss: 0.0167771 (Best: 0.0003369 @iter7729) ([92m↓59.56%[0m) [6.67% of initial]
[Iter 8030/20000] Loss: 0.0053012 (Best: 0.0003369 @iter7729) ([92m↓68.40%[0m) [2.11% of initial]
[Iter 8040/20000] Loss: 0.0039965 (Best: 0.0003369 @iter7729) ([92m↓24.61%[0m) [1.59% of initial]
[Iter 8050/20000] Loss: 0.0022967 (Best: 0.0003369 @iter7729) ([92m↓42.53%[0m) [0.91% of initial]
[Iter 8060/20000] Loss: 0.0015482 (Best: 0.0003369 @iter7729) ([92m↓32.59%[0m) [0.62% of initial]
[Iter 8070/20000] Loss: 0.0012269 (Best: 0.0003369 @iter7729) ([92m↓20.75%[0m) [0.49% of initial]
[Iter 8080/20000] Loss: 0.0009606 (Best: 0.0003369 @iter7729) ([92m↓21.71%[0m) [0.38% of initial]
[Iter 8090/20000] Loss: 0.0008050 (Best: 0.0003369 @iter7729) ([92m↓16.20%[0m) [0.32% of initial]
Iter:8099, L1 loss=0.0007337, Total loss=0.000715, Time:52
[Iter 8100/20000] Loss: 0.0007718 (Best: 0.0003369 @iter7729) ([92m↓4.12%[0m) [0.31% of initial]
[Iter 8110/20000] Loss: 0.0006750 (Best: 0.0003369 @iter7729) ([92m↓12.54%[0m) [0.27% of initial]
[Iter 8120/20000] Loss: 0.0006090 (Best: 0.0003369 @iter7729) ([92m↓9.78%[0m) [0.24% of initial]
[Iter 8130/20000] Loss: 0.0006347 (Best: 0.0003369 @iter7729) ([91m↑4.21%[0m) [0.25% of initial]
[Iter 8140/20000] Loss: 0.0005993 (Best: 0.0003369 @iter7729) ([92m↓5.57%[0m) [0.24% of initial]
[Iter 8150/20000] Loss: 0.0005877 (Best: 0.0003369 @iter7729) ([92m↓1.94%[0m) [0.23% of initial]
[Iter 8160/20000] Loss: 0.0005901 (Best: 0.0003369 @iter7729) ([91m↑0.40%[0m) [0.23% of initial]
[Iter 8170/20000] Loss: 0.0005682 (Best: 0.0003369 @iter7729) ([92m↓3.70%[0m) [0.23% of initial]
[Iter 8180/20000] Loss: 0.0005749 (Best: 0.0003369 @iter7729) ([91m↑1.17%[0m) [0.23% of initial]
[Iter 8190/20000] Loss: 0.0005507 (Best: 0.0003369 @iter7729) ([92m↓4.20%[0m) [0.22% of initial]
Iter:8199, L1 loss=0.0005961, Total loss=0.0005534, Time:51
[Iter 8200/20000] Loss: 0.0005335 (Best: 0.0003369 @iter7729) ([92m↓3.12%[0m) [0.21% of initial]
[Iter 8210/20000] Loss: 0.0005019 (Best: 0.0003369 @iter7729) ([92m↓5.93%[0m) [0.20% of initial]
[Iter 8220/20000] Loss: 0.0005190 (Best: 0.0003369 @iter7729) ([91m↑3.40%[0m) [0.21% of initial]
[Iter 8230/20000] Loss: 0.0005274 (Best: 0.0003369 @iter7729) ([91m↑1.61%[0m) [0.21% of initial]
[Iter 8240/20000] Loss: 0.0005199 (Best: 0.0003369 @iter7729) ([92m↓1.42%[0m) [0.21% of initial]
[Iter 8250/20000] Loss: 0.0005618 (Best: 0.0003369 @iter7729) ([91m↑8.07%[0m) [0.22% of initial]
[Iter 8260/20000] Loss: 0.0005497 (Best: 0.0003369 @iter7729) ([92m↓2.15%[0m) [0.22% of initial]
[Iter 8270/20000] Loss: 0.0005540 (Best: 0.0003369 @iter7729) ([91m↑0.78%[0m) [0.22% of initial]
[Iter 8280/20000] Loss: 0.0005327 (Best: 0.0003369 @iter7729) ([92m↓3.85%[0m) [0.21% of initial]
[Iter 8290/20000] Loss: 0.0005622 (Best: 0.0003369 @iter7729) ([91m↑5.52%[0m) [0.22% of initial]
Iter:8299, L1 loss=0.0005179, Total loss=0.0004607, Time:55
[Iter 8300/20000] Loss: 0.0005112 (Best: 0.0003369 @iter7729) ([92m↓9.07%[0m) [0.20% of initial]
[Iter 8310/20000] Loss: 0.0005019 (Best: 0.0003369 @iter7729) ([92m↓1.81%[0m) [0.20% of initial]
[Iter 8320/20000] Loss: 0.0004797 (Best: 0.0003369 @iter7729) ([92m↓4.43%[0m) [0.19% of initial]
[Iter 8330/20000] Loss: 0.0004841 (Best: 0.0003369 @iter7729) ([91m↑0.92%[0m) [0.19% of initial]
[Iter 8340/20000] Loss: 0.0005028 (Best: 0.0003369 @iter7729) ([91m↑3.86%[0m) [0.20% of initial]
[Iter 8350/20000] Loss: 0.0004602 (Best: 0.0003369 @iter7729) ([92m↓8.48%[0m) [0.18% of initial]
[Iter 8360/20000] Loss: 0.0004704 (Best: 0.0003369 @iter7729) ([91m↑2.22%[0m) [0.19% of initial]
[Iter 8370/20000] Loss: 0.0004648 (Best: 0.0003369 @iter7729) ([92m↓1.18%[0m) [0.18% of initial]
[Iter 8380/20000] Loss: 0.0004734 (Best: 0.0003369 @iter7729) ([91m↑1.84%[0m) [0.19% of initial]
[Iter 8390/20000] Loss: 0.0004370 (Best: 0.0003369 @iter7729) ([92m↓7.68%[0m) [0.17% of initial]
Iter:8399, L1 loss=0.0004929, Total loss=0.0004543, Time:51
[Iter 8400/20000] Loss: 0.0004879 (Best: 0.0003369 @iter7729) ([91m↑11.64%[0m) [0.19% of initial]
[Iter 8410/20000] Loss: 0.0004667 (Best: 0.0003369 @iter7729) ([92m↓4.34%[0m) [0.19% of initial]
[Iter 8420/20000] Loss: 0.0004577 (Best: 0.0003369 @iter7729) ([92m↓1.93%[0m) [0.18% of initial]
[Iter 8430/20000] Loss: 0.0004774 (Best: 0.0003369 @iter7729) ([91m↑4.31%[0m) [0.19% of initial]
[Iter 8440/20000] Loss: 0.0004618 (Best: 0.0003369 @iter7729) ([92m↓3.27%[0m) [0.18% of initial]
[Iter 8450/20000] Loss: 0.0004917 (Best: 0.0003369 @iter7729) ([91m↑6.47%[0m) [0.20% of initial]
[Iter 8460/20000] Loss: 0.0004746 (Best: 0.0003369 @iter7729) ([92m↓3.47%[0m) [0.19% of initial]
[Iter 8470/20000] Loss: 0.0004841 (Best: 0.0003369 @iter7729) ([91m↑2.01%[0m) [0.19% of initial]
[Iter 8480/20000] Loss: 0.0004691 (Best: 0.0003369 @iter7729) ([92m↓3.11%[0m) [0.19% of initial]
[Iter 8490/20000] Loss: 0.0004891 (Best: 0.0003369 @iter7729) ([91m↑4.26%[0m) [0.19% of initial]
Iter:8499, L1 loss=0.0005655, Total loss=0.0005052, Time:56
[Iter 8500/20000] Loss: 0.0005093 (Best: 0.0003369 @iter7729) ([91m↑4.12%[0m) [0.20% of initial]
Pruning 55 points (0.0%) from gaussian0 at iteration 8500
Pruning 70 points (0.0%) from gaussian1 at iteration 8500
[Iter 8510/20000] Loss: 0.0010629 (Best: 0.0003369 @iter7729) ([91m↑108.71%[0m) [0.42% of initial]
[Iter 8520/20000] Loss: 0.0007686 (Best: 0.0003369 @iter7729) ([92m↓27.69%[0m) [0.31% of initial]
[Iter 8530/20000] Loss: 0.0006062 (Best: 0.0003369 @iter7729) ([92m↓21.12%[0m) [0.24% of initial]
[Iter 8540/20000] Loss: 0.0005482 (Best: 0.0003369 @iter7729) ([92m↓9.58%[0m) [0.22% of initial]
[Iter 8550/20000] Loss: 0.0005209 (Best: 0.0003369 @iter7729) ([92m↓4.97%[0m) [0.21% of initial]
[Iter 8560/20000] Loss: 0.0005042 (Best: 0.0003369 @iter7729) ([92m↓3.20%[0m) [0.20% of initial]
[Iter 8570/20000] Loss: 0.0004960 (Best: 0.0003369 @iter7729) ([92m↓1.64%[0m) [0.20% of initial]
[Iter 8580/20000] Loss: 0.0004933 (Best: 0.0003369 @iter7729) ([92m↓0.54%[0m) [0.20% of initial]
[Iter 8590/20000] Loss: 0.0004787 (Best: 0.0003369 @iter7729) ([92m↓2.94%[0m) [0.19% of initial]
Iter:8599, L1 loss=0.0004769, Total loss=0.0004286, Time:52
[Iter 8600/20000] Loss: 0.0004693 (Best: 0.0003369 @iter7729) ([92m↓1.97%[0m) [0.19% of initial]
[Iter 8610/20000] Loss: 0.0004886 (Best: 0.0003369 @iter7729) ([91m↑4.10%[0m) [0.19% of initial]
[Iter 8620/20000] Loss: 0.0004783 (Best: 0.0003369 @iter7729) ([92m↓2.09%[0m) [0.19% of initial]
[Iter 8630/20000] Loss: 0.0004715 (Best: 0.0003369 @iter7729) ([92m↓1.43%[0m) [0.19% of initial]
[Iter 8640/20000] Loss: 0.0004535 (Best: 0.0003369 @iter7729) ([92m↓3.82%[0m) [0.18% of initial]
[Iter 8650/20000] Loss: 0.0004711 (Best: 0.0003369 @iter7729) ([91m↑3.90%[0m) [0.19% of initial]
[Iter 8660/20000] Loss: 0.0005133 (Best: 0.0003369 @iter7729) ([91m↑8.94%[0m) [0.20% of initial]
[Iter 8670/20000] Loss: 0.0005125 (Best: 0.0003369 @iter7729) ([92m↓0.16%[0m) [0.20% of initial]
[Iter 8680/20000] Loss: 0.0004909 (Best: 0.0003369 @iter7729) ([92m↓4.20%[0m) [0.20% of initial]
[Iter 8690/20000] Loss: 0.0005903 (Best: 0.0003369 @iter7729) ([91m↑20.23%[0m) [0.23% of initial]
Iter:8699, L1 loss=0.0005364, Total loss=0.0004959, Time:61
[Iter 8700/20000] Loss: 0.0005236 (Best: 0.0003369 @iter7729) ([92m↓11.30%[0m) [0.21% of initial]
[Iter 8710/20000] Loss: 0.0005063 (Best: 0.0003369 @iter7729) ([92m↓3.30%[0m) [0.20% of initial]
[Iter 8720/20000] Loss: 0.0004824 (Best: 0.0003369 @iter7729) ([92m↓4.73%[0m) [0.19% of initial]
[Iter 8730/20000] Loss: 0.0005204 (Best: 0.0003369 @iter7729) ([91m↑7.87%[0m) [0.21% of initial]
[Iter 8740/20000] Loss: 0.0004862 (Best: 0.0003369 @iter7729) ([92m↓6.56%[0m) [0.19% of initial]
[Iter 8750/20000] Loss: 0.0005446 (Best: 0.0003369 @iter7729) ([91m↑12.00%[0m) [0.22% of initial]
[Iter 8760/20000] Loss: 0.0005098 (Best: 0.0003369 @iter7729) ([92m↓6.38%[0m) [0.20% of initial]
[Iter 8770/20000] Loss: 0.0005301 (Best: 0.0003369 @iter7729) ([91m↑3.98%[0m) [0.21% of initial]
[Iter 8780/20000] Loss: 0.0005182 (Best: 0.0003369 @iter7729) ([92m↓2.25%[0m) [0.21% of initial]
[Iter 8790/20000] Loss: 0.0005228 (Best: 0.0003369 @iter7729) ([91m↑0.89%[0m) [0.21% of initial]
Iter:8799, L1 loss=0.0005895, Total loss=0.0005658, Time:51
[Iter 8800/20000] Loss: 0.0005015 (Best: 0.0003369 @iter7729) ([92m↓4.08%[0m) [0.20% of initial]
[Iter 8810/20000] Loss: 0.0004533 (Best: 0.0003369 @iter7729) ([92m↓9.60%[0m) [0.18% of initial]
[Iter 8820/20000] Loss: 0.0005183 (Best: 0.0003369 @iter7729) ([91m↑14.32%[0m) [0.21% of initial]
[Iter 8830/20000] Loss: 0.0006222 (Best: 0.0003369 @iter7729) ([91m↑20.06%[0m) [0.25% of initial]
[Iter 8840/20000] Loss: 0.0005779 (Best: 0.0003369 @iter7729) ([92m↓7.12%[0m) [0.23% of initial]
[Iter 8850/20000] Loss: 0.0005972 (Best: 0.0003369 @iter7729) ([91m↑3.34%[0m) [0.24% of initial]
[Iter 8860/20000] Loss: 0.0005029 (Best: 0.0003369 @iter7729) ([92m↓15.79%[0m) [0.20% of initial]
[Iter 8870/20000] Loss: 0.0004712 (Best: 0.0003369 @iter7729) ([92m↓6.30%[0m) [0.19% of initial]
[Iter 8880/20000] Loss: 0.0004658 (Best: 0.0003369 @iter7729) ([92m↓1.14%[0m) [0.19% of initial]
[Iter 8890/20000] Loss: 0.0004472 (Best: 0.0003369 @iter7729) ([92m↓4.00%[0m) [0.18% of initial]
Iter:8899, L1 loss=0.0004799, Total loss=0.0004553, Time:51
[Iter 8900/20000] Loss: 0.0004892 (Best: 0.0003369 @iter7729) ([91m↑9.39%[0m) [0.19% of initial]
[Iter 8910/20000] Loss: 0.0004858 (Best: 0.0003369 @iter7729) ([92m↓0.68%[0m) [0.19% of initial]
[Iter 8920/20000] Loss: 0.0004657 (Best: 0.0003369 @iter7729) ([92m↓4.14%[0m) [0.19% of initial]
[Iter 8930/20000] Loss: 0.0004665 (Best: 0.0003369 @iter7729) ([91m↑0.17%[0m) [0.19% of initial]
[Iter 8940/20000] Loss: 0.0004572 (Best: 0.0003369 @iter7729) ([92m↓1.99%[0m) [0.18% of initial]
[Iter 8950/20000] Loss: 0.0004903 (Best: 0.0003369 @iter7729) ([91m↑7.23%[0m) [0.19% of initial]
[Iter 8960/20000] Loss: 0.0004903 (Best: 0.0003369 @iter7729) ([91m↑0.01%[0m) [0.19% of initial]
[Iter 8970/20000] Loss: 0.0005635 (Best: 0.0003369 @iter7729) ([91m↑14.93%[0m) [0.22% of initial]
[Iter 8980/20000] Loss: 0.0005386 (Best: 0.0003369 @iter7729) ([92m↓4.42%[0m) [0.21% of initial]
[Iter 8990/20000] Loss: 0.0005008 (Best: 0.0003369 @iter7729) ([92m↓7.01%[0m) [0.20% of initial]
Iter:8999, L1 loss=0.0004685, Total loss=0.0004271, Time:55
[Iter 9000/20000] Loss: 0.0004704 (Best: 0.0003369 @iter7729) ([92m↓6.08%[0m) [0.19% of initial]
Pruning 38 points (0.0%) from gaussian0 at iteration 9000
Pruning 35 points (0.0%) from gaussian1 at iteration 9000
[Iter 9010/20000] Loss: 0.0008847 (Best: 0.0003369 @iter7729) ([91m↑88.07%[0m) [0.35% of initial]
[Iter 9020/20000] Loss: 0.0006625 (Best: 0.0003369 @iter7729) ([92m↓25.12%[0m) [0.26% of initial]
[Iter 9030/20000] Loss: 0.0005421 (Best: 0.0003369 @iter7729) ([92m↓18.18%[0m) [0.22% of initial]
[Iter 9040/20000] Loss: 0.0004983 (Best: 0.0003369 @iter7729) ([92m↓8.07%[0m) [0.20% of initial]
[Iter 9050/20000] Loss: 0.0004628 (Best: 0.0003369 @iter7729) ([92m↓7.12%[0m) [0.18% of initial]
[Iter 9060/20000] Loss: 0.0004803 (Best: 0.0003369 @iter7729) ([91m↑3.78%[0m) [0.19% of initial]
[Iter 9070/20000] Loss: 0.0004950 (Best: 0.0003369 @iter7729) ([91m↑3.06%[0m) [0.20% of initial]
[Iter 9080/20000] Loss: 0.0005207 (Best: 0.0003369 @iter7729) ([91m↑5.19%[0m) [0.21% of initial]
[Iter 9090/20000] Loss: 0.0004908 (Best: 0.0003369 @iter7729) ([92m↓5.74%[0m) [0.19% of initial]
Iter:9099, L1 loss=0.0005775, Total loss=0.000541, Time:55
[Iter 9100/20000] Loss: 0.0004881 (Best: 0.0003369 @iter7729) ([92m↓0.55%[0m) [0.19% of initial]
[Iter 9110/20000] Loss: 0.0005164 (Best: 0.0003369 @iter7729) ([91m↑5.79%[0m) [0.21% of initial]
[Iter 9120/20000] Loss: 0.0004611 (Best: 0.0003369 @iter7729) ([92m↓10.70%[0m) [0.18% of initial]
[Iter 9130/20000] Loss: 0.0004648 (Best: 0.0003369 @iter7729) ([91m↑0.79%[0m) [0.18% of initial]
[Iter 9140/20000] Loss: 0.0004561 (Best: 0.0003369 @iter7729) ([92m↓1.86%[0m) [0.18% of initial]
[Iter 9150/20000] Loss: 0.0004299 (Best: 0.0003369 @iter7729) ([92m↓5.75%[0m) [0.17% of initial]
[Iter 9160/20000] Loss: 0.0004630 (Best: 0.0003369 @iter7729) ([91m↑7.68%[0m) [0.18% of initial]
[Iter 9170/20000] Loss: 0.0004341 (Best: 0.0003369 @iter7729) ([92m↓6.24%[0m) [0.17% of initial]
[Iter 9180/20000] Loss: 0.0004488 (Best: 0.0003369 @iter7729) ([91m↑3.39%[0m) [0.18% of initial]
[Iter 9190/20000] Loss: 0.0004046 (Best: 0.0003369 @iter7729) ([92m↓9.84%[0m) [0.16% of initial]
Iter:9199, L1 loss=0.0004736, Total loss=0.0004307, Time:50
[Iter 9200/20000] Loss: 0.0004283 (Best: 0.0003369 @iter7729) ([91m↑5.85%[0m) [0.17% of initial]
[Iter 9210/20000] Loss: 0.0004427 (Best: 0.0003369 @iter7729) ([91m↑3.38%[0m) [0.18% of initial]
[Iter 9220/20000] Loss: 0.0004452 (Best: 0.0003369 @iter7729) ([91m↑0.55%[0m) [0.18% of initial]
[Iter 9230/20000] Loss: 0.0004359 (Best: 0.0003369 @iter7729) ([92m↓2.09%[0m) [0.17% of initial]
[Iter 9240/20000] Loss: 0.0004596 (Best: 0.0003369 @iter7729) ([91m↑5.44%[0m) [0.18% of initial]
[Iter 9250/20000] Loss: 0.0004388 (Best: 0.0003369 @iter7729) ([92m↓4.52%[0m) [0.17% of initial]
[Iter 9260/20000] Loss: 0.0004415 (Best: 0.0003369 @iter7729) ([91m↑0.61%[0m) [0.18% of initial]
[Iter 9270/20000] Loss: 0.0004337 (Best: 0.0003369 @iter7729) ([92m↓1.77%[0m) [0.17% of initial]
[Iter 9280/20000] Loss: 0.0004077 (Best: 0.0003369 @iter7729) ([92m↓5.99%[0m) [0.16% of initial]
[Iter 9290/20000] Loss: 0.0004036 (Best: 0.0003369 @iter7729) ([92m↓1.01%[0m) [0.16% of initial]
Iter:9299, L1 loss=0.0004281, Total loss=0.0003889, Time:55
[Iter 9300/20000] Loss: 0.0004409 (Best: 0.0003369 @iter7729) ([91m↑9.23%[0m) [0.18% of initial]
[Iter 9310/20000] Loss: 0.0004530 (Best: 0.0003369 @iter7729) ([91m↑2.76%[0m) [0.18% of initial]
[Iter 9320/20000] Loss: 0.0004623 (Best: 0.0003369 @iter7729) ([91m↑2.05%[0m) [0.18% of initial]
[Iter 9330/20000] Loss: 0.0005002 (Best: 0.0003369 @iter7729) ([91m↑8.19%[0m) [0.20% of initial]
[Iter 9340/20000] Loss: 0.0004886 (Best: 0.0003369 @iter7729) ([92m↓2.31%[0m) [0.19% of initial]
[Iter 9350/20000] Loss: 0.0004444 (Best: 0.0003369 @iter7729) ([92m↓9.05%[0m) [0.18% of initial]
[Iter 9360/20000] Loss: 0.0004913 (Best: 0.0003369 @iter7729) ([91m↑10.55%[0m) [0.20% of initial]
[Iter 9370/20000] Loss: 0.0004290 (Best: 0.0003369 @iter7729) ([92m↓12.67%[0m) [0.17% of initial]
[Iter 9380/20000] Loss: 0.0004580 (Best: 0.0003369 @iter7729) ([91m↑6.74%[0m) [0.18% of initial]
[Iter 9390/20000] Loss: 0.0004563 (Best: 0.0003369 @iter7729) ([92m↓0.36%[0m) [0.18% of initial]
Iter:9399, L1 loss=0.0005855, Total loss=0.0005371, Time:55
[Iter 9400/20000] Loss: 0.0004375 (Best: 0.0003369 @iter7729) ([92m↓4.13%[0m) [0.17% of initial]
[Iter 9410/20000] Loss: 0.0004376 (Best: 0.0003369 @iter7729) ([91m↑0.04%[0m) [0.17% of initial]
[Iter 9420/20000] Loss: 0.0004466 (Best: 0.0003369 @iter7729) ([91m↑2.05%[0m) [0.18% of initial]
[Iter 9430/20000] Loss: 0.0004134 (Best: 0.0003369 @iter7729) ([92m↓7.44%[0m) [0.16% of initial]
[Iter 9440/20000] Loss: 0.0004642 (Best: 0.0003369 @iter7729) ([91m↑12.31%[0m) [0.18% of initial]
[Iter 9450/20000] Loss: 0.0004406 (Best: 0.0003369 @iter7729) ([92m↓5.09%[0m) [0.18% of initial]
[Iter 9460/20000] Loss: 0.0003863 (Best: 0.0003369 @iter7729) ([92m↓12.32%[0m) [0.15% of initial]
[Iter 9470/20000] Loss: 0.0003736 (Best: 0.0003369 @iter7729) ([92m↓3.31%[0m) [0.15% of initial]
[Iter 9480/20000] Loss: 0.0004483 (Best: 0.0003369 @iter7729) ([91m↑20.01%[0m) [0.18% of initial]
[Iter 9490/20000] Loss: 0.0004076 (Best: 0.0003369 @iter7729) ([92m↓9.09%[0m) [0.16% of initial]
Iter:9499, L1 loss=0.0004406, Total loss=0.0004212, Time:50
[Iter 9500/20000] Loss: 0.0004235 (Best: 0.0003369 @iter7729) ([91m↑3.91%[0m) [0.17% of initial]
Pruning 19 points (0.0%) from gaussian0 at iteration 9500
Pruning 24 points (0.0%) from gaussian1 at iteration 9500
[Iter 9510/20000] Loss: 0.0007228 (Best: 0.0003369 @iter7729) ([91m↑70.69%[0m) [0.29% of initial]
[Iter 9520/20000] Loss: 0.0005496 (Best: 0.0003369 @iter7729) ([92m↓23.97%[0m) [0.22% of initial]
[Iter 9530/20000] Loss: 0.0005880 (Best: 0.0003369 @iter7729) ([91m↑6.99%[0m) [0.23% of initial]
[Iter 9540/20000] Loss: 0.0004660 (Best: 0.0003369 @iter7729) ([92m↓20.75%[0m) [0.19% of initial]
[Iter 9550/20000] Loss: 0.0004077 (Best: 0.0003369 @iter7729) ([92m↓12.51%[0m) [0.16% of initial]
[Iter 9560/20000] Loss: 0.0003920 (Best: 0.0003369 @iter7729) ([92m↓3.84%[0m) [0.16% of initial]
[Iter 9570/20000] Loss: 0.0003853 (Best: 0.0003369 @iter7729) ([92m↓1.71%[0m) [0.15% of initial]
[Iter 9580/20000] Loss: 0.0003701 (Best: 0.0003369 @iter7729) ([92m↓3.94%[0m) [0.15% of initial]
[Iter 9590/20000] Loss: 0.0003750 (Best: 0.0003369 @iter7729) ([91m↑1.32%[0m) [0.15% of initial]
Iter:9599, L1 loss=0.0005031, Total loss=0.0004572, Time:57
[Iter 9600/20000] Loss: 0.0004052 (Best: 0.0003369 @iter7729) ([91m↑8.04%[0m) [0.16% of initial]
[Iter 9610/20000] Loss: 0.0003915 (Best: 0.0003369 @iter7729) ([92m↓3.39%[0m) [0.16% of initial]
[Iter 9620/20000] Loss: 0.0004074 (Best: 0.0003369 @iter7729) ([91m↑4.08%[0m) [0.16% of initial]
[Iter 9630/20000] Loss: 0.0003837 (Best: 0.0003369 @iter7729) ([92m↓5.82%[0m) [0.15% of initial]
[Iter 9640/20000] Loss: 0.0003746 (Best: 0.0003344 @iter9638) ([92m↓2.39%[0m) [0.15% of initial]
[Iter 9650/20000] Loss: 0.0004283 (Best: 0.0003344 @iter9638) ([91m↑14.34%[0m) [0.17% of initial]
[Iter 9660/20000] Loss: 0.0004002 (Best: 0.0003344 @iter9638) ([92m↓6.56%[0m) [0.16% of initial]
[Iter 9670/20000] Loss: 0.0004413 (Best: 0.0003344 @iter9638) ([91m↑10.27%[0m) [0.18% of initial]
[Iter 9680/20000] Loss: 0.0004533 (Best: 0.0003344 @iter9638) ([91m↑2.72%[0m) [0.18% of initial]
[Iter 9690/20000] Loss: 0.0005206 (Best: 0.0003344 @iter9638) ([91m↑14.86%[0m) [0.21% of initial]
Iter:9699, L1 loss=0.0006151, Total loss=0.0005896, Time:51
[Iter 9700/20000] Loss: 0.0004847 (Best: 0.0003344 @iter9638) ([92m↓6.91%[0m) [0.19% of initial]
[Iter 9710/20000] Loss: 0.0004636 (Best: 0.0003344 @iter9638) ([92m↓4.35%[0m) [0.18% of initial]
[Iter 9720/20000] Loss: 0.0004326 (Best: 0.0003344 @iter9638) ([92m↓6.69%[0m) [0.17% of initial]
[Iter 9730/20000] Loss: 0.0003984 (Best: 0.0003344 @iter9638) ([92m↓7.89%[0m) [0.16% of initial]
[Iter 9740/20000] Loss: 0.0004230 (Best: 0.0003344 @iter9638) ([91m↑6.16%[0m) [0.17% of initial]
[Iter 9750/20000] Loss: 0.0004162 (Best: 0.0003344 @iter9638) ([92m↓1.61%[0m) [0.17% of initial]
[Iter 9760/20000] Loss: 0.0003854 (Best: 0.0003344 @iter9638) ([92m↓7.40%[0m) [0.15% of initial]
[Iter 9770/20000] Loss: 0.0003944 (Best: 0.0003344 @iter9638) ([91m↑2.34%[0m) [0.16% of initial]
[Iter 9780/20000] Loss: 0.0003752 (Best: 0.0003344 @iter9638) ([92m↓4.87%[0m) [0.15% of initial]
[Iter 9790/20000] Loss: 0.0003709 (Best: 0.0003240 @iter9784) ([92m↓1.15%[0m) [0.15% of initial]
Iter:9799, L1 loss=0.00043, Total loss=0.0003805, Time:34
[Iter 9800/20000] Loss: 0.0003951 (Best: 0.0003240 @iter9784) ([91m↑6.53%[0m) [0.16% of initial]
[Iter 9810/20000] Loss: 0.0003834 (Best: 0.0003240 @iter9784) ([92m↓2.97%[0m) [0.15% of initial]
[Iter 9820/20000] Loss: 0.0003609 (Best: 0.0003240 @iter9784) ([92m↓5.86%[0m) [0.14% of initial]
[Iter 9830/20000] Loss: 0.0003781 (Best: 0.0003240 @iter9784) ([91m↑4.75%[0m) [0.15% of initial]
[Iter 9840/20000] Loss: 0.0003944 (Best: 0.0003240 @iter9784) ([91m↑4.33%[0m) [0.16% of initial]
[Iter 9850/20000] Loss: 0.0003670 (Best: 0.0003240 @iter9784) ([92m↓6.95%[0m) [0.15% of initial]
[Iter 9860/20000] Loss: 0.0003691 (Best: 0.0003240 @iter9784) ([91m↑0.57%[0m) [0.15% of initial]
[Iter 9870/20000] Loss: 0.0003933 (Best: 0.0003240 @iter9784) ([91m↑6.55%[0m) [0.16% of initial]
[Iter 9880/20000] Loss: 0.0004117 (Best: 0.0003240 @iter9784) ([91m↑4.69%[0m) [0.16% of initial]
[Iter 9890/20000] Loss: 0.0005077 (Best: 0.0003240 @iter9784) ([91m↑23.31%[0m) [0.20% of initial]
Iter:9899, L1 loss=0.0005777, Total loss=0.0005037, Time:55
[Iter 9900/20000] Loss: 0.0004611 (Best: 0.0003240 @iter9784) ([92m↓9.19%[0m) [0.18% of initial]
[Iter 9910/20000] Loss: 0.0003856 (Best: 0.0003240 @iter9784) ([92m↓16.36%[0m) [0.15% of initial]
[Iter 9920/20000] Loss: 0.0003770 (Best: 0.0003240 @iter9784) ([92m↓2.25%[0m) [0.15% of initial]
[Iter 9930/20000] Loss: 0.0003986 (Best: 0.0003240 @iter9784) ([91m↑5.74%[0m) [0.16% of initial]
[Iter 9940/20000] Loss: 0.0003920 (Best: 0.0003240 @iter9784) ([92m↓1.67%[0m) [0.16% of initial]
[Iter 9950/20000] Loss: 0.0004331 (Best: 0.0003240 @iter9784) ([91m↑10.50%[0m) [0.17% of initial]
[Iter 9960/20000] Loss: 0.0004330 (Best: 0.0003240 @iter9784) ([92m↓0.02%[0m) [0.17% of initial]
[Iter 9970/20000] Loss: 0.0004127 (Best: 0.0003240 @iter9784) ([92m↓4.70%[0m) [0.16% of initial]
[Iter 9980/20000] Loss: 0.0004080 (Best: 0.0003240 @iter9784) ([92m↓1.12%[0m) [0.16% of initial]
[Iter 9990/20000] Loss: 0.0004276 (Best: 0.0003240 @iter9784) ([91m↑4.80%[0m) [0.17% of initial]
Iter:9999, L1 loss=0.0006446, Total loss=0.0005581, Time:55
[Iter 10000/20000] Loss: 0.0004964 (Best: 0.0003240 @iter9784) ([91m↑16.08%[0m) [0.20% of initial]
Pruning 19 points (0.0%) from gaussian0 at iteration 10000
Pruning 21 points (0.0%) from gaussian1 at iteration 10000
[Iter 10010/20000] Loss: 0.0007291 (Best: 0.0003240 @iter9784) ([91m↑46.88%[0m) [0.29% of initial]
[Iter 10020/20000] Loss: 0.0005973 (Best: 0.0003240 @iter9784) ([92m↓18.08%[0m) [0.24% of initial]
[Iter 10030/20000] Loss: 0.0004440 (Best: 0.0003240 @iter9784) ([92m↓25.66%[0m) [0.18% of initial]
[Iter 10040/20000] Loss: 0.0004054 (Best: 0.0003240 @iter9784) ([92m↓8.69%[0m) [0.16% of initial]
[Iter 10050/20000] Loss: 0.0003786 (Best: 0.0003240 @iter9784) ([92m↓6.61%[0m) [0.15% of initial]
[Iter 10060/20000] Loss: 0.0003414 (Best: 0.0003235 @iter10060) ([92m↓9.82%[0m) [0.14% of initial]
[Iter 10070/20000] Loss: 0.0003452 (Best: 0.0003235 @iter10060) ([91m↑1.12%[0m) [0.14% of initial]
[Iter 10080/20000] Loss: 0.0003507 (Best: 0.0003201 @iter10072) ([91m↑1.59%[0m) [0.14% of initial]
[Iter 10090/20000] Loss: 0.0003454 (Best: 0.0003163 @iter10084) ([92m↓1.51%[0m) [0.14% of initial]
Iter:10099, L1 loss=0.000378, Total loss=0.0003592, Time:55
[Iter 10100/20000] Loss: 0.0003674 (Best: 0.0003163 @iter10084) ([91m↑6.35%[0m) [0.15% of initial]
[Iter 10110/20000] Loss: 0.0003623 (Best: 0.0003163 @iter10084) ([92m↓1.38%[0m) [0.14% of initial]
[Iter 10120/20000] Loss: 0.0003589 (Best: 0.0003163 @iter10084) ([92m↓0.92%[0m) [0.14% of initial]
[Iter 10130/20000] Loss: 0.0003637 (Best: 0.0003163 @iter10084) ([91m↑1.33%[0m) [0.14% of initial]
[Iter 10140/20000] Loss: 0.0004160 (Best: 0.0003163 @iter10084) ([91m↑14.38%[0m) [0.17% of initial]
[Iter 10150/20000] Loss: 0.0004179 (Best: 0.0003163 @iter10084) ([91m↑0.46%[0m) [0.17% of initial]
[Iter 10160/20000] Loss: 0.0004021 (Best: 0.0003163 @iter10084) ([92m↓3.78%[0m) [0.16% of initial]
[Iter 10170/20000] Loss: 0.0003789 (Best: 0.0003163 @iter10084) ([92m↓5.79%[0m) [0.15% of initial]
[Iter 10180/20000] Loss: 0.0003393 (Best: 0.0003163 @iter10084) ([92m↓10.45%[0m) [0.13% of initial]
[Iter 10190/20000] Loss: 0.0003256 (Best: 0.0003074 @iter10183) ([92m↓4.02%[0m) [0.13% of initial]
Iter:10199, L1 loss=0.0004303, Total loss=0.0003925, Time:58
[Iter 10200/20000] Loss: 0.0003680 (Best: 0.0003074 @iter10183) ([91m↑13.02%[0m) [0.15% of initial]
[Iter 10210/20000] Loss: 0.0003522 (Best: 0.0003074 @iter10183) ([92m↓4.29%[0m) [0.14% of initial]
[Iter 10220/20000] Loss: 0.0003636 (Best: 0.0003074 @iter10183) ([91m↑3.23%[0m) [0.14% of initial]
[Iter 10230/20000] Loss: 0.0003633 (Best: 0.0003074 @iter10183) ([92m↓0.09%[0m) [0.14% of initial]
[Iter 10240/20000] Loss: 0.0003500 (Best: 0.0003064 @iter10237) ([92m↓3.67%[0m) [0.14% of initial]
[Iter 10250/20000] Loss: 0.0003567 (Best: 0.0003064 @iter10237) ([91m↑1.93%[0m) [0.14% of initial]
[Iter 10260/20000] Loss: 0.0003849 (Best: 0.0003064 @iter10237) ([91m↑7.90%[0m) [0.15% of initial]
[Iter 10270/20000] Loss: 0.0003611 (Best: 0.0003064 @iter10237) ([92m↓6.18%[0m) [0.14% of initial]
[Iter 10280/20000] Loss: 0.0003632 (Best: 0.0003064 @iter10237) ([91m↑0.58%[0m) [0.14% of initial]
[Iter 10290/20000] Loss: 0.0003991 (Best: 0.0003064 @iter10237) ([91m↑9.88%[0m) [0.16% of initial]
Iter:10299, L1 loss=0.0004022, Total loss=0.0003547, Time:55
[Iter 10300/20000] Loss: 0.0003648 (Best: 0.0003064 @iter10237) ([92m↓8.61%[0m) [0.14% of initial]
[Iter 10310/20000] Loss: 0.0003827 (Best: 0.0003064 @iter10237) ([91m↑4.92%[0m) [0.15% of initial]
[Iter 10320/20000] Loss: 0.0003870 (Best: 0.0003064 @iter10237) ([91m↑1.13%[0m) [0.15% of initial]
[Iter 10330/20000] Loss: 0.0003674 (Best: 0.0003064 @iter10237) ([92m↓5.06%[0m) [0.15% of initial]
[Iter 10340/20000] Loss: 0.0003949 (Best: 0.0003064 @iter10237) ([91m↑7.47%[0m) [0.16% of initial]
[Iter 10350/20000] Loss: 0.0003722 (Best: 0.0003064 @iter10237) ([92m↓5.74%[0m) [0.15% of initial]
[Iter 10360/20000] Loss: 0.0003990 (Best: 0.0003064 @iter10237) ([91m↑7.19%[0m) [0.16% of initial]
[Iter 10370/20000] Loss: 0.0004175 (Best: 0.0003064 @iter10237) ([91m↑4.63%[0m) [0.17% of initial]
[Iter 10380/20000] Loss: 0.0003716 (Best: 0.0003064 @iter10237) ([92m↓10.98%[0m) [0.15% of initial]
[Iter 10390/20000] Loss: 0.0003959 (Best: 0.0003064 @iter10237) ([91m↑6.54%[0m) [0.16% of initial]
Iter:10399, L1 loss=0.0004162, Total loss=0.0003524, Time:56
[Iter 10400/20000] Loss: 0.0003789 (Best: 0.0003064 @iter10237) ([92m↓4.31%[0m) [0.15% of initial]
[Iter 10410/20000] Loss: 0.0003516 (Best: 0.0003064 @iter10237) ([92m↓7.21%[0m) [0.14% of initial]
[Iter 10420/20000] Loss: 0.0003633 (Best: 0.0003064 @iter10237) ([91m↑3.34%[0m) [0.14% of initial]
[Iter 10430/20000] Loss: 0.0004092 (Best: 0.0003064 @iter10237) ([91m↑12.63%[0m) [0.16% of initial]
[Iter 10440/20000] Loss: 0.0003799 (Best: 0.0003064 @iter10237) ([92m↓7.16%[0m) [0.15% of initial]
[Iter 10450/20000] Loss: 0.0003953 (Best: 0.0003064 @iter10237) ([91m↑4.06%[0m) [0.16% of initial]
[Iter 10460/20000] Loss: 0.0003343 (Best: 0.0003064 @iter10237) ([92m↓15.42%[0m) [0.13% of initial]
[Iter 10470/20000] Loss: 0.0004002 (Best: 0.0003064 @iter10237) ([91m↑19.69%[0m) [0.16% of initial]
[Iter 10480/20000] Loss: 0.0003625 (Best: 0.0003064 @iter10237) ([92m↓9.42%[0m) [0.14% of initial]
[Iter 10490/20000] Loss: 0.0003407 (Best: 0.0003064 @iter10237) ([92m↓6.01%[0m) [0.14% of initial]
Iter:10499, L1 loss=0.0003905, Total loss=0.0003314, Time:50
[Iter 10500/20000] Loss: 0.0003410 (Best: 0.0003064 @iter10237) ([91m↑0.08%[0m) [0.14% of initial]
Pruning 20 points (0.0%) from gaussian0 at iteration 10500
Pruning 20 points (0.0%) from gaussian1 at iteration 10500
[Iter 10510/20000] Loss: 0.0006188 (Best: 0.0003064 @iter10237) ([91m↑81.50%[0m) [0.25% of initial]
[Iter 10520/20000] Loss: 0.0004944 (Best: 0.0003064 @iter10237) ([92m↓20.12%[0m) [0.20% of initial]
[Iter 10530/20000] Loss: 0.0004555 (Best: 0.0003064 @iter10237) ([92m↓7.86%[0m) [0.18% of initial]
[Iter 10540/20000] Loss: 0.0004095 (Best: 0.0003064 @iter10237) ([92m↓10.09%[0m) [0.16% of initial]
[Iter 10550/20000] Loss: 0.0003548 (Best: 0.0003064 @iter10237) ([92m↓13.36%[0m) [0.14% of initial]
[Iter 10560/20000] Loss: 0.0003405 (Best: 0.0003064 @iter10237) ([92m↓4.04%[0m) [0.14% of initial]
[Iter 10570/20000] Loss: 0.0003356 (Best: 0.0003054 @iter10561) ([92m↓1.45%[0m) [0.13% of initial]
[Iter 10580/20000] Loss: 0.0003428 (Best: 0.0003054 @iter10561) ([91m↑2.15%[0m) [0.14% of initial]
[Iter 10590/20000] Loss: 0.0003423 (Best: 0.0003018 @iter10583) ([92m↓0.15%[0m) [0.14% of initial]
Iter:10599, L1 loss=0.0003717, Total loss=0.0003268, Time:50
[Iter 10600/20000] Loss: 0.0003357 (Best: 0.0003018 @iter10583) ([92m↓1.91%[0m) [0.13% of initial]
[Iter 10610/20000] Loss: 0.0003322 (Best: 0.0002961 @iter10603) ([92m↓1.04%[0m) [0.13% of initial]
[Iter 10620/20000] Loss: 0.0003280 (Best: 0.0002961 @iter10603) ([92m↓1.26%[0m) [0.13% of initial]
[Iter 10630/20000] Loss: 0.0003450 (Best: 0.0002961 @iter10603) ([91m↑5.16%[0m) [0.14% of initial]
[Iter 10640/20000] Loss: 0.0003866 (Best: 0.0002961 @iter10603) ([91m↑12.06%[0m) [0.15% of initial]
[Iter 10650/20000] Loss: 0.0003840 (Best: 0.0002961 @iter10603) ([92m↓0.66%[0m) [0.15% of initial]
[Iter 10660/20000] Loss: 0.0003591 (Best: 0.0002961 @iter10603) ([92m↓6.48%[0m) [0.14% of initial]
[Iter 10670/20000] Loss: 0.0003488 (Best: 0.0002961 @iter10603) ([92m↓2.87%[0m) [0.14% of initial]
[Iter 10680/20000] Loss: 0.0003291 (Best: 0.0002961 @iter10603) ([92m↓5.66%[0m) [0.13% of initial]
[Iter 10690/20000] Loss: 0.0003373 (Best: 0.0002961 @iter10603) ([91m↑2.51%[0m) [0.13% of initial]
Iter:10699, L1 loss=0.000413, Total loss=0.0003684, Time:49
[Iter 10700/20000] Loss: 0.0003792 (Best: 0.0002961 @iter10603) ([91m↑12.40%[0m) [0.15% of initial]
[Iter 10710/20000] Loss: 0.0003888 (Best: 0.0002961 @iter10603) ([91m↑2.54%[0m) [0.15% of initial]
[Iter 10720/20000] Loss: 0.0004384 (Best: 0.0002961 @iter10603) ([91m↑12.77%[0m) [0.17% of initial]
[Iter 10730/20000] Loss: 0.0004377 (Best: 0.0002961 @iter10603) ([92m↓0.17%[0m) [0.17% of initial]
[Iter 10740/20000] Loss: 0.0003729 (Best: 0.0002961 @iter10603) ([92m↓14.81%[0m) [0.15% of initial]
[Iter 10750/20000] Loss: 0.0003702 (Best: 0.0002961 @iter10603) ([92m↓0.71%[0m) [0.15% of initial]
[Iter 10760/20000] Loss: 0.0003398 (Best: 0.0002961 @iter10603) ([92m↓8.21%[0m) [0.14% of initial]
[Iter 10770/20000] Loss: 0.0003260 (Best: 0.0002961 @iter10603) ([92m↓4.09%[0m) [0.13% of initial]
[Iter 10780/20000] Loss: 0.0003231 (Best: 0.0002961 @iter10603) ([92m↓0.88%[0m) [0.13% of initial]
[Iter 10790/20000] Loss: 0.0003358 (Best: 0.0002850 @iter10784) ([91m↑3.92%[0m) [0.13% of initial]
Iter:10799, L1 loss=0.0003668, Total loss=0.0003312, Time:50
[Iter 10800/20000] Loss: 0.0003643 (Best: 0.0002850 @iter10784) ([91m↑8.50%[0m) [0.14% of initial]
[Iter 10810/20000] Loss: 0.0004076 (Best: 0.0002850 @iter10784) ([91m↑11.88%[0m) [0.16% of initial]
[Iter 10820/20000] Loss: 0.0003876 (Best: 0.0002850 @iter10784) ([92m↓4.90%[0m) [0.15% of initial]
[Iter 10830/20000] Loss: 0.0003834 (Best: 0.0002850 @iter10784) ([92m↓1.07%[0m) [0.15% of initial]
[Iter 10840/20000] Loss: 0.0003423 (Best: 0.0002850 @iter10784) ([92m↓10.72%[0m) [0.14% of initial]
[Iter 10850/20000] Loss: 0.0003666 (Best: 0.0002850 @iter10784) ([91m↑7.10%[0m) [0.15% of initial]
[Iter 10860/20000] Loss: 0.0003375 (Best: 0.0002850 @iter10784) ([92m↓7.96%[0m) [0.13% of initial]
[Iter 10870/20000] Loss: 0.0003331 (Best: 0.0002850 @iter10784) ([92m↓1.28%[0m) [0.13% of initial]
[Iter 10880/20000] Loss: 0.0003328 (Best: 0.0002850 @iter10784) ([92m↓0.11%[0m) [0.13% of initial]
[Iter 10890/20000] Loss: 0.0003477 (Best: 0.0002850 @iter10784) ([91m↑4.48%[0m) [0.14% of initial]
Iter:10899, L1 loss=0.0004616, Total loss=0.00041, Time:55
[Iter 10900/20000] Loss: 0.0003673 (Best: 0.0002850 @iter10784) ([91m↑5.64%[0m) [0.15% of initial]
[Iter 10910/20000] Loss: 0.0003631 (Best: 0.0002850 @iter10784) ([92m↓1.12%[0m) [0.14% of initial]
[Iter 10920/20000] Loss: 0.0003448 (Best: 0.0002850 @iter10784) ([92m↓5.06%[0m) [0.14% of initial]
[Iter 10930/20000] Loss: 0.0003312 (Best: 0.0002850 @iter10784) ([92m↓3.95%[0m) [0.13% of initial]
[Iter 10940/20000] Loss: 0.0003484 (Best: 0.0002850 @iter10784) ([91m↑5.20%[0m) [0.14% of initial]
[Iter 10950/20000] Loss: 0.0003320 (Best: 0.0002850 @iter10784) ([92m↓4.71%[0m) [0.13% of initial]
[Iter 10960/20000] Loss: 0.0003274 (Best: 0.0002850 @iter10784) ([92m↓1.38%[0m) [0.13% of initial]
[Iter 10970/20000] Loss: 0.0003118 (Best: 0.0002850 @iter10784) ([92m↓4.76%[0m) [0.12% of initial]
[Iter 10980/20000] Loss: 0.0003021 (Best: 0.0002850 @iter10784) ([92m↓3.11%[0m) [0.12% of initial]
[Iter 10990/20000] Loss: 0.0003148 (Best: 0.0002841 @iter10981) ([91m↑4.21%[0m) [0.13% of initial]
Iter:10999, L1 loss=0.0003685, Total loss=0.0003307, Time:53
[Iter 11000/20000] Loss: 0.0003062 (Best: 0.0002841 @iter10981) ([92m↓2.73%[0m) [0.12% of initial]
Pruning 10 points (0.0%) from gaussian0 at iteration 11000
Pruning 26 points (0.0%) from gaussian1 at iteration 11000
[Iter 11010/20000] Loss: 0.0006979 (Best: 0.0002841 @iter10981) ([91m↑127.91%[0m) [0.28% of initial]
[Iter 11020/20000] Loss: 0.0005304 (Best: 0.0002841 @iter10981) ([92m↓24.00%[0m) [0.21% of initial]
[Iter 11030/20000] Loss: 0.0004006 (Best: 0.0002841 @iter10981) ([92m↓24.46%[0m) [0.16% of initial]
[Iter 11040/20000] Loss: 0.0003502 (Best: 0.0002841 @iter10981) ([92m↓12.60%[0m) [0.14% of initial]
[Iter 11050/20000] Loss: 0.0003428 (Best: 0.0002841 @iter10981) ([92m↓2.11%[0m) [0.14% of initial]
[Iter 11060/20000] Loss: 0.0003057 (Best: 0.0002779 @iter11056) ([92m↓10.82%[0m) [0.12% of initial]
[Iter 11070/20000] Loss: 0.0003234 (Best: 0.0002779 @iter11056) ([91m↑5.80%[0m) [0.13% of initial]
[Iter 11080/20000] Loss: 0.0003073 (Best: 0.0002779 @iter11056) ([92m↓5.00%[0m) [0.12% of initial]
[Iter 11090/20000] Loss: 0.0003302 (Best: 0.0002779 @iter11056) ([91m↑7.47%[0m) [0.13% of initial]
Iter:11099, L1 loss=0.0003536, Total loss=0.0002995, Time:53
[Iter 11100/20000] Loss: 0.0003379 (Best: 0.0002779 @iter11056) ([91m↑2.32%[0m) [0.13% of initial]
[Iter 11110/20000] Loss: 0.0003815 (Best: 0.0002779 @iter11056) ([91m↑12.92%[0m) [0.15% of initial]
[Iter 11120/20000] Loss: 0.0003291 (Best: 0.0002779 @iter11056) ([92m↓13.73%[0m) [0.13% of initial]
[Iter 11130/20000] Loss: 0.0003841 (Best: 0.0002779 @iter11056) ([91m↑16.71%[0m) [0.15% of initial]
[Iter 11140/20000] Loss: 0.0003376 (Best: 0.0002779 @iter11056) ([92m↓12.12%[0m) [0.13% of initial]
[Iter 11150/20000] Loss: 0.0003283 (Best: 0.0002779 @iter11056) ([92m↓2.75%[0m) [0.13% of initial]
[Iter 11160/20000] Loss: 0.0003444 (Best: 0.0002779 @iter11056) ([91m↑4.91%[0m) [0.14% of initial]
[Iter 11170/20000] Loss: 0.0003418 (Best: 0.0002779 @iter11056) ([92m↓0.76%[0m) [0.14% of initial]
[Iter 11180/20000] Loss: 0.0003291 (Best: 0.0002779 @iter11056) ([92m↓3.71%[0m) [0.13% of initial]
[Iter 11190/20000] Loss: 0.0003429 (Best: 0.0002779 @iter11056) ([91m↑4.17%[0m) [0.14% of initial]
Iter:11199, L1 loss=0.000362, Total loss=0.0003169, Time:53
[Iter 11200/20000] Loss: 0.0003175 (Best: 0.0002779 @iter11056) ([92m↓7.40%[0m) [0.13% of initial]
[Iter 11210/20000] Loss: 0.0003035 (Best: 0.0002779 @iter11056) ([92m↓4.41%[0m) [0.12% of initial]
[Iter 11220/20000] Loss: 0.0003233 (Best: 0.0002779 @iter11056) ([91m↑6.52%[0m) [0.13% of initial]
[Iter 11230/20000] Loss: 0.0003192 (Best: 0.0002779 @iter11056) ([92m↓1.28%[0m) [0.13% of initial]
[Iter 11240/20000] Loss: 0.0003206 (Best: 0.0002779 @iter11056) ([91m↑0.46%[0m) [0.13% of initial]
[Iter 11250/20000] Loss: 0.0003203 (Best: 0.0002779 @iter11056) ([92m↓0.11%[0m) [0.13% of initial]
[Iter 11260/20000] Loss: 0.0002976 (Best: 0.0002779 @iter11056) ([92m↓7.09%[0m) [0.12% of initial]
[Iter 11270/20000] Loss: 0.0002906 (Best: 0.0002720 @iter11270) ([92m↓2.34%[0m) [0.12% of initial]
[Iter 11280/20000] Loss: 0.0003112 (Best: 0.0002720 @iter11270) ([91m↑7.08%[0m) [0.12% of initial]
[Iter 11290/20000] Loss: 0.0003067 (Best: 0.0002720 @iter11270) ([92m↓1.44%[0m) [0.12% of initial]
Iter:11299, L1 loss=0.0003207, Total loss=0.0002841, Time:52
[Iter 11300/20000] Loss: 0.0003138 (Best: 0.0002720 @iter11270) ([91m↑2.30%[0m) [0.12% of initial]
[Iter 11310/20000] Loss: 0.0002850 (Best: 0.0002720 @iter11270) ([92m↓9.16%[0m) [0.11% of initial]
[Iter 11320/20000] Loss: 0.0002969 (Best: 0.0002655 @iter11315) ([91m↑4.19%[0m) [0.12% of initial]
[Iter 11330/20000] Loss: 0.0003062 (Best: 0.0002655 @iter11315) ([91m↑3.11%[0m) [0.12% of initial]
[Iter 11340/20000] Loss: 0.0003261 (Best: 0.0002655 @iter11315) ([91m↑6.50%[0m) [0.13% of initial]
[Iter 11350/20000] Loss: 0.0003926 (Best: 0.0002655 @iter11315) ([91m↑20.40%[0m) [0.16% of initial]
[Iter 11360/20000] Loss: 0.0003178 (Best: 0.0002655 @iter11315) ([92m↓19.04%[0m) [0.13% of initial]
[Iter 11370/20000] Loss: 0.0003524 (Best: 0.0002655 @iter11315) ([91m↑10.87%[0m) [0.14% of initial]
[Iter 11380/20000] Loss: 0.0003604 (Best: 0.0002655 @iter11315) ([91m↑2.28%[0m) [0.14% of initial]
[Iter 11390/20000] Loss: 0.0003603 (Best: 0.0002655 @iter11315) ([92m↓0.03%[0m) [0.14% of initial]
Iter:11399, L1 loss=0.0004158, Total loss=0.0003581, Time:50
[Iter 11400/20000] Loss: 0.0003628 (Best: 0.0002655 @iter11315) ([91m↑0.71%[0m) [0.14% of initial]
[Iter 11410/20000] Loss: 0.0003336 (Best: 0.0002655 @iter11315) ([92m↓8.06%[0m) [0.13% of initial]
[Iter 11420/20000] Loss: 0.0003104 (Best: 0.0002655 @iter11315) ([92m↓6.96%[0m) [0.12% of initial]
[Iter 11430/20000] Loss: 0.0003410 (Best: 0.0002655 @iter11315) ([91m↑9.87%[0m) [0.14% of initial]
[Iter 11440/20000] Loss: 0.0003503 (Best: 0.0002655 @iter11315) ([91m↑2.72%[0m) [0.14% of initial]
[Iter 11450/20000] Loss: 0.0003652 (Best: 0.0002655 @iter11315) ([91m↑4.26%[0m) [0.15% of initial]
[Iter 11460/20000] Loss: 0.0003296 (Best: 0.0002655 @iter11315) ([92m↓9.75%[0m) [0.13% of initial]
[Iter 11470/20000] Loss: 0.0003251 (Best: 0.0002655 @iter11315) ([92m↓1.36%[0m) [0.13% of initial]
[Iter 11480/20000] Loss: 0.0003095 (Best: 0.0002655 @iter11315) ([92m↓4.79%[0m) [0.12% of initial]
[Iter 11490/20000] Loss: 0.0003160 (Best: 0.0002655 @iter11315) ([91m↑2.08%[0m) [0.13% of initial]
Iter:11499, L1 loss=0.0004346, Total loss=0.000383, Time:46
[Iter 11500/20000] Loss: 0.0003364 (Best: 0.0002655 @iter11315) ([91m↑6.45%[0m) [0.13% of initial]
Pruning 13 points (0.0%) from gaussian0 at iteration 11500
Pruning 18 points (0.0%) from gaussian1 at iteration 11500
[Iter 11510/20000] Loss: 0.0005936 (Best: 0.0002655 @iter11315) ([91m↑76.47%[0m) [0.24% of initial]
[Iter 11520/20000] Loss: 0.0004291 (Best: 0.0002655 @iter11315) ([92m↓27.71%[0m) [0.17% of initial]
[Iter 11530/20000] Loss: 0.0003676 (Best: 0.0002655 @iter11315) ([92m↓14.34%[0m) [0.15% of initial]
[Iter 11540/20000] Loss: 0.0003310 (Best: 0.0002655 @iter11315) ([92m↓9.97%[0m) [0.13% of initial]
[Iter 11550/20000] Loss: 0.0003301 (Best: 0.0002655 @iter11315) ([92m↓0.27%[0m) [0.13% of initial]
[Iter 11560/20000] Loss: 0.0003301 (Best: 0.0002655 @iter11315) ([91m↑0.02%[0m) [0.13% of initial]
[Iter 11570/20000] Loss: 0.0003176 (Best: 0.0002655 @iter11315) ([92m↓3.79%[0m) [0.13% of initial]
[Iter 11580/20000] Loss: 0.0003417 (Best: 0.0002655 @iter11315) ([91m↑7.57%[0m) [0.14% of initial]
[Iter 11590/20000] Loss: 0.0003399 (Best: 0.0002655 @iter11315) ([92m↓0.53%[0m) [0.14% of initial]
Iter:11599, L1 loss=0.0003369, Total loss=0.0003464, Time:51
[Iter 11600/20000] Loss: 0.0003377 (Best: 0.0002655 @iter11315) ([92m↓0.63%[0m) [0.13% of initial]
[Iter 11610/20000] Loss: 0.0003299 (Best: 0.0002655 @iter11315) ([92m↓2.32%[0m) [0.13% of initial]
[Iter 11620/20000] Loss: 0.0003242 (Best: 0.0002655 @iter11315) ([92m↓1.74%[0m) [0.13% of initial]
[Iter 11630/20000] Loss: 0.0002812 (Best: 0.0002655 @iter11315) ([92m↓13.24%[0m) [0.11% of initial]
[Iter 11640/20000] Loss: 0.0003263 (Best: 0.0002655 @iter11315) ([91m↑16.01%[0m) [0.13% of initial]
[Iter 11650/20000] Loss: 0.0003286 (Best: 0.0002655 @iter11315) ([91m↑0.71%[0m) [0.13% of initial]
[Iter 11660/20000] Loss: 0.0003317 (Best: 0.0002655 @iter11315) ([91m↑0.93%[0m) [0.13% of initial]
[Iter 11670/20000] Loss: 0.0003098 (Best: 0.0002655 @iter11315) ([92m↓6.59%[0m) [0.12% of initial]
[Iter 11680/20000] Loss: 0.0003173 (Best: 0.0002655 @iter11315) ([91m↑2.41%[0m) [0.13% of initial]
[Iter 11690/20000] Loss: 0.0003017 (Best: 0.0002655 @iter11315) ([92m↓4.92%[0m) [0.12% of initial]
Iter:11699, L1 loss=0.000319, Total loss=0.0002772, Time:50
[Iter 11700/20000] Loss: 0.0002813 (Best: 0.0002655 @iter11315) ([92m↓6.75%[0m) [0.11% of initial]
[Iter 11710/20000] Loss: 0.0002790 (Best: 0.0002556 @iter11707) ([92m↓0.83%[0m) [0.11% of initial]
[Iter 11720/20000] Loss: 0.0002741 (Best: 0.0002512 @iter11719) ([92m↓1.75%[0m) [0.11% of initial]
[Iter 11730/20000] Loss: 0.0002963 (Best: 0.0002512 @iter11719) ([91m↑8.11%[0m) [0.12% of initial]
[Iter 11740/20000] Loss: 0.0003132 (Best: 0.0002512 @iter11719) ([91m↑5.70%[0m) [0.12% of initial]
[Iter 11750/20000] Loss: 0.0003246 (Best: 0.0002512 @iter11719) ([91m↑3.64%[0m) [0.13% of initial]
[Iter 11760/20000] Loss: 0.0003406 (Best: 0.0002512 @iter11719) ([91m↑4.90%[0m) [0.14% of initial]
[Iter 11770/20000] Loss: 0.0002983 (Best: 0.0002512 @iter11719) ([92m↓12.42%[0m) [0.12% of initial]
[Iter 11780/20000] Loss: 0.0003034 (Best: 0.0002512 @iter11719) ([91m↑1.71%[0m) [0.12% of initial]
[Iter 11790/20000] Loss: 0.0002876 (Best: 0.0002512 @iter11719) ([92m↓5.21%[0m) [0.11% of initial]
Iter:11799, L1 loss=0.0003444, Total loss=0.0003007, Time:46
[Iter 11800/20000] Loss: 0.0002809 (Best: 0.0002512 @iter11719) ([92m↓2.32%[0m) [0.11% of initial]
[Iter 11810/20000] Loss: 0.0002648 (Best: 0.0002512 @iter11719) ([92m↓5.75%[0m) [0.11% of initial]
[Iter 11820/20000] Loss: 0.0002924 (Best: 0.0002512 @iter11719) ([91m↑10.44%[0m) [0.12% of initial]
[Iter 11830/20000] Loss: 0.0003237 (Best: 0.0002512 @iter11719) ([91m↑10.69%[0m) [0.13% of initial]
[Iter 11840/20000] Loss: 0.0003233 (Best: 0.0002512 @iter11719) ([92m↓0.11%[0m) [0.13% of initial]
[Iter 11850/20000] Loss: 0.0003092 (Best: 0.0002512 @iter11719) ([92m↓4.37%[0m) [0.12% of initial]
[Iter 11860/20000] Loss: 0.0002992 (Best: 0.0002512 @iter11719) ([92m↓3.24%[0m) [0.12% of initial]
[Iter 11870/20000] Loss: 0.0002943 (Best: 0.0002512 @iter11719) ([92m↓1.62%[0m) [0.12% of initial]
[Iter 11880/20000] Loss: 0.0002974 (Best: 0.0002512 @iter11719) ([91m↑1.05%[0m) [0.12% of initial]
[Iter 11890/20000] Loss: 0.0003610 (Best: 0.0002512 @iter11719) ([91m↑21.37%[0m) [0.14% of initial]
Iter:11899, L1 loss=0.0003594, Total loss=0.0003372, Time:49
[Iter 11900/20000] Loss: 0.0003327 (Best: 0.0002512 @iter11719) ([92m↓7.84%[0m) [0.13% of initial]
[Iter 11910/20000] Loss: 0.0003429 (Best: 0.0002512 @iter11719) ([91m↑3.07%[0m) [0.14% of initial]
[Iter 11920/20000] Loss: 0.0003818 (Best: 0.0002512 @iter11719) ([91m↑11.34%[0m) [0.15% of initial]
[Iter 11930/20000] Loss: 0.0003416 (Best: 0.0002512 @iter11719) ([92m↓10.54%[0m) [0.14% of initial]
[Iter 11940/20000] Loss: 0.0003106 (Best: 0.0002512 @iter11719) ([92m↓9.07%[0m) [0.12% of initial]
[Iter 11950/20000] Loss: 0.0003148 (Best: 0.0002512 @iter11719) ([91m↑1.37%[0m) [0.13% of initial]
[Iter 11960/20000] Loss: 0.0002715 (Best: 0.0002512 @iter11719) ([92m↓13.78%[0m) [0.11% of initial]
[Iter 11970/20000] Loss: 0.0002977 (Best: 0.0002512 @iter11719) ([91m↑9.66%[0m) [0.12% of initial]
[Iter 11980/20000] Loss: 0.0002754 (Best: 0.0002512 @iter11719) ([92m↓7.48%[0m) [0.11% of initial]
[Iter 11990/20000] Loss: 0.0002914 (Best: 0.0002512 @iter11719) ([91m↑5.81%[0m) [0.12% of initial]
Iter:11999, L1 loss=0.0003212, Total loss=0.0002881, Time:53
[Iter 12000/20000] Loss: 0.0003265 (Best: 0.0002512 @iter11719) ([91m↑12.04%[0m) [0.13% of initial]
Pruning 10 points (0.0%) from gaussian0 at iteration 12000
Pruning 11 points (0.0%) from gaussian1 at iteration 12000
[Iter 12010/20000] Loss: 0.0212707 (Best: 0.0002512 @iter11719) ([91m↑6414.70%[0m) [8.45% of initial]
[Iter 12020/20000] Loss: 0.0058984 (Best: 0.0002512 @iter11719) ([92m↓72.27%[0m) [2.34% of initial]
[Iter 12030/20000] Loss: 0.0040671 (Best: 0.0002512 @iter11719) ([92m↓31.05%[0m) [1.62% of initial]
[Iter 12040/20000] Loss: 0.0022118 (Best: 0.0002512 @iter11719) ([92m↓45.62%[0m) [0.88% of initial]
[Iter 12050/20000] Loss: 0.0014724 (Best: 0.0002512 @iter11719) ([92m↓33.43%[0m) [0.58% of initial]
[Iter 12060/20000] Loss: 0.0010354 (Best: 0.0002512 @iter11719) ([92m↓29.68%[0m) [0.41% of initial]
[Iter 12070/20000] Loss: 0.0007856 (Best: 0.0002512 @iter11719) ([92m↓24.13%[0m) [0.31% of initial]
[Iter 12080/20000] Loss: 0.0006639 (Best: 0.0002512 @iter11719) ([92m↓15.49%[0m) [0.26% of initial]
[Iter 12090/20000] Loss: 0.0005797 (Best: 0.0002512 @iter11719) ([92m↓12.69%[0m) [0.23% of initial]
Iter:12099, L1 loss=0.0006134, Total loss=0.0005614, Time:54
[Iter 12100/20000] Loss: 0.0005358 (Best: 0.0002512 @iter11719) ([92m↓7.56%[0m) [0.21% of initial]
[Iter 12110/20000] Loss: 0.0004911 (Best: 0.0002512 @iter11719) ([92m↓8.35%[0m) [0.20% of initial]
[Iter 12120/20000] Loss: 0.0004632 (Best: 0.0002512 @iter11719) ([92m↓5.67%[0m) [0.18% of initial]
[Iter 12130/20000] Loss: 0.0004371 (Best: 0.0002512 @iter11719) ([92m↓5.63%[0m) [0.17% of initial]
[Iter 12140/20000] Loss: 0.0004161 (Best: 0.0002512 @iter11719) ([92m↓4.82%[0m) [0.17% of initial]
[Iter 12150/20000] Loss: 0.0004198 (Best: 0.0002512 @iter11719) ([91m↑0.89%[0m) [0.17% of initial]
[Iter 12160/20000] Loss: 0.0003979 (Best: 0.0002512 @iter11719) ([92m↓5.21%[0m) [0.16% of initial]
[Iter 12170/20000] Loss: 0.0003804 (Best: 0.0002512 @iter11719) ([92m↓4.40%[0m) [0.15% of initial]
[Iter 12180/20000] Loss: 0.0003744 (Best: 0.0002512 @iter11719) ([92m↓1.58%[0m) [0.15% of initial]
[Iter 12190/20000] Loss: 0.0003734 (Best: 0.0002512 @iter11719) ([92m↓0.25%[0m) [0.15% of initial]
Iter:12199, L1 loss=0.0004077, Total loss=0.0003718, Time:52
[Iter 12200/20000] Loss: 0.0003595 (Best: 0.0002512 @iter11719) ([92m↓3.73%[0m) [0.14% of initial]
[Iter 12210/20000] Loss: 0.0003699 (Best: 0.0002512 @iter11719) ([91m↑2.90%[0m) [0.15% of initial]
[Iter 12220/20000] Loss: 0.0003571 (Best: 0.0002512 @iter11719) ([92m↓3.48%[0m) [0.14% of initial]
[Iter 12230/20000] Loss: 0.0003584 (Best: 0.0002512 @iter11719) ([91m↑0.37%[0m) [0.14% of initial]
[Iter 12240/20000] Loss: 0.0003690 (Best: 0.0002512 @iter11719) ([91m↑2.97%[0m) [0.15% of initial]
[Iter 12250/20000] Loss: 0.0003518 (Best: 0.0002512 @iter11719) ([92m↓4.68%[0m) [0.14% of initial]
[Iter 12260/20000] Loss: 0.0003478 (Best: 0.0002512 @iter11719) ([92m↓1.12%[0m) [0.14% of initial]
[Iter 12270/20000] Loss: 0.0003456 (Best: 0.0002512 @iter11719) ([92m↓0.65%[0m) [0.14% of initial]
[Iter 12280/20000] Loss: 0.0003544 (Best: 0.0002512 @iter11719) ([91m↑2.56%[0m) [0.14% of initial]
[Iter 12290/20000] Loss: 0.0003807 (Best: 0.0002512 @iter11719) ([91m↑7.43%[0m) [0.15% of initial]
Iter:12299, L1 loss=0.000405, Total loss=0.0003461, Time:50
[Iter 12300/20000] Loss: 0.0003496 (Best: 0.0002512 @iter11719) ([92m↓8.18%[0m) [0.14% of initial]
[Iter 12310/20000] Loss: 0.0003398 (Best: 0.0002512 @iter11719) ([92m↓2.79%[0m) [0.14% of initial]
[Iter 12320/20000] Loss: 0.0003402 (Best: 0.0002512 @iter11719) ([91m↑0.10%[0m) [0.14% of initial]
[Iter 12330/20000] Loss: 0.0003565 (Best: 0.0002512 @iter11719) ([91m↑4.79%[0m) [0.14% of initial]
[Iter 12340/20000] Loss: 0.0003497 (Best: 0.0002512 @iter11719) ([92m↓1.91%[0m) [0.14% of initial]
[Iter 12350/20000] Loss: 0.0003479 (Best: 0.0002512 @iter11719) ([92m↓0.52%[0m) [0.14% of initial]
[Iter 12360/20000] Loss: 0.0003437 (Best: 0.0002512 @iter11719) ([92m↓1.19%[0m) [0.14% of initial]
[Iter 12370/20000] Loss: 0.0003330 (Best: 0.0002512 @iter11719) ([92m↓3.12%[0m) [0.13% of initial]
[Iter 12380/20000] Loss: 0.0003365 (Best: 0.0002512 @iter11719) ([91m↑1.04%[0m) [0.13% of initial]
[Iter 12390/20000] Loss: 0.0003397 (Best: 0.0002512 @iter11719) ([91m↑0.96%[0m) [0.13% of initial]
Iter:12399, L1 loss=0.0003642, Total loss=0.0003278, Time:54
[Iter 12400/20000] Loss: 0.0003363 (Best: 0.0002512 @iter11719) ([92m↓1.01%[0m) [0.13% of initial]
[Iter 12410/20000] Loss: 0.0003511 (Best: 0.0002512 @iter11719) ([91m↑4.42%[0m) [0.14% of initial]
[Iter 12420/20000] Loss: 0.0003427 (Best: 0.0002512 @iter11719) ([92m↓2.41%[0m) [0.14% of initial]
[Iter 12430/20000] Loss: 0.0003407 (Best: 0.0002512 @iter11719) ([92m↓0.58%[0m) [0.14% of initial]
[Iter 12440/20000] Loss: 0.0003471 (Best: 0.0002512 @iter11719) ([91m↑1.88%[0m) [0.14% of initial]
[Iter 12450/20000] Loss: 0.0003495 (Best: 0.0002512 @iter11719) ([91m↑0.69%[0m) [0.14% of initial]
[Iter 12460/20000] Loss: 0.0003324 (Best: 0.0002512 @iter11719) ([92m↓4.88%[0m) [0.13% of initial]
[Iter 12470/20000] Loss: 0.0003470 (Best: 0.0002512 @iter11719) ([91m↑4.40%[0m) [0.14% of initial]
[Iter 12480/20000] Loss: 0.0003447 (Best: 0.0002512 @iter11719) ([92m↓0.68%[0m) [0.14% of initial]
[Iter 12490/20000] Loss: 0.0003414 (Best: 0.0002512 @iter11719) ([92m↓0.95%[0m) [0.14% of initial]
Iter:12499, L1 loss=0.0003831, Total loss=0.0003763, Time:56
[Iter 12500/20000] Loss: 0.0003674 (Best: 0.0002512 @iter11719) ([91m↑7.61%[0m) [0.15% of initial]
Pruning 20 points (0.0%) from gaussian0 at iteration 12500
Pruning 18 points (0.0%) from gaussian1 at iteration 12500
[Iter 12510/20000] Loss: 0.0007595 (Best: 0.0002512 @iter11719) ([91m↑106.72%[0m) [0.30% of initial]
[Iter 12520/20000] Loss: 0.0005669 (Best: 0.0002512 @iter11719) ([92m↓25.36%[0m) [0.23% of initial]
[Iter 12530/20000] Loss: 0.0004234 (Best: 0.0002512 @iter11719) ([92m↓25.31%[0m) [0.17% of initial]
[Iter 12540/20000] Loss: 0.0003804 (Best: 0.0002512 @iter11719) ([92m↓10.15%[0m) [0.15% of initial]
[Iter 12550/20000] Loss: 0.0003431 (Best: 0.0002512 @iter11719) ([92m↓9.81%[0m) [0.14% of initial]
[Iter 12560/20000] Loss: 0.0003292 (Best: 0.0002512 @iter11719) ([92m↓4.04%[0m) [0.13% of initial]
[Iter 12570/20000] Loss: 0.0003419 (Best: 0.0002512 @iter11719) ([91m↑3.86%[0m) [0.14% of initial]
[Iter 12580/20000] Loss: 0.0003540 (Best: 0.0002512 @iter11719) ([91m↑3.52%[0m) [0.14% of initial]
[Iter 12590/20000] Loss: 0.0003404 (Best: 0.0002512 @iter11719) ([92m↓3.85%[0m) [0.14% of initial]
Iter:12599, L1 loss=0.0004725, Total loss=0.0003984, Time:59
[Iter 12600/20000] Loss: 0.0003599 (Best: 0.0002512 @iter11719) ([91m↑5.74%[0m) [0.14% of initial]
[Iter 12610/20000] Loss: 0.0003524 (Best: 0.0002512 @iter11719) ([92m↓2.08%[0m) [0.14% of initial]
[Iter 12620/20000] Loss: 0.0003421 (Best: 0.0002512 @iter11719) ([92m↓2.92%[0m) [0.14% of initial]
[Iter 12630/20000] Loss: 0.0003544 (Best: 0.0002512 @iter11719) ([91m↑3.58%[0m) [0.14% of initial]
[Iter 12640/20000] Loss: 0.0003528 (Best: 0.0002512 @iter11719) ([92m↓0.44%[0m) [0.14% of initial]
[Iter 12650/20000] Loss: 0.0003401 (Best: 0.0002512 @iter11719) ([92m↓3.60%[0m) [0.14% of initial]
[Iter 12660/20000] Loss: 0.0003517 (Best: 0.0002512 @iter11719) ([91m↑3.39%[0m) [0.14% of initial]
[Iter 12670/20000] Loss: 0.0003542 (Best: 0.0002512 @iter11719) ([91m↑0.72%[0m) [0.14% of initial]
[Iter 12680/20000] Loss: 0.0003750 (Best: 0.0002512 @iter11719) ([91m↑5.86%[0m) [0.15% of initial]
[Iter 12690/20000] Loss: 0.0003546 (Best: 0.0002512 @iter11719) ([92m↓5.43%[0m) [0.14% of initial]
Iter:12699, L1 loss=0.0004588, Total loss=0.0003713, Time:53
[Iter 12700/20000] Loss: 0.0003581 (Best: 0.0002512 @iter11719) ([91m↑0.98%[0m) [0.14% of initial]
[Iter 12710/20000] Loss: 0.0003567 (Best: 0.0002512 @iter11719) ([92m↓0.39%[0m) [0.14% of initial]
[Iter 12720/20000] Loss: 0.0003468 (Best: 0.0002512 @iter11719) ([92m↓2.76%[0m) [0.14% of initial]
[Iter 12730/20000] Loss: 0.0003659 (Best: 0.0002512 @iter11719) ([91m↑5.51%[0m) [0.15% of initial]
[Iter 12740/20000] Loss: 0.0003508 (Best: 0.0002512 @iter11719) ([92m↓4.15%[0m) [0.14% of initial]
[Iter 12750/20000] Loss: 0.0003299 (Best: 0.0002512 @iter11719) ([92m↓5.94%[0m) [0.13% of initial]
[Iter 12760/20000] Loss: 0.0003337 (Best: 0.0002512 @iter11719) ([91m↑1.16%[0m) [0.13% of initial]
[Iter 12770/20000] Loss: 0.0003584 (Best: 0.0002512 @iter11719) ([91m↑7.37%[0m) [0.14% of initial]
[Iter 12780/20000] Loss: 0.0003751 (Best: 0.0002512 @iter11719) ([91m↑4.68%[0m) [0.15% of initial]
[Iter 12790/20000] Loss: 0.0003569 (Best: 0.0002512 @iter11719) ([92m↓4.87%[0m) [0.14% of initial]
Iter:12799, L1 loss=0.0003686, Total loss=0.0003517, Time:51
[Iter 12800/20000] Loss: 0.0003588 (Best: 0.0002512 @iter11719) ([91m↑0.53%[0m) [0.14% of initial]
[Iter 12810/20000] Loss: 0.0003821 (Best: 0.0002512 @iter11719) ([91m↑6.51%[0m) [0.15% of initial]
[Iter 12820/20000] Loss: 0.0003572 (Best: 0.0002512 @iter11719) ([92m↓6.52%[0m) [0.14% of initial]
[Iter 12830/20000] Loss: 0.0003467 (Best: 0.0002512 @iter11719) ([92m↓2.92%[0m) [0.14% of initial]
[Iter 12840/20000] Loss: 0.0003556 (Best: 0.0002512 @iter11719) ([91m↑2.56%[0m) [0.14% of initial]
[Iter 12850/20000] Loss: 0.0003364 (Best: 0.0002512 @iter11719) ([92m↓5.39%[0m) [0.13% of initial]
[Iter 12860/20000] Loss: 0.0003235 (Best: 0.0002512 @iter11719) ([92m↓3.86%[0m) [0.13% of initial]
[Iter 12870/20000] Loss: 0.0003189 (Best: 0.0002512 @iter11719) ([92m↓1.42%[0m) [0.13% of initial]
[Iter 12880/20000] Loss: 0.0003139 (Best: 0.0002512 @iter11719) ([92m↓1.57%[0m) [0.12% of initial]
[Iter 12890/20000] Loss: 0.0003171 (Best: 0.0002512 @iter11719) ([91m↑1.01%[0m) [0.13% of initial]
Iter:12899, L1 loss=0.0003607, Total loss=0.0003259, Time:53
[Iter 12900/20000] Loss: 0.0003139 (Best: 0.0002512 @iter11719) ([92m↓1.00%[0m) [0.12% of initial]
[Iter 12910/20000] Loss: 0.0003176 (Best: 0.0002512 @iter11719) ([91m↑1.18%[0m) [0.13% of initial]
[Iter 12920/20000] Loss: 0.0003300 (Best: 0.0002512 @iter11719) ([91m↑3.92%[0m) [0.13% of initial]
[Iter 12930/20000] Loss: 0.0003524 (Best: 0.0002512 @iter11719) ([91m↑6.77%[0m) [0.14% of initial]
[Iter 12940/20000] Loss: 0.0003745 (Best: 0.0002512 @iter11719) ([91m↑6.28%[0m) [0.15% of initial]
[Iter 12950/20000] Loss: 0.0003806 (Best: 0.0002512 @iter11719) ([91m↑1.63%[0m) [0.15% of initial]
[Iter 12960/20000] Loss: 0.0004019 (Best: 0.0002512 @iter11719) ([91m↑5.58%[0m) [0.16% of initial]
[Iter 12970/20000] Loss: 0.0003322 (Best: 0.0002512 @iter11719) ([92m↓17.32%[0m) [0.13% of initial]
[Iter 12980/20000] Loss: 0.0003639 (Best: 0.0002512 @iter11719) ([91m↑9.53%[0m) [0.14% of initial]
[Iter 12990/20000] Loss: 0.0003614 (Best: 0.0002512 @iter11719) ([92m↓0.70%[0m) [0.14% of initial]
Iter:12999, L1 loss=0.0003632, Total loss=0.0003274, Time:51
[Iter 13000/20000] Loss: 0.0003325 (Best: 0.0002512 @iter11719) ([92m↓7.98%[0m) [0.13% of initial]
Pruning 10 points (0.0%) from gaussian0 at iteration 13000
Pruning 13 points (0.0%) from gaussian1 at iteration 13000
[Iter 13010/20000] Loss: 0.0008516 (Best: 0.0002512 @iter11719) ([91m↑156.11%[0m) [0.34% of initial]
[Iter 13020/20000] Loss: 0.0005368 (Best: 0.0002512 @iter11719) ([92m↓36.97%[0m) [0.21% of initial]
[Iter 13030/20000] Loss: 0.0004252 (Best: 0.0002512 @iter11719) ([92m↓20.79%[0m) [0.17% of initial]
[Iter 13040/20000] Loss: 0.0003767 (Best: 0.0002512 @iter11719) ([92m↓11.40%[0m) [0.15% of initial]
[Iter 13050/20000] Loss: 0.0003378 (Best: 0.0002512 @iter11719) ([92m↓10.33%[0m) [0.13% of initial]
[Iter 13060/20000] Loss: 0.0003296 (Best: 0.0002512 @iter11719) ([92m↓2.42%[0m) [0.13% of initial]
[Iter 13070/20000] Loss: 0.0003283 (Best: 0.0002512 @iter11719) ([92m↓0.39%[0m) [0.13% of initial]
[Iter 13080/20000] Loss: 0.0003200 (Best: 0.0002512 @iter11719) ([92m↓2.52%[0m) [0.13% of initial]
[Iter 13090/20000] Loss: 0.0003366 (Best: 0.0002512 @iter11719) ([91m↑5.17%[0m) [0.13% of initial]
Iter:13099, L1 loss=0.000384, Total loss=0.0003333, Time:54
[Iter 13100/20000] Loss: 0.0003389 (Best: 0.0002512 @iter11719) ([91m↑0.69%[0m) [0.13% of initial]
[Iter 13110/20000] Loss: 0.0003439 (Best: 0.0002512 @iter11719) ([91m↑1.49%[0m) [0.14% of initial]
[Iter 13120/20000] Loss: 0.0003348 (Best: 0.0002512 @iter11719) ([92m↓2.65%[0m) [0.13% of initial]
[Iter 13130/20000] Loss: 0.0003407 (Best: 0.0002512 @iter11719) ([91m↑1.75%[0m) [0.14% of initial]
[Iter 13140/20000] Loss: 0.0003338 (Best: 0.0002512 @iter11719) ([92m↓2.01%[0m) [0.13% of initial]
[Iter 13150/20000] Loss: 0.0003124 (Best: 0.0002512 @iter11719) ([92m↓6.41%[0m) [0.12% of initial]
[Iter 13160/20000] Loss: 0.0003310 (Best: 0.0002512 @iter11719) ([91m↑5.94%[0m) [0.13% of initial]
[Iter 13170/20000] Loss: 0.0003227 (Best: 0.0002512 @iter11719) ([92m↓2.51%[0m) [0.13% of initial]
[Iter 13180/20000] Loss: 0.0003472 (Best: 0.0002512 @iter11719) ([91m↑7.61%[0m) [0.14% of initial]
[Iter 13190/20000] Loss: 0.0003346 (Best: 0.0002512 @iter11719) ([92m↓3.65%[0m) [0.13% of initial]
Iter:13199, L1 loss=0.0003925, Total loss=0.0003292, Time:61
[Iter 13200/20000] Loss: 0.0003433 (Best: 0.0002512 @iter11719) ([91m↑2.61%[0m) [0.14% of initial]
[Iter 13210/20000] Loss: 0.0003545 (Best: 0.0002512 @iter11719) ([91m↑3.24%[0m) [0.14% of initial]
[Iter 13220/20000] Loss: 0.0003120 (Best: 0.0002512 @iter11719) ([92m↓11.97%[0m) [0.12% of initial]
[Iter 13230/20000] Loss: 0.0003533 (Best: 0.0002512 @iter11719) ([91m↑13.22%[0m) [0.14% of initial]
[Iter 13240/20000] Loss: 0.0003316 (Best: 0.0002512 @iter11719) ([92m↓6.13%[0m) [0.13% of initial]
[Iter 13250/20000] Loss: 0.0003236 (Best: 0.0002512 @iter11719) ([92m↓2.40%[0m) [0.13% of initial]
[Iter 13260/20000] Loss: 0.0003182 (Best: 0.0002512 @iter11719) ([92m↓1.68%[0m) [0.13% of initial]
[Iter 13270/20000] Loss: 0.0003276 (Best: 0.0002512 @iter11719) ([91m↑2.96%[0m) [0.13% of initial]
[Iter 13280/20000] Loss: 0.0003330 (Best: 0.0002512 @iter11719) ([91m↑1.64%[0m) [0.13% of initial]
[Iter 13290/20000] Loss: 0.0003075 (Best: 0.0002512 @iter11719) ([92m↓7.66%[0m) [0.12% of initial]
Iter:13299, L1 loss=0.0003349, Total loss=0.0003066, Time:54
[Iter 13300/20000] Loss: 0.0003072 (Best: 0.0002512 @iter11719) ([92m↓0.10%[0m) [0.12% of initial]
[Iter 13310/20000] Loss: 0.0003030 (Best: 0.0002512 @iter11719) ([92m↓1.37%[0m) [0.12% of initial]
[Iter 13320/20000] Loss: 0.0003073 (Best: 0.0002512 @iter11719) ([91m↑1.43%[0m) [0.12% of initial]
[Iter 13330/20000] Loss: 0.0003152 (Best: 0.0002512 @iter11719) ([91m↑2.58%[0m) [0.13% of initial]
[Iter 13340/20000] Loss: 0.0003152 (Best: 0.0002512 @iter11719) ([91m↑0.01%[0m) [0.13% of initial]
[Iter 13350/20000] Loss: 0.0003043 (Best: 0.0002512 @iter11719) ([92m↓3.47%[0m) [0.12% of initial]
[Iter 13360/20000] Loss: 0.0003189 (Best: 0.0002512 @iter11719) ([91m↑4.81%[0m) [0.13% of initial]
[Iter 13370/20000] Loss: 0.0002993 (Best: 0.0002512 @iter11719) ([92m↓6.14%[0m) [0.12% of initial]
[Iter 13380/20000] Loss: 0.0003377 (Best: 0.0002512 @iter11719) ([91m↑12.81%[0m) [0.13% of initial]
[Iter 13390/20000] Loss: 0.0003655 (Best: 0.0002512 @iter11719) ([91m↑8.22%[0m) [0.15% of initial]
Iter:13399, L1 loss=0.0003427, Total loss=0.0003067, Time:52
[Iter 13400/20000] Loss: 0.0003381 (Best: 0.0002512 @iter11719) ([92m↓7.47%[0m) [0.13% of initial]
[Iter 13410/20000] Loss: 0.0003184 (Best: 0.0002512 @iter11719) ([92m↓5.84%[0m) [0.13% of initial]
[Iter 13420/20000] Loss: 0.0003176 (Best: 0.0002512 @iter11719) ([92m↓0.25%[0m) [0.13% of initial]
[Iter 13430/20000] Loss: 0.0002990 (Best: 0.0002512 @iter11719) ([92m↓5.85%[0m) [0.12% of initial]
[Iter 13440/20000] Loss: 0.0003108 (Best: 0.0002512 @iter11719) ([91m↑3.92%[0m) [0.12% of initial]
[Iter 13450/20000] Loss: 0.0003012 (Best: 0.0002512 @iter11719) ([92m↓3.08%[0m) [0.12% of initial]
[Iter 13460/20000] Loss: 0.0002983 (Best: 0.0002512 @iter11719) ([92m↓0.96%[0m) [0.12% of initial]
[Iter 13470/20000] Loss: 0.0003240 (Best: 0.0002512 @iter11719) ([91m↑8.63%[0m) [0.13% of initial]
[Iter 13480/20000] Loss: 0.0003163 (Best: 0.0002512 @iter11719) ([92m↓2.40%[0m) [0.13% of initial]
[Iter 13490/20000] Loss: 0.0003914 (Best: 0.0002512 @iter11719) ([91m↑23.76%[0m) [0.16% of initial]
Iter:13499, L1 loss=0.0003673, Total loss=0.0003408, Time:56
[Iter 13500/20000] Loss: 0.0003521 (Best: 0.0002512 @iter11719) ([92m↓10.06%[0m) [0.14% of initial]
Pruning 5 points (0.0%) from gaussian0 at iteration 13500
Pruning 14 points (0.0%) from gaussian1 at iteration 13500
[Iter 13510/20000] Loss: 0.0006619 (Best: 0.0002512 @iter11719) ([91m↑88.01%[0m) [0.26% of initial]
[Iter 13520/20000] Loss: 0.0004606 (Best: 0.0002512 @iter11719) ([92m↓30.41%[0m) [0.18% of initial]
[Iter 13530/20000] Loss: 0.0003988 (Best: 0.0002512 @iter11719) ([92m↓13.41%[0m) [0.16% of initial]
[Iter 13540/20000] Loss: 0.0003431 (Best: 0.0002512 @iter11719) ([92m↓13.98%[0m) [0.14% of initial]
[Iter 13550/20000] Loss: 0.0003171 (Best: 0.0002512 @iter11719) ([92m↓7.57%[0m) [0.13% of initial]
[Iter 13560/20000] Loss: 0.0002985 (Best: 0.0002512 @iter11719) ([92m↓5.86%[0m) [0.12% of initial]
[Iter 13570/20000] Loss: 0.0002841 (Best: 0.0002512 @iter11719) ([92m↓4.81%[0m) [0.11% of initial]
[Iter 13580/20000] Loss: 0.0003077 (Best: 0.0002512 @iter11719) ([91m↑8.29%[0m) [0.12% of initial]
[Iter 13590/20000] Loss: 0.0003117 (Best: 0.0002512 @iter11719) ([91m↑1.31%[0m) [0.12% of initial]
Iter:13599, L1 loss=0.0003252, Total loss=0.0002896, Time:50
[Iter 13600/20000] Loss: 0.0002862 (Best: 0.0002512 @iter11719) ([92m↓8.19%[0m) [0.11% of initial]
[Iter 13610/20000] Loss: 0.0002998 (Best: 0.0002512 @iter11719) ([91m↑4.75%[0m) [0.12% of initial]
[Iter 13620/20000] Loss: 0.0002900 (Best: 0.0002512 @iter11719) ([92m↓3.28%[0m) [0.12% of initial]
[Iter 13630/20000] Loss: 0.0002806 (Best: 0.0002512 @iter11719) ([92m↓3.21%[0m) [0.11% of initial]
[Iter 13640/20000] Loss: 0.0003253 (Best: 0.0002512 @iter11719) ([91m↑15.91%[0m) [0.13% of initial]
[Iter 13650/20000] Loss: 0.0003283 (Best: 0.0002512 @iter11719) ([91m↑0.92%[0m) [0.13% of initial]
[Iter 13660/20000] Loss: 0.0003144 (Best: 0.0002512 @iter11719) ([92m↓4.22%[0m) [0.12% of initial]
[Iter 13670/20000] Loss: 0.0003381 (Best: 0.0002512 @iter11719) ([91m↑7.54%[0m) [0.13% of initial]
[Iter 13680/20000] Loss: 0.0003299 (Best: 0.0002512 @iter11719) ([92m↓2.42%[0m) [0.13% of initial]
[Iter 13690/20000] Loss: 0.0003458 (Best: 0.0002512 @iter11719) ([91m↑4.82%[0m) [0.14% of initial]
Iter:13699, L1 loss=0.0004009, Total loss=0.0003557, Time:53
[Iter 13700/20000] Loss: 0.0003348 (Best: 0.0002512 @iter11719) ([92m↓3.19%[0m) [0.13% of initial]
[Iter 13710/20000] Loss: 0.0003067 (Best: 0.0002512 @iter11719) ([92m↓8.41%[0m) [0.12% of initial]
[Iter 13720/20000] Loss: 0.0003132 (Best: 0.0002512 @iter11719) ([91m↑2.12%[0m) [0.12% of initial]
[Iter 13730/20000] Loss: 0.0003237 (Best: 0.0002512 @iter11719) ([91m↑3.35%[0m) [0.13% of initial]
[Iter 13740/20000] Loss: 0.0003278 (Best: 0.0002512 @iter11719) ([91m↑1.28%[0m) [0.13% of initial]
[Iter 13750/20000] Loss: 0.0003037 (Best: 0.0002512 @iter11719) ([92m↓7.35%[0m) [0.12% of initial]
[Iter 13760/20000] Loss: 0.0003101 (Best: 0.0002512 @iter11719) ([91m↑2.10%[0m) [0.12% of initial]
[Iter 13770/20000] Loss: 0.0003239 (Best: 0.0002512 @iter11719) ([91m↑4.45%[0m) [0.13% of initial]
[Iter 13780/20000] Loss: 0.0003272 (Best: 0.0002512 @iter11719) ([91m↑1.04%[0m) [0.13% of initial]
[Iter 13790/20000] Loss: 0.0003896 (Best: 0.0002512 @iter11719) ([91m↑19.06%[0m) [0.15% of initial]
Iter:13799, L1 loss=0.0003949, Total loss=0.0003305, Time:51
[Iter 13800/20000] Loss: 0.0003249 (Best: 0.0002512 @iter11719) ([92m↓16.62%[0m) [0.13% of initial]
[Iter 13810/20000] Loss: 0.0003271 (Best: 0.0002512 @iter11719) ([91m↑0.70%[0m) [0.13% of initial]
[Iter 13820/20000] Loss: 0.0003192 (Best: 0.0002512 @iter11719) ([92m↓2.43%[0m) [0.13% of initial]
[Iter 13830/20000] Loss: 0.0003326 (Best: 0.0002512 @iter11719) ([91m↑4.20%[0m) [0.13% of initial]
[Iter 13840/20000] Loss: 0.0003183 (Best: 0.0002512 @iter11719) ([92m↓4.31%[0m) [0.13% of initial]
[Iter 13850/20000] Loss: 0.0002948 (Best: 0.0002512 @iter11719) ([92m↓7.37%[0m) [0.12% of initial]
[Iter 13860/20000] Loss: 0.0002932 (Best: 0.0002512 @iter11719) ([92m↓0.54%[0m) [0.12% of initial]
[Iter 13870/20000] Loss: 0.0002849 (Best: 0.0002512 @iter11719) ([92m↓2.82%[0m) [0.11% of initial]
[Iter 13880/20000] Loss: 0.0002930 (Best: 0.0002512 @iter11719) ([91m↑2.85%[0m) [0.12% of initial]
[Iter 13890/20000] Loss: 0.0002985 (Best: 0.0002512 @iter11719) ([91m↑1.86%[0m) [0.12% of initial]
Iter:13899, L1 loss=0.0003926, Total loss=0.0003381, Time:30
[Iter 13900/20000] Loss: 0.0003184 (Best: 0.0002512 @iter11719) ([91m↑6.67%[0m) [0.13% of initial]
[Iter 13910/20000] Loss: 0.0003010 (Best: 0.0002512 @iter11719) ([92m↓5.48%[0m) [0.12% of initial]
[Iter 13920/20000] Loss: 0.0003194 (Best: 0.0002512 @iter11719) ([91m↑6.11%[0m) [0.13% of initial]
[Iter 13930/20000] Loss: 0.0002881 (Best: 0.0002512 @iter11719) ([92m↓9.80%[0m) [0.11% of initial]
[Iter 13940/20000] Loss: 0.0002996 (Best: 0.0002512 @iter11719) ([91m↑4.01%[0m) [0.12% of initial]
[Iter 13950/20000] Loss: 0.0003137 (Best: 0.0002512 @iter11719) ([91m↑4.70%[0m) [0.12% of initial]
[Iter 13960/20000] Loss: 0.0002905 (Best: 0.0002512 @iter11719) ([92m↓7.37%[0m) [0.12% of initial]
[Iter 13970/20000] Loss: 0.0003077 (Best: 0.0002512 @iter11719) ([91m↑5.90%[0m) [0.12% of initial]
[Iter 13980/20000] Loss: 0.0003164 (Best: 0.0002512 @iter11719) ([91m↑2.83%[0m) [0.13% of initial]
[Iter 13990/20000] Loss: 0.0003696 (Best: 0.0002512 @iter11719) ([91m↑16.80%[0m) [0.15% of initial]
Iter:13999, L1 loss=0.0003163, Total loss=0.0002825, Time:40
[Iter 14000/20000] Loss: 0.0003215 (Best: 0.0002512 @iter11719) ([92m↓13.01%[0m) [0.13% of initial]
Pruning 7 points (0.0%) from gaussian0 at iteration 14000
Pruning 8 points (0.0%) from gaussian1 at iteration 14000
[Iter 14010/20000] Loss: 0.0006089 (Best: 0.0002512 @iter11719) ([91m↑89.40%[0m) [0.24% of initial]
[Iter 14020/20000] Loss: 0.0004848 (Best: 0.0002512 @iter11719) ([92m↓20.37%[0m) [0.19% of initial]
[Iter 14030/20000] Loss: 0.0003864 (Best: 0.0002512 @iter11719) ([92m↓20.30%[0m) [0.15% of initial]
[Iter 14040/20000] Loss: 0.0003247 (Best: 0.0002512 @iter11719) ([92m↓15.99%[0m) [0.13% of initial]
[Iter 14050/20000] Loss: 0.0003222 (Best: 0.0002512 @iter11719) ([92m↓0.75%[0m) [0.13% of initial]
[Iter 14060/20000] Loss: 0.0003203 (Best: 0.0002512 @iter11719) ([92m↓0.59%[0m) [0.13% of initial]
[Iter 14070/20000] Loss: 0.0003057 (Best: 0.0002512 @iter11719) ([92m↓4.57%[0m) [0.12% of initial]
[Iter 14080/20000] Loss: 0.0002854 (Best: 0.0002512 @iter11719) ([92m↓6.64%[0m) [0.11% of initial]
[Iter 14090/20000] Loss: 0.0002857 (Best: 0.0002512 @iter11719) ([91m↑0.09%[0m) [0.11% of initial]
Iter:14099, L1 loss=0.0003292, Total loss=0.0002767, Time:50
[Iter 14100/20000] Loss: 0.0002816 (Best: 0.0002512 @iter11719) ([92m↓1.43%[0m) [0.11% of initial]
[Iter 14110/20000] Loss: 0.0003082 (Best: 0.0002512 @iter11719) ([91m↑9.45%[0m) [0.12% of initial]
[Iter 14120/20000] Loss: 0.0002998 (Best: 0.0002512 @iter11719) ([92m↓2.70%[0m) [0.12% of initial]
[Iter 14130/20000] Loss: 0.0003212 (Best: 0.0002512 @iter11719) ([91m↑7.14%[0m) [0.13% of initial]
[Iter 14140/20000] Loss: 0.0002921 (Best: 0.0002512 @iter11719) ([92m↓9.08%[0m) [0.12% of initial]
[Iter 14150/20000] Loss: 0.0002919 (Best: 0.0002512 @iter11719) ([92m↓0.04%[0m) [0.12% of initial]
[Iter 14160/20000] Loss: 0.0002782 (Best: 0.0002512 @iter11719) ([92m↓4.72%[0m) [0.11% of initial]
[Iter 14170/20000] Loss: 0.0003173 (Best: 0.0002512 @iter11719) ([91m↑14.06%[0m) [0.13% of initial]
[Iter 14180/20000] Loss: 0.0002887 (Best: 0.0002512 @iter11719) ([92m↓9.01%[0m) [0.11% of initial]
[Iter 14190/20000] Loss: 0.0002855 (Best: 0.0002512 @iter11719) ([92m↓1.12%[0m) [0.11% of initial]
Iter:14199, L1 loss=0.0003189, Total loss=0.0002693, Time:54
[Iter 14200/20000] Loss: 0.0002809 (Best: 0.0002512 @iter11719) ([92m↓1.60%[0m) [0.11% of initial]
[Iter 14210/20000] Loss: 0.0002699 (Best: 0.0002493 @iter14206) ([92m↓3.93%[0m) [0.11% of initial]
[Iter 14220/20000] Loss: 0.0002708 (Best: 0.0002493 @iter14206) ([91m↑0.34%[0m) [0.11% of initial]
[Iter 14230/20000] Loss: 0.0003365 (Best: 0.0002493 @iter14206) ([91m↑24.27%[0m) [0.13% of initial]
[Iter 14240/20000] Loss: 0.0002929 (Best: 0.0002493 @iter14206) ([92m↓12.95%[0m) [0.12% of initial]
[Iter 14250/20000] Loss: 0.0003068 (Best: 0.0002493 @iter14206) ([91m↑4.71%[0m) [0.12% of initial]
[Iter 14260/20000] Loss: 0.0002907 (Best: 0.0002493 @iter14206) ([92m↓5.23%[0m) [0.12% of initial]
[Iter 14270/20000] Loss: 0.0003061 (Best: 0.0002493 @iter14206) ([91m↑5.28%[0m) [0.12% of initial]
[Iter 14280/20000] Loss: 0.0002974 (Best: 0.0002493 @iter14206) ([92m↓2.82%[0m) [0.12% of initial]
[Iter 14290/20000] Loss: 0.0002769 (Best: 0.0002493 @iter14206) ([92m↓6.89%[0m) [0.11% of initial]
Iter:14299, L1 loss=0.0003206, Total loss=0.0002888, Time:51
[Iter 14300/20000] Loss: 0.0002851 (Best: 0.0002493 @iter14206) ([91m↑2.96%[0m) [0.11% of initial]
[Iter 14310/20000] Loss: 0.0003077 (Best: 0.0002493 @iter14206) ([91m↑7.94%[0m) [0.12% of initial]
[Iter 14320/20000] Loss: 0.0002877 (Best: 0.0002493 @iter14206) ([92m↓6.52%[0m) [0.11% of initial]
[Iter 14330/20000] Loss: 0.0002952 (Best: 0.0002493 @iter14206) ([91m↑2.64%[0m) [0.12% of initial]
[Iter 14340/20000] Loss: 0.0003350 (Best: 0.0002493 @iter14206) ([91m↑13.46%[0m) [0.13% of initial]
[Iter 14350/20000] Loss: 0.0003305 (Best: 0.0002493 @iter14206) ([92m↓1.33%[0m) [0.13% of initial]
[Iter 14360/20000] Loss: 0.0003145 (Best: 0.0002493 @iter14206) ([92m↓4.86%[0m) [0.12% of initial]
[Iter 14370/20000] Loss: 0.0002967 (Best: 0.0002493 @iter14206) ([92m↓5.63%[0m) [0.12% of initial]
[Iter 14380/20000] Loss: 0.0003028 (Best: 0.0002493 @iter14206) ([91m↑2.05%[0m) [0.12% of initial]
[Iter 14390/20000] Loss: 0.0003269 (Best: 0.0002493 @iter14206) ([91m↑7.95%[0m) [0.13% of initial]
Iter:14399, L1 loss=0.0003174, Total loss=0.0002769, Time:48
[Iter 14400/20000] Loss: 0.0003023 (Best: 0.0002493 @iter14206) ([92m↓7.52%[0m) [0.12% of initial]
[Iter 14410/20000] Loss: 0.0003104 (Best: 0.0002493 @iter14206) ([91m↑2.68%[0m) [0.12% of initial]
[Iter 14420/20000] Loss: 0.0004075 (Best: 0.0002493 @iter14206) ([91m↑31.27%[0m) [0.16% of initial]
[Iter 14430/20000] Loss: 0.0004108 (Best: 0.0002493 @iter14206) ([91m↑0.82%[0m) [0.16% of initial]
[Iter 14440/20000] Loss: 0.0003810 (Best: 0.0002493 @iter14206) ([92m↓7.24%[0m) [0.15% of initial]
[Iter 14450/20000] Loss: 0.0003557 (Best: 0.0002493 @iter14206) ([92m↓6.65%[0m) [0.14% of initial]
[Iter 14460/20000] Loss: 0.0003477 (Best: 0.0002493 @iter14206) ([92m↓2.25%[0m) [0.14% of initial]
[Iter 14470/20000] Loss: 0.0003323 (Best: 0.0002493 @iter14206) ([92m↓4.41%[0m) [0.13% of initial]
[Iter 14480/20000] Loss: 0.0003135 (Best: 0.0002493 @iter14206) ([92m↓5.68%[0m) [0.12% of initial]
[Iter 14490/20000] Loss: 0.0003179 (Best: 0.0002493 @iter14206) ([91m↑1.40%[0m) [0.13% of initial]
Iter:14499, L1 loss=0.0003442, Total loss=0.000325, Time:54
[Iter 14500/20000] Loss: 0.0003108 (Best: 0.0002493 @iter14206) ([92m↓2.21%[0m) [0.12% of initial]
Pruning 4 points (0.0%) from gaussian0 at iteration 14500
Pruning 10 points (0.0%) from gaussian1 at iteration 14500
[Iter 14510/20000] Loss: 0.0006681 (Best: 0.0002493 @iter14206) ([91m↑114.96%[0m) [0.27% of initial]
[Iter 14520/20000] Loss: 0.0005674 (Best: 0.0002493 @iter14206) ([92m↓15.09%[0m) [0.23% of initial]
[Iter 14530/20000] Loss: 0.0004527 (Best: 0.0002493 @iter14206) ([92m↓20.20%[0m) [0.18% of initial]
[Iter 14540/20000] Loss: 0.0003678 (Best: 0.0002493 @iter14206) ([92m↓18.77%[0m) [0.15% of initial]
[Iter 14550/20000] Loss: 0.0003291 (Best: 0.0002493 @iter14206) ([92m↓10.52%[0m) [0.13% of initial]
[Iter 14560/20000] Loss: 0.0002890 (Best: 0.0002493 @iter14206) ([92m↓12.17%[0m) [0.11% of initial]
[Iter 14570/20000] Loss: 0.0002786 (Best: 0.0002493 @iter14206) ([92m↓3.62%[0m) [0.11% of initial]
[Iter 14580/20000] Loss: 0.0002655 (Best: 0.0002467 @iter14578) ([92m↓4.69%[0m) [0.11% of initial]
[Iter 14590/20000] Loss: 0.0002691 (Best: 0.0002467 @iter14578) ([91m↑1.36%[0m) [0.11% of initial]
Iter:14599, L1 loss=0.0003175, Total loss=0.0002986, Time:50
[Iter 14600/20000] Loss: 0.0002917 (Best: 0.0002467 @iter14578) ([91m↑8.40%[0m) [0.12% of initial]
[Iter 14610/20000] Loss: 0.0003375 (Best: 0.0002467 @iter14578) ([91m↑15.69%[0m) [0.13% of initial]
[Iter 14620/20000] Loss: 0.0002971 (Best: 0.0002467 @iter14578) ([92m↓11.95%[0m) [0.12% of initial]
[Iter 14630/20000] Loss: 0.0002685 (Best: 0.0002467 @iter14578) ([92m↓9.62%[0m) [0.11% of initial]
[Iter 14640/20000] Loss: 0.0002805 (Best: 0.0002467 @iter14578) ([91m↑4.45%[0m) [0.11% of initial]
[Iter 14650/20000] Loss: 0.0002886 (Best: 0.0002467 @iter14578) ([91m↑2.87%[0m) [0.11% of initial]
[Iter 14660/20000] Loss: 0.0002774 (Best: 0.0002467 @iter14578) ([92m↓3.88%[0m) [0.11% of initial]
[Iter 14670/20000] Loss: 0.0002715 (Best: 0.0002467 @iter14578) ([92m↓2.12%[0m) [0.11% of initial]
[Iter 14680/20000] Loss: 0.0002675 (Best: 0.0002467 @iter14578) ([92m↓1.47%[0m) [0.11% of initial]
[Iter 14690/20000] Loss: 0.0002575 (Best: 0.0002454 @iter14689) ([92m↓3.74%[0m) [0.10% of initial]
Iter:14699, L1 loss=0.0003144, Total loss=0.0002788, Time:53
[Iter 14700/20000] Loss: 0.0002735 (Best: 0.0002454 @iter14689) ([91m↑6.22%[0m) [0.11% of initial]
[Iter 14710/20000] Loss: 0.0002639 (Best: 0.0002454 @iter14689) ([92m↓3.51%[0m) [0.10% of initial]
[Iter 14720/20000] Loss: 0.0002746 (Best: 0.0002454 @iter14689) ([91m↑4.05%[0m) [0.11% of initial]
[Iter 14730/20000] Loss: 0.0002813 (Best: 0.0002454 @iter14689) ([91m↑2.44%[0m) [0.11% of initial]
[Iter 14740/20000] Loss: 0.0002608 (Best: 0.0002454 @iter14689) ([92m↓7.29%[0m) [0.10% of initial]
[Iter 14750/20000] Loss: 0.0002613 (Best: 0.0002454 @iter14689) ([91m↑0.17%[0m) [0.10% of initial]
[Iter 14760/20000] Loss: 0.0002760 (Best: 0.0002419 @iter14752) ([91m↑5.62%[0m) [0.11% of initial]
[Iter 14770/20000] Loss: 0.0003300 (Best: 0.0002419 @iter14752) ([91m↑19.59%[0m) [0.13% of initial]
[Iter 14780/20000] Loss: 0.0003506 (Best: 0.0002419 @iter14752) ([91m↑6.23%[0m) [0.14% of initial]
[Iter 14790/20000] Loss: 0.0003382 (Best: 0.0002419 @iter14752) ([92m↓3.54%[0m) [0.13% of initial]
Iter:14799, L1 loss=0.0004117, Total loss=0.0003554, Time:53
[Iter 14800/20000] Loss: 0.0003166 (Best: 0.0002419 @iter14752) ([92m↓6.38%[0m) [0.13% of initial]
[Iter 14810/20000] Loss: 0.0002947 (Best: 0.0002419 @iter14752) ([92m↓6.94%[0m) [0.12% of initial]
[Iter 14820/20000] Loss: 0.0002860 (Best: 0.0002419 @iter14752) ([92m↓2.96%[0m) [0.11% of initial]
[Iter 14830/20000] Loss: 0.0002825 (Best: 0.0002419 @iter14752) ([92m↓1.20%[0m) [0.11% of initial]
[Iter 14840/20000] Loss: 0.0002923 (Best: 0.0002419 @iter14752) ([91m↑3.45%[0m) [0.12% of initial]
[Iter 14850/20000] Loss: 0.0002985 (Best: 0.0002419 @iter14752) ([91m↑2.13%[0m) [0.12% of initial]
[Iter 14860/20000] Loss: 0.0003015 (Best: 0.0002419 @iter14752) ([91m↑1.01%[0m) [0.12% of initial]
[Iter 14870/20000] Loss: 0.0002987 (Best: 0.0002419 @iter14752) ([92m↓0.92%[0m) [0.12% of initial]
[Iter 14880/20000] Loss: 0.0002908 (Best: 0.0002419 @iter14752) ([92m↓2.67%[0m) [0.12% of initial]
[Iter 14890/20000] Loss: 0.0002781 (Best: 0.0002419 @iter14752) ([92m↓4.35%[0m) [0.11% of initial]
Iter:14899, L1 loss=0.0003029, Total loss=0.0002729, Time:51
[Iter 14900/20000] Loss: 0.0002713 (Best: 0.0002419 @iter14752) ([92m↓2.45%[0m) [0.11% of initial]
[Iter 14910/20000] Loss: 0.0002699 (Best: 0.0002419 @iter14752) ([92m↓0.50%[0m) [0.11% of initial]
[Iter 14920/20000] Loss: 0.0002740 (Best: 0.0002419 @iter14752) ([91m↑1.49%[0m) [0.11% of initial]
[Iter 14930/20000] Loss: 0.0002713 (Best: 0.0002419 @iter14752) ([92m↓0.98%[0m) [0.11% of initial]
[Iter 14940/20000] Loss: 0.0002586 (Best: 0.0002419 @iter14752) ([92m↓4.67%[0m) [0.10% of initial]
[Iter 14950/20000] Loss: 0.0002761 (Best: 0.0002419 @iter14752) ([91m↑6.77%[0m) [0.11% of initial]
[Iter 14960/20000] Loss: 0.0002597 (Best: 0.0002419 @iter14752) ([92m↓5.95%[0m) [0.10% of initial]
[Iter 14970/20000] Loss: 0.0003102 (Best: 0.0002419 @iter14752) ([91m↑19.45%[0m) [0.12% of initial]
[Iter 14980/20000] Loss: 0.0002870 (Best: 0.0002419 @iter14752) ([92m↓7.47%[0m) [0.11% of initial]
[Iter 14990/20000] Loss: 0.0003420 (Best: 0.0002419 @iter14752) ([91m↑19.16%[0m) [0.14% of initial]
Iter:14999, L1 loss=0.0004155, Total loss=0.0003421, Time:50
[Iter 15000/20000] Loss: 0.0004061 (Best: 0.0002419 @iter14752) ([91m↑18.73%[0m) [0.16% of initial]
Pruning 5 points (0.0%) from gaussian0 at iteration 15000
Pruning 4 points (0.0%) from gaussian1 at iteration 15000
[Iter 15010/20000] Loss: 0.0004689 (Best: 0.0002419 @iter14752) ([91m↑15.47%[0m) [0.19% of initial]
[Iter 15020/20000] Loss: 0.0003985 (Best: 0.0002419 @iter14752) ([92m↓15.01%[0m) [0.16% of initial]
[Iter 15030/20000] Loss: 0.0003182 (Best: 0.0002419 @iter14752) ([92m↓20.16%[0m) [0.13% of initial]
[Iter 15040/20000] Loss: 0.0002886 (Best: 0.0002419 @iter14752) ([92m↓9.29%[0m) [0.11% of initial]
[Iter 15050/20000] Loss: 0.0002699 (Best: 0.0002419 @iter14752) ([92m↓6.49%[0m) [0.11% of initial]
[Iter 15060/20000] Loss: 0.0002908 (Best: 0.0002419 @iter14752) ([91m↑7.74%[0m) [0.12% of initial]
[Iter 15070/20000] Loss: 0.0002740 (Best: 0.0002419 @iter14752) ([92m↓5.76%[0m) [0.11% of initial]
[Iter 15080/20000] Loss: 0.0002679 (Best: 0.0002419 @iter14752) ([92m↓2.24%[0m) [0.11% of initial]
[Iter 15090/20000] Loss: 0.0002582 (Best: 0.0002419 @iter14752) ([92m↓3.62%[0m) [0.10% of initial]
Iter:15099, L1 loss=0.0002794, Total loss=0.0002425, Time:50
[Iter 15100/20000] Loss: 0.0002441 (Best: 0.0002382 @iter15092) ([92m↓5.46%[0m) [0.10% of initial]
[Iter 15110/20000] Loss: 0.0002668 (Best: 0.0002331 @iter15101) ([91m↑9.28%[0m) [0.11% of initial]
[Iter 15120/20000] Loss: 0.0002816 (Best: 0.0002331 @iter15101) ([91m↑5.58%[0m) [0.11% of initial]
[Iter 15130/20000] Loss: 0.0002914 (Best: 0.0002331 @iter15101) ([91m↑3.48%[0m) [0.12% of initial]
[Iter 15140/20000] Loss: 0.0002596 (Best: 0.0002331 @iter15101) ([92m↓10.93%[0m) [0.10% of initial]
[Iter 15150/20000] Loss: 0.0003014 (Best: 0.0002331 @iter15101) ([91m↑16.12%[0m) [0.12% of initial]
[Iter 15160/20000] Loss: 0.0002660 (Best: 0.0002331 @iter15101) ([92m↓11.76%[0m) [0.11% of initial]
[Iter 15170/20000] Loss: 0.0002660 (Best: 0.0002331 @iter15101) ([91m↑0.00%[0m) [0.11% of initial]
[Iter 15180/20000] Loss: 0.0002789 (Best: 0.0002331 @iter15101) ([91m↑4.87%[0m) [0.11% of initial]
[Iter 15190/20000] Loss: 0.0002751 (Best: 0.0002331 @iter15101) ([92m↓1.39%[0m) [0.11% of initial]
Iter:15199, L1 loss=0.00031, Total loss=0.0002789, Time:47
[Iter 15200/20000] Loss: 0.0002787 (Best: 0.0002331 @iter15101) ([91m↑1.32%[0m) [0.11% of initial]
[Iter 15210/20000] Loss: 0.0003440 (Best: 0.0002331 @iter15101) ([91m↑23.42%[0m) [0.14% of initial]
[Iter 15220/20000] Loss: 0.0003409 (Best: 0.0002331 @iter15101) ([92m↓0.88%[0m) [0.14% of initial]
[Iter 15230/20000] Loss: 0.0002950 (Best: 0.0002331 @iter15101) ([92m↓13.48%[0m) [0.12% of initial]
[Iter 15240/20000] Loss: 0.0002738 (Best: 0.0002331 @iter15101) ([92m↓7.17%[0m) [0.11% of initial]
[Iter 15250/20000] Loss: 0.0002551 (Best: 0.0002331 @iter15101) ([92m↓6.83%[0m) [0.10% of initial]
[Iter 15260/20000] Loss: 0.0002723 (Best: 0.0002331 @iter15101) ([91m↑6.75%[0m) [0.11% of initial]
[Iter 15270/20000] Loss: 0.0002712 (Best: 0.0002331 @iter15101) ([92m↓0.41%[0m) [0.11% of initial]
[Iter 15280/20000] Loss: 0.0002692 (Best: 0.0002331 @iter15101) ([92m↓0.74%[0m) [0.11% of initial]
[Iter 15290/20000] Loss: 0.0002732 (Best: 0.0002331 @iter15101) ([91m↑1.49%[0m) [0.11% of initial]
Iter:15299, L1 loss=0.0003542, Total loss=0.0002882, Time:48
[Iter 15300/20000] Loss: 0.0002876 (Best: 0.0002331 @iter15101) ([91m↑5.25%[0m) [0.11% of initial]
[Iter 15310/20000] Loss: 0.0002932 (Best: 0.0002331 @iter15101) ([91m↑1.97%[0m) [0.12% of initial]
[Iter 15320/20000] Loss: 0.0002862 (Best: 0.0002331 @iter15101) ([92m↓2.40%[0m) [0.11% of initial]
[Iter 15330/20000] Loss: 0.0002798 (Best: 0.0002331 @iter15101) ([92m↓2.22%[0m) [0.11% of initial]
[Iter 15340/20000] Loss: 0.0002699 (Best: 0.0002331 @iter15101) ([92m↓3.56%[0m) [0.11% of initial]
[Iter 15350/20000] Loss: 0.0002769 (Best: 0.0002331 @iter15101) ([91m↑2.59%[0m) [0.11% of initial]
[Iter 15360/20000] Loss: 0.0002894 (Best: 0.0002331 @iter15101) ([91m↑4.54%[0m) [0.11% of initial]
[Iter 15370/20000] Loss: 0.0002859 (Best: 0.0002331 @iter15101) ([92m↓1.23%[0m) [0.11% of initial]
[Iter 15380/20000] Loss: 0.0002786 (Best: 0.0002331 @iter15101) ([92m↓2.53%[0m) [0.11% of initial]
[Iter 15390/20000] Loss: 0.0002739 (Best: 0.0002331 @iter15101) ([92m↓1.70%[0m) [0.11% of initial]
Iter:15399, L1 loss=0.0002861, Total loss=0.0002467, Time:51
[Iter 15400/20000] Loss: 0.0002455 (Best: 0.0002331 @iter15101) ([92m↓10.35%[0m) [0.10% of initial]
[Iter 15410/20000] Loss: 0.0002472 (Best: 0.0002292 @iter15409) ([91m↑0.66%[0m) [0.10% of initial]
[Iter 15420/20000] Loss: 0.0002490 (Best: 0.0002292 @iter15409) ([91m↑0.75%[0m) [0.10% of initial]
[Iter 15430/20000] Loss: 0.0002451 (Best: 0.0002292 @iter15409) ([92m↓1.57%[0m) [0.10% of initial]
[Iter 15440/20000] Loss: 0.0002469 (Best: 0.0002292 @iter15409) ([91m↑0.73%[0m) [0.10% of initial]
[Iter 15450/20000] Loss: 0.0002655 (Best: 0.0002292 @iter15409) ([91m↑7.52%[0m) [0.11% of initial]
[Iter 15460/20000] Loss: 0.0002912 (Best: 0.0002292 @iter15409) ([91m↑9.70%[0m) [0.12% of initial]
[Iter 15470/20000] Loss: 0.0003199 (Best: 0.0002292 @iter15409) ([91m↑9.84%[0m) [0.13% of initial]
[Iter 15480/20000] Loss: 0.0003053 (Best: 0.0002292 @iter15409) ([92m↓4.56%[0m) [0.12% of initial]
[Iter 15490/20000] Loss: 0.0002655 (Best: 0.0002292 @iter15409) ([92m↓13.03%[0m) [0.11% of initial]
Iter:15499, L1 loss=0.0002982, Total loss=0.0002667, Time:54
[Iter 15500/20000] Loss: 0.0002753 (Best: 0.0002292 @iter15409) ([91m↑3.69%[0m) [0.11% of initial]
Pruning 6 points (0.0%) from gaussian0 at iteration 15500
Pruning 7 points (0.0%) from gaussian1 at iteration 15500
[Iter 15510/20000] Loss: 0.0006046 (Best: 0.0002292 @iter15409) ([91m↑119.60%[0m) [0.24% of initial]
[Iter 15520/20000] Loss: 0.0004058 (Best: 0.0002292 @iter15409) ([92m↓32.88%[0m) [0.16% of initial]
[Iter 15530/20000] Loss: 0.0003282 (Best: 0.0002292 @iter15409) ([92m↓19.11%[0m) [0.13% of initial]
[Iter 15540/20000] Loss: 0.0002768 (Best: 0.0002292 @iter15409) ([92m↓15.66%[0m) [0.11% of initial]
[Iter 15550/20000] Loss: 0.0002646 (Best: 0.0002292 @iter15409) ([92m↓4.41%[0m) [0.11% of initial]
[Iter 15560/20000] Loss: 0.0002817 (Best: 0.0002292 @iter15409) ([91m↑6.45%[0m) [0.11% of initial]
[Iter 15570/20000] Loss: 0.0003477 (Best: 0.0002292 @iter15409) ([91m↑23.43%[0m) [0.14% of initial]
[Iter 15580/20000] Loss: 0.0002804 (Best: 0.0002292 @iter15409) ([92m↓19.34%[0m) [0.11% of initial]
[Iter 15590/20000] Loss: 0.0002645 (Best: 0.0002292 @iter15409) ([92m↓5.68%[0m) [0.11% of initial]
Iter:15599, L1 loss=0.00028, Total loss=0.0002575, Time:47
[Iter 15600/20000] Loss: 0.0002588 (Best: 0.0002292 @iter15409) ([92m↓2.16%[0m) [0.10% of initial]
[Iter 15610/20000] Loss: 0.0002685 (Best: 0.0002292 @iter15409) ([91m↑3.74%[0m) [0.11% of initial]
[Iter 15620/20000] Loss: 0.0002456 (Best: 0.0002292 @iter15409) ([92m↓8.53%[0m) [0.10% of initial]
[Iter 15630/20000] Loss: 0.0002714 (Best: 0.0002292 @iter15409) ([91m↑10.50%[0m) [0.11% of initial]
[Iter 15640/20000] Loss: 0.0002555 (Best: 0.0002292 @iter15409) ([92m↓5.87%[0m) [0.10% of initial]
[Iter 15650/20000] Loss: 0.0002888 (Best: 0.0002292 @iter15409) ([91m↑13.04%[0m) [0.11% of initial]
[Iter 15660/20000] Loss: 0.0002630 (Best: 0.0002292 @iter15409) ([92m↓8.94%[0m) [0.10% of initial]
[Iter 15670/20000] Loss: 0.0002708 (Best: 0.0002292 @iter15409) ([91m↑2.99%[0m) [0.11% of initial]
[Iter 15680/20000] Loss: 0.0003601 (Best: 0.0002292 @iter15409) ([91m↑32.98%[0m) [0.14% of initial]
[Iter 15690/20000] Loss: 0.0003207 (Best: 0.0002292 @iter15409) ([92m↓10.96%[0m) [0.13% of initial]
Iter:15699, L1 loss=0.0003277, Total loss=0.0002618, Time:55
[Iter 15700/20000] Loss: 0.0002709 (Best: 0.0002292 @iter15409) ([92m↓15.51%[0m) [0.11% of initial]
[Iter 15710/20000] Loss: 0.0002491 (Best: 0.0002292 @iter15409) ([92m↓8.06%[0m) [0.10% of initial]
[Iter 15720/20000] Loss: 0.0002615 (Best: 0.0002292 @iter15409) ([91m↑4.97%[0m) [0.10% of initial]
[Iter 15730/20000] Loss: 0.0002716 (Best: 0.0002292 @iter15409) ([91m↑3.86%[0m) [0.11% of initial]
[Iter 15740/20000] Loss: 0.0002623 (Best: 0.0002292 @iter15409) ([92m↓3.42%[0m) [0.10% of initial]
[Iter 15750/20000] Loss: 0.0002710 (Best: 0.0002292 @iter15409) ([91m↑3.31%[0m) [0.11% of initial]
[Iter 15760/20000] Loss: 0.0002807 (Best: 0.0002292 @iter15409) ([91m↑3.60%[0m) [0.11% of initial]
[Iter 15770/20000] Loss: 0.0003014 (Best: 0.0002292 @iter15409) ([91m↑7.37%[0m) [0.12% of initial]
[Iter 15780/20000] Loss: 0.0002662 (Best: 0.0002292 @iter15409) ([92m↓11.70%[0m) [0.11% of initial]
[Iter 15790/20000] Loss: 0.0002569 (Best: 0.0002292 @iter15409) ([92m↓3.48%[0m) [0.10% of initial]
Iter:15799, L1 loss=0.0002602, Total loss=0.0002357, Time:53
[Iter 15800/20000] Loss: 0.0002533 (Best: 0.0002292 @iter15409) ([92m↓1.39%[0m) [0.10% of initial]
[Iter 15810/20000] Loss: 0.0002727 (Best: 0.0002292 @iter15409) ([91m↑7.65%[0m) [0.11% of initial]
[Iter 15820/20000] Loss: 0.0002776 (Best: 0.0002292 @iter15409) ([91m↑1.78%[0m) [0.11% of initial]
[Iter 15830/20000] Loss: 0.0002968 (Best: 0.0002292 @iter15409) ([91m↑6.93%[0m) [0.12% of initial]
[Iter 15840/20000] Loss: 0.0002687 (Best: 0.0002292 @iter15409) ([92m↓9.48%[0m) [0.11% of initial]
[Iter 15850/20000] Loss: 0.0002713 (Best: 0.0002292 @iter15409) ([91m↑0.97%[0m) [0.11% of initial]
[Iter 15860/20000] Loss: 0.0002756 (Best: 0.0002292 @iter15409) ([91m↑1.59%[0m) [0.11% of initial]
[Iter 15870/20000] Loss: 0.0002752 (Best: 0.0002292 @iter15409) ([92m↓0.13%[0m) [0.11% of initial]
[Iter 15880/20000] Loss: 0.0002701 (Best: 0.0002292 @iter15409) ([92m↓1.84%[0m) [0.11% of initial]
[Iter 15890/20000] Loss: 0.0002623 (Best: 0.0002292 @iter15409) ([92m↓2.90%[0m) [0.10% of initial]
Iter:15899, L1 loss=0.0002584, Total loss=0.0002328, Time:54
[Iter 15900/20000] Loss: 0.0002421 (Best: 0.0002292 @iter15409) ([92m↓7.70%[0m) [0.10% of initial]
[Iter 15910/20000] Loss: 0.0002363 (Best: 0.0002252 @iter15907) ([92m↓2.40%[0m) [0.09% of initial]
[Iter 15920/20000] Loss: 0.0002421 (Best: 0.0002225 @iter15916) ([91m↑2.44%[0m) [0.10% of initial]
[Iter 15930/20000] Loss: 0.0002370 (Best: 0.0002225 @iter15916) ([92m↓2.08%[0m) [0.09% of initial]
[Iter 15940/20000] Loss: 0.0002288 (Best: 0.0002192 @iter15940) ([92m↓3.46%[0m) [0.09% of initial]
[Iter 15950/20000] Loss: 0.0002765 (Best: 0.0002192 @iter15940) ([91m↑20.84%[0m) [0.11% of initial]
[Iter 15960/20000] Loss: 0.0002513 (Best: 0.0002192 @iter15940) ([92m↓9.11%[0m) [0.10% of initial]
[Iter 15970/20000] Loss: 0.0002393 (Best: 0.0002192 @iter15940) ([92m↓4.80%[0m) [0.10% of initial]
[Iter 15980/20000] Loss: 0.0002352 (Best: 0.0002192 @iter15940) ([92m↓1.72%[0m) [0.09% of initial]
[Iter 15990/20000] Loss: 0.0002584 (Best: 0.0002192 @iter15940) ([91m↑9.88%[0m) [0.10% of initial]
Iter:15999, L1 loss=0.000295, Total loss=0.0002746, Time:47
[Iter 16000/20000] Loss: 0.0002768 (Best: 0.0002192 @iter15940) ([91m↑7.11%[0m) [0.11% of initial]
Pruning 3 points (0.0%) from gaussian0 at iteration 16000
Pruning 1 points (0.0%) from gaussian1 at iteration 16000
[Iter 16010/20000] Loss: 0.0136174 (Best: 0.0002192 @iter15940) ([91m↑4820.10%[0m) [5.41% of initial]
[Iter 16020/20000] Loss: 0.0055585 (Best: 0.0002192 @iter15940) ([92m↓59.18%[0m) [2.21% of initial]
[Iter 16030/20000] Loss: 0.0026645 (Best: 0.0002192 @iter15940) ([92m↓52.06%[0m) [1.06% of initial]
[Iter 16040/20000] Loss: 0.0017834 (Best: 0.0002192 @iter15940) ([92m↓33.07%[0m) [0.71% of initial]
[Iter 16050/20000] Loss: 0.0011882 (Best: 0.0002192 @iter15940) ([92m↓33.37%[0m) [0.47% of initial]
[Iter 16060/20000] Loss: 0.0009063 (Best: 0.0002192 @iter15940) ([92m↓23.73%[0m) [0.36% of initial]
[Iter 16070/20000] Loss: 0.0007262 (Best: 0.0002192 @iter15940) ([92m↓19.87%[0m) [0.29% of initial]
[Iter 16080/20000] Loss: 0.0006201 (Best: 0.0002192 @iter15940) ([92m↓14.61%[0m) [0.25% of initial]
[Iter 16090/20000] Loss: 0.0005379 (Best: 0.0002192 @iter15940) ([92m↓13.26%[0m) [0.21% of initial]
Iter:16099, L1 loss=0.0004823, Total loss=0.0004697, Time:53
[Iter 16100/20000] Loss: 0.0004761 (Best: 0.0002192 @iter15940) ([92m↓11.47%[0m) [0.19% of initial]
[Iter 16110/20000] Loss: 0.0004471 (Best: 0.0002192 @iter15940) ([92m↓6.09%[0m) [0.18% of initial]
[Iter 16120/20000] Loss: 0.0004154 (Best: 0.0002192 @iter15940) ([92m↓7.11%[0m) [0.17% of initial]
[Iter 16130/20000] Loss: 0.0003953 (Best: 0.0002192 @iter15940) ([92m↓4.83%[0m) [0.16% of initial]
[Iter 16140/20000] Loss: 0.0003722 (Best: 0.0002192 @iter15940) ([92m↓5.85%[0m) [0.15% of initial]
[Iter 16150/20000] Loss: 0.0003563 (Best: 0.0002192 @iter15940) ([92m↓4.26%[0m) [0.14% of initial]
[Iter 16160/20000] Loss: 0.0003496 (Best: 0.0002192 @iter15940) ([92m↓1.88%[0m) [0.14% of initial]
[Iter 16170/20000] Loss: 0.0003521 (Best: 0.0002192 @iter15940) ([91m↑0.70%[0m) [0.14% of initial]
[Iter 16180/20000] Loss: 0.0003432 (Best: 0.0002192 @iter15940) ([92m↓2.52%[0m) [0.14% of initial]
[Iter 16190/20000] Loss: 0.0003365 (Best: 0.0002192 @iter15940) ([92m↓1.96%[0m) [0.13% of initial]
Iter:16199, L1 loss=0.0003719, Total loss=0.0003302, Time:53
[Iter 16200/20000] Loss: 0.0003372 (Best: 0.0002192 @iter15940) ([91m↑0.21%[0m) [0.13% of initial]
[Iter 16210/20000] Loss: 0.0003449 (Best: 0.0002192 @iter15940) ([91m↑2.30%[0m) [0.14% of initial]
[Iter 16220/20000] Loss: 0.0003341 (Best: 0.0002192 @iter15940) ([92m↓3.16%[0m) [0.13% of initial]
[Iter 16230/20000] Loss: 0.0003326 (Best: 0.0002192 @iter15940) ([92m↓0.44%[0m) [0.13% of initial]
[Iter 16240/20000] Loss: 0.0003663 (Best: 0.0002192 @iter15940) ([91m↑10.12%[0m) [0.15% of initial]
[Iter 16250/20000] Loss: 0.0003305 (Best: 0.0002192 @iter15940) ([92m↓9.76%[0m) [0.13% of initial]
[Iter 16260/20000] Loss: 0.0003121 (Best: 0.0002192 @iter15940) ([92m↓5.58%[0m) [0.12% of initial]
[Iter 16270/20000] Loss: 0.0002997 (Best: 0.0002192 @iter15940) ([92m↓3.96%[0m) [0.12% of initial]
[Iter 16280/20000] Loss: 0.0002945 (Best: 0.0002192 @iter15940) ([92m↓1.74%[0m) [0.12% of initial]
[Iter 16290/20000] Loss: 0.0003073 (Best: 0.0002192 @iter15940) ([91m↑4.34%[0m) [0.12% of initial]
Iter:16299, L1 loss=0.0003762, Total loss=0.0003362, Time:50
[Iter 16300/20000] Loss: 0.0003004 (Best: 0.0002192 @iter15940) ([92m↓2.24%[0m) [0.12% of initial]
[Iter 16310/20000] Loss: 0.0003317 (Best: 0.0002192 @iter15940) ([91m↑10.41%[0m) [0.13% of initial]
[Iter 16320/20000] Loss: 0.0003267 (Best: 0.0002192 @iter15940) ([92m↓1.49%[0m) [0.13% of initial]
[Iter 16330/20000] Loss: 0.0003223 (Best: 0.0002192 @iter15940) ([92m↓1.34%[0m) [0.13% of initial]
[Iter 16340/20000] Loss: 0.0003040 (Best: 0.0002192 @iter15940) ([92m↓5.70%[0m) [0.12% of initial]
[Iter 16350/20000] Loss: 0.0003148 (Best: 0.0002192 @iter15940) ([91m↑3.56%[0m) [0.13% of initial]
[Iter 16360/20000] Loss: 0.0003141 (Best: 0.0002192 @iter15940) ([92m↓0.21%[0m) [0.12% of initial]
[Iter 16370/20000] Loss: 0.0003417 (Best: 0.0002192 @iter15940) ([91m↑8.76%[0m) [0.14% of initial]
[Iter 16380/20000] Loss: 0.0003393 (Best: 0.0002192 @iter15940) ([92m↓0.70%[0m) [0.13% of initial]
[Iter 16390/20000] Loss: 0.0003420 (Best: 0.0002192 @iter15940) ([91m↑0.80%[0m) [0.14% of initial]
Iter:16399, L1 loss=0.0003218, Total loss=0.0002904, Time:53
[Iter 16400/20000] Loss: 0.0003144 (Best: 0.0002192 @iter15940) ([92m↓8.07%[0m) [0.12% of initial]
[Iter 16410/20000] Loss: 0.0003152 (Best: 0.0002192 @iter15940) ([91m↑0.25%[0m) [0.13% of initial]
[Iter 16420/20000] Loss: 0.0002954 (Best: 0.0002192 @iter15940) ([92m↓6.28%[0m) [0.12% of initial]
[Iter 16430/20000] Loss: 0.0002909 (Best: 0.0002192 @iter15940) ([92m↓1.52%[0m) [0.12% of initial]
[Iter 16440/20000] Loss: 0.0002974 (Best: 0.0002192 @iter15940) ([91m↑2.24%[0m) [0.12% of initial]
[Iter 16450/20000] Loss: 0.0002936 (Best: 0.0002192 @iter15940) ([92m↓1.28%[0m) [0.12% of initial]
[Iter 16460/20000] Loss: 0.0002871 (Best: 0.0002192 @iter15940) ([92m↓2.20%[0m) [0.11% of initial]
[Iter 16470/20000] Loss: 0.0003059 (Best: 0.0002192 @iter15940) ([91m↑6.53%[0m) [0.12% of initial]
[Iter 16480/20000] Loss: 0.0002946 (Best: 0.0002192 @iter15940) ([92m↓3.70%[0m) [0.12% of initial]
[Iter 16490/20000] Loss: 0.0002934 (Best: 0.0002192 @iter15940) ([92m↓0.40%[0m) [0.12% of initial]
Iter:16499, L1 loss=0.0003443, Total loss=0.0003288, Time:52
[Iter 16500/20000] Loss: 0.0003389 (Best: 0.0002192 @iter15940) ([91m↑15.50%[0m) [0.13% of initial]
Pruning 9 points (0.0%) from gaussian0 at iteration 16500
Pruning 15 points (0.0%) from gaussian1 at iteration 16500
[Iter 16510/20000] Loss: 0.0005189 (Best: 0.0002192 @iter15940) ([91m↑53.13%[0m) [0.21% of initial]
[Iter 16520/20000] Loss: 0.0003996 (Best: 0.0002192 @iter15940) ([92m↓22.98%[0m) [0.16% of initial]
[Iter 16530/20000] Loss: 0.0003426 (Best: 0.0002192 @iter15940) ([92m↓14.28%[0m) [0.14% of initial]
[Iter 16540/20000] Loss: 0.0003296 (Best: 0.0002192 @iter15940) ([92m↓3.81%[0m) [0.13% of initial]
[Iter 16550/20000] Loss: 0.0003070 (Best: 0.0002192 @iter15940) ([92m↓6.84%[0m) [0.12% of initial]
[Iter 16560/20000] Loss: 0.0003061 (Best: 0.0002192 @iter15940) ([92m↓0.30%[0m) [0.12% of initial]
[Iter 16570/20000] Loss: 0.0002938 (Best: 0.0002192 @iter15940) ([92m↓4.01%[0m) [0.12% of initial]
[Iter 16580/20000] Loss: 0.0002966 (Best: 0.0002192 @iter15940) ([91m↑0.95%[0m) [0.12% of initial]
[Iter 16590/20000] Loss: 0.0003018 (Best: 0.0002192 @iter15940) ([91m↑1.76%[0m) [0.12% of initial]
Iter:16599, L1 loss=0.0003385, Total loss=0.0003006, Time:51
[Iter 16600/20000] Loss: 0.0002901 (Best: 0.0002192 @iter15940) ([92m↓3.89%[0m) [0.12% of initial]
[Iter 16610/20000] Loss: 0.0002883 (Best: 0.0002192 @iter15940) ([92m↓0.60%[0m) [0.11% of initial]
[Iter 16620/20000] Loss: 0.0002939 (Best: 0.0002192 @iter15940) ([91m↑1.94%[0m) [0.12% of initial]
[Iter 16630/20000] Loss: 0.0003080 (Best: 0.0002192 @iter15940) ([91m↑4.77%[0m) [0.12% of initial]
[Iter 16640/20000] Loss: 0.0003277 (Best: 0.0002192 @iter15940) ([91m↑6.43%[0m) [0.13% of initial]
[Iter 16650/20000] Loss: 0.0003709 (Best: 0.0002192 @iter15940) ([91m↑13.16%[0m) [0.15% of initial]
[Iter 16660/20000] Loss: 0.0003854 (Best: 0.0002192 @iter15940) ([91m↑3.91%[0m) [0.15% of initial]
[Iter 16670/20000] Loss: 0.0003529 (Best: 0.0002192 @iter15940) ([92m↓8.43%[0m) [0.14% of initial]
[Iter 16680/20000] Loss: 0.0003146 (Best: 0.0002192 @iter15940) ([92m↓10.84%[0m) [0.13% of initial]
[Iter 16690/20000] Loss: 0.0002947 (Best: 0.0002192 @iter15940) ([92m↓6.34%[0m) [0.12% of initial]
Iter:16699, L1 loss=0.0003531, Total loss=0.0003162, Time:56
[Iter 16700/20000] Loss: 0.0002979 (Best: 0.0002192 @iter15940) ([91m↑1.10%[0m) [0.12% of initial]
[Iter 16710/20000] Loss: 0.0002977 (Best: 0.0002192 @iter15940) ([92m↓0.08%[0m) [0.12% of initial]
[Iter 16720/20000] Loss: 0.0003107 (Best: 0.0002192 @iter15940) ([91m↑4.36%[0m) [0.12% of initial]
[Iter 16730/20000] Loss: 0.0002906 (Best: 0.0002192 @iter15940) ([92m↓6.45%[0m) [0.12% of initial]
[Iter 16740/20000] Loss: 0.0003009 (Best: 0.0002192 @iter15940) ([91m↑3.53%[0m) [0.12% of initial]
[Iter 16750/20000] Loss: 0.0002908 (Best: 0.0002192 @iter15940) ([92m↓3.37%[0m) [0.12% of initial]
[Iter 16760/20000] Loss: 0.0002855 (Best: 0.0002192 @iter15940) ([92m↓1.81%[0m) [0.11% of initial]
[Iter 16770/20000] Loss: 0.0002891 (Best: 0.0002192 @iter15940) ([91m↑1.27%[0m) [0.11% of initial]
[Iter 16780/20000] Loss: 0.0002899 (Best: 0.0002192 @iter15940) ([91m↑0.27%[0m) [0.12% of initial]
[Iter 16790/20000] Loss: 0.0003047 (Best: 0.0002192 @iter15940) ([91m↑5.10%[0m) [0.12% of initial]
Iter:16799, L1 loss=0.00036, Total loss=0.000319, Time:51
[Iter 16800/20000] Loss: 0.0003080 (Best: 0.0002192 @iter15940) ([91m↑1.08%[0m) [0.12% of initial]
[Iter 16810/20000] Loss: 0.0003548 (Best: 0.0002192 @iter15940) ([91m↑15.20%[0m) [0.14% of initial]
[Iter 16820/20000] Loss: 0.0003449 (Best: 0.0002192 @iter15940) ([92m↓2.77%[0m) [0.14% of initial]
[Iter 16830/20000] Loss: 0.0003311 (Best: 0.0002192 @iter15940) ([92m↓4.00%[0m) [0.13% of initial]
[Iter 16840/20000] Loss: 0.0004153 (Best: 0.0002192 @iter15940) ([91m↑25.41%[0m) [0.16% of initial]
[Iter 16850/20000] Loss: 0.0003446 (Best: 0.0002192 @iter15940) ([92m↓17.01%[0m) [0.14% of initial]
[Iter 16860/20000] Loss: 0.0003141 (Best: 0.0002192 @iter15940) ([92m↓8.86%[0m) [0.12% of initial]
[Iter 16870/20000] Loss: 0.0002909 (Best: 0.0002192 @iter15940) ([92m↓7.37%[0m) [0.12% of initial]
[Iter 16880/20000] Loss: 0.0002872 (Best: 0.0002192 @iter15940) ([92m↓1.29%[0m) [0.11% of initial]
[Iter 16890/20000] Loss: 0.0003117 (Best: 0.0002192 @iter15940) ([91m↑8.55%[0m) [0.12% of initial]
Iter:16899, L1 loss=0.0003761, Total loss=0.0003544, Time:51
[Iter 16900/20000] Loss: 0.0003082 (Best: 0.0002192 @iter15940) ([92m↓1.15%[0m) [0.12% of initial]
[Iter 16910/20000] Loss: 0.0002988 (Best: 0.0002192 @iter15940) ([92m↓3.03%[0m) [0.12% of initial]
[Iter 16920/20000] Loss: 0.0003534 (Best: 0.0002192 @iter15940) ([91m↑18.26%[0m) [0.14% of initial]
[Iter 16930/20000] Loss: 0.0003257 (Best: 0.0002192 @iter15940) ([92m↓7.86%[0m) [0.13% of initial]
[Iter 16940/20000] Loss: 0.0003390 (Best: 0.0002192 @iter15940) ([91m↑4.09%[0m) [0.13% of initial]
[Iter 16950/20000] Loss: 0.0003340 (Best: 0.0002192 @iter15940) ([92m↓1.46%[0m) [0.13% of initial]
[Iter 16960/20000] Loss: 0.0003310 (Best: 0.0002192 @iter15940) ([92m↓0.92%[0m) [0.13% of initial]
[Iter 16970/20000] Loss: 0.0003187 (Best: 0.0002192 @iter15940) ([92m↓3.70%[0m) [0.13% of initial]
[Iter 16980/20000] Loss: 0.0003009 (Best: 0.0002192 @iter15940) ([92m↓5.58%[0m) [0.12% of initial]
[Iter 16990/20000] Loss: 0.0003150 (Best: 0.0002192 @iter15940) ([91m↑4.69%[0m) [0.13% of initial]
Iter:16999, L1 loss=0.0003372, Total loss=0.0003015, Time:53
[Iter 17000/20000] Loss: 0.0002999 (Best: 0.0002192 @iter15940) ([92m↓4.82%[0m) [0.12% of initial]
Pruning 3 points (0.0%) from gaussian0 at iteration 17000
Pruning 8 points (0.0%) from gaussian1 at iteration 17000
[Iter 17010/20000] Loss: 0.0005706 (Best: 0.0002192 @iter15940) ([91m↑90.27%[0m) [0.23% of initial]
[Iter 17020/20000] Loss: 0.0003960 (Best: 0.0002192 @iter15940) ([92m↓30.59%[0m) [0.16% of initial]
[Iter 17030/20000] Loss: 0.0003288 (Best: 0.0002192 @iter15940) ([92m↓16.99%[0m) [0.13% of initial]
[Iter 17040/20000] Loss: 0.0002976 (Best: 0.0002192 @iter15940) ([92m↓9.46%[0m) [0.12% of initial]
[Iter 17050/20000] Loss: 0.0003022 (Best: 0.0002192 @iter15940) ([91m↑1.53%[0m) [0.12% of initial]
[Iter 17060/20000] Loss: 0.0003350 (Best: 0.0002192 @iter15940) ([91m↑10.85%[0m) [0.13% of initial]
[Iter 17070/20000] Loss: 0.0003436 (Best: 0.0002192 @iter15940) ([91m↑2.57%[0m) [0.14% of initial]
[Iter 17080/20000] Loss: 0.0003037 (Best: 0.0002192 @iter15940) ([92m↓11.62%[0m) [0.12% of initial]
[Iter 17090/20000] Loss: 0.0002789 (Best: 0.0002192 @iter15940) ([92m↓8.16%[0m) [0.11% of initial]
Iter:17099, L1 loss=0.0002922, Total loss=0.0002502, Time:52
[Iter 17100/20000] Loss: 0.0002648 (Best: 0.0002192 @iter15940) ([92m↓5.05%[0m) [0.11% of initial]
[Iter 17110/20000] Loss: 0.0002722 (Best: 0.0002192 @iter15940) ([91m↑2.80%[0m) [0.11% of initial]
[Iter 17120/20000] Loss: 0.0002649 (Best: 0.0002192 @iter15940) ([92m↓2.70%[0m) [0.11% of initial]
[Iter 17130/20000] Loss: 0.0002710 (Best: 0.0002192 @iter15940) ([91m↑2.32%[0m) [0.11% of initial]
[Iter 17140/20000] Loss: 0.0002909 (Best: 0.0002192 @iter15940) ([91m↑7.32%[0m) [0.12% of initial]
[Iter 17150/20000] Loss: 0.0002807 (Best: 0.0002192 @iter15940) ([92m↓3.52%[0m) [0.11% of initial]
[Iter 17160/20000] Loss: 0.0002938 (Best: 0.0002192 @iter15940) ([91m↑4.67%[0m) [0.12% of initial]
[Iter 17170/20000] Loss: 0.0003054 (Best: 0.0002192 @iter15940) ([91m↑3.96%[0m) [0.12% of initial]
[Iter 17180/20000] Loss: 0.0002814 (Best: 0.0002192 @iter15940) ([92m↓7.86%[0m) [0.11% of initial]
[Iter 17190/20000] Loss: 0.0002872 (Best: 0.0002192 @iter15940) ([91m↑2.07%[0m) [0.11% of initial]
Iter:17199, L1 loss=0.0003165, Total loss=0.0002691, Time:53
[Iter 17200/20000] Loss: 0.0002750 (Best: 0.0002192 @iter15940) ([92m↓4.26%[0m) [0.11% of initial]
[Iter 17210/20000] Loss: 0.0002757 (Best: 0.0002192 @iter15940) ([91m↑0.28%[0m) [0.11% of initial]
[Iter 17220/20000] Loss: 0.0002695 (Best: 0.0002192 @iter15940) ([92m↓2.27%[0m) [0.11% of initial]
[Iter 17230/20000] Loss: 0.0002687 (Best: 0.0002192 @iter15940) ([92m↓0.28%[0m) [0.11% of initial]
[Iter 17240/20000] Loss: 0.0003019 (Best: 0.0002192 @iter15940) ([91m↑12.34%[0m) [0.12% of initial]
[Iter 17250/20000] Loss: 0.0003295 (Best: 0.0002192 @iter15940) ([91m↑9.16%[0m) [0.13% of initial]
[Iter 17260/20000] Loss: 0.0002950 (Best: 0.0002192 @iter15940) ([92m↓10.49%[0m) [0.12% of initial]
[Iter 17270/20000] Loss: 0.0003095 (Best: 0.0002192 @iter15940) ([91m↑4.91%[0m) [0.12% of initial]
[Iter 17280/20000] Loss: 0.0003086 (Best: 0.0002192 @iter15940) ([92m↓0.27%[0m) [0.12% of initial]
[Iter 17290/20000] Loss: 0.0004109 (Best: 0.0002192 @iter15940) ([91m↑33.14%[0m) [0.16% of initial]
Iter:17299, L1 loss=0.0003396, Total loss=0.00031, Time:50
[Iter 17300/20000] Loss: 0.0003195 (Best: 0.0002192 @iter15940) ([92m↓22.24%[0m) [0.13% of initial]
[Iter 17310/20000] Loss: 0.0003030 (Best: 0.0002192 @iter15940) ([92m↓5.16%[0m) [0.12% of initial]
[Iter 17320/20000] Loss: 0.0003200 (Best: 0.0002192 @iter15940) ([91m↑5.59%[0m) [0.13% of initial]
[Iter 17330/20000] Loss: 0.0002831 (Best: 0.0002192 @iter15940) ([92m↓11.53%[0m) [0.11% of initial]
[Iter 17340/20000] Loss: 0.0002752 (Best: 0.0002192 @iter15940) ([92m↓2.78%[0m) [0.11% of initial]
[Iter 17350/20000] Loss: 0.0003017 (Best: 0.0002192 @iter15940) ([91m↑9.61%[0m) [0.12% of initial]
[Iter 17360/20000] Loss: 0.0003284 (Best: 0.0002192 @iter15940) ([91m↑8.86%[0m) [0.13% of initial]
[Iter 17370/20000] Loss: 0.0003022 (Best: 0.0002192 @iter15940) ([92m↓7.97%[0m) [0.12% of initial]
[Iter 17380/20000] Loss: 0.0003505 (Best: 0.0002192 @iter15940) ([91m↑16.00%[0m) [0.14% of initial]
[Iter 17390/20000] Loss: 0.0003289 (Best: 0.0002192 @iter15940) ([92m↓6.16%[0m) [0.13% of initial]
Iter:17399, L1 loss=0.0003124, Total loss=0.0002783, Time:53
[Iter 17400/20000] Loss: 0.0002976 (Best: 0.0002192 @iter15940) ([92m↓9.52%[0m) [0.12% of initial]
[Iter 17410/20000] Loss: 0.0002938 (Best: 0.0002192 @iter15940) ([92m↓1.28%[0m) [0.12% of initial]
[Iter 17420/20000] Loss: 0.0003031 (Best: 0.0002192 @iter15940) ([91m↑3.16%[0m) [0.12% of initial]
[Iter 17430/20000] Loss: 0.0002995 (Best: 0.0002192 @iter15940) ([92m↓1.19%[0m) [0.12% of initial]
[Iter 17440/20000] Loss: 0.0002806 (Best: 0.0002192 @iter15940) ([92m↓6.30%[0m) [0.11% of initial]
[Iter 17450/20000] Loss: 0.0002936 (Best: 0.0002192 @iter15940) ([91m↑4.61%[0m) [0.12% of initial]
[Iter 17460/20000] Loss: 0.0003076 (Best: 0.0002192 @iter15940) ([91m↑4.80%[0m) [0.12% of initial]
[Iter 17470/20000] Loss: 0.0002839 (Best: 0.0002192 @iter15940) ([92m↓7.70%[0m) [0.11% of initial]
[Iter 17480/20000] Loss: 0.0002809 (Best: 0.0002192 @iter15940) ([92m↓1.06%[0m) [0.11% of initial]
[Iter 17490/20000] Loss: 0.0002891 (Best: 0.0002192 @iter15940) ([91m↑2.91%[0m) [0.11% of initial]
Iter:17499, L1 loss=0.0003371, Total loss=0.000278, Time:41
[Iter 17500/20000] Loss: 0.0002688 (Best: 0.0002192 @iter15940) ([92m↓7.01%[0m) [0.11% of initial]
Pruning 3 points (0.0%) from gaussian0 at iteration 17500
Pruning 5 points (0.0%) from gaussian1 at iteration 17500
[Iter 17510/20000] Loss: 0.0005654 (Best: 0.0002192 @iter15940) ([91m↑110.31%[0m) [0.22% of initial]
[Iter 17520/20000] Loss: 0.0004005 (Best: 0.0002192 @iter15940) ([92m↓29.16%[0m) [0.16% of initial]
[Iter 17530/20000] Loss: 0.0003137 (Best: 0.0002192 @iter15940) ([92m↓21.68%[0m) [0.12% of initial]
[Iter 17540/20000] Loss: 0.0002897 (Best: 0.0002192 @iter15940) ([92m↓7.65%[0m) [0.12% of initial]
[Iter 17550/20000] Loss: 0.0002684 (Best: 0.0002192 @iter15940) ([92m↓7.34%[0m) [0.11% of initial]
[Iter 17560/20000] Loss: 0.0002609 (Best: 0.0002192 @iter15940) ([92m↓2.82%[0m) [0.10% of initial]
[Iter 17570/20000] Loss: 0.0002791 (Best: 0.0002192 @iter15940) ([91m↑7.00%[0m) [0.11% of initial]
[Iter 17580/20000] Loss: 0.0002740 (Best: 0.0002192 @iter15940) ([92m↓1.84%[0m) [0.11% of initial]
[Iter 17590/20000] Loss: 0.0002702 (Best: 0.0002192 @iter15940) ([92m↓1.37%[0m) [0.11% of initial]
Iter:17599, L1 loss=0.0002931, Total loss=0.0002638, Time:50
[Iter 17600/20000] Loss: 0.0002924 (Best: 0.0002192 @iter15940) ([91m↑8.20%[0m) [0.12% of initial]
[Iter 17610/20000] Loss: 0.0002929 (Best: 0.0002192 @iter15940) ([91m↑0.17%[0m) [0.12% of initial]
[Iter 17620/20000] Loss: 0.0002901 (Best: 0.0002192 @iter15940) ([92m↓0.93%[0m) [0.12% of initial]
[Iter 17630/20000] Loss: 0.0002755 (Best: 0.0002192 @iter15940) ([92m↓5.04%[0m) [0.11% of initial]
[Iter 17640/20000] Loss: 0.0002767 (Best: 0.0002192 @iter15940) ([91m↑0.44%[0m) [0.11% of initial]
[Iter 17650/20000] Loss: 0.0002916 (Best: 0.0002192 @iter15940) ([91m↑5.38%[0m) [0.12% of initial]
[Iter 17660/20000] Loss: 0.0002799 (Best: 0.0002192 @iter15940) ([92m↓4.01%[0m) [0.11% of initial]
[Iter 17670/20000] Loss: 0.0002873 (Best: 0.0002192 @iter15940) ([91m↑2.64%[0m) [0.11% of initial]
[Iter 17680/20000] Loss: 0.0002904 (Best: 0.0002192 @iter15940) ([91m↑1.10%[0m) [0.12% of initial]
[Iter 17690/20000] Loss: 0.0002874 (Best: 0.0002192 @iter15940) ([92m↓1.04%[0m) [0.11% of initial]
Iter:17699, L1 loss=0.0003041, Total loss=0.0002589, Time:49
[Iter 17700/20000] Loss: 0.0002787 (Best: 0.0002192 @iter15940) ([92m↓3.02%[0m) [0.11% of initial]
[Iter 17710/20000] Loss: 0.0002627 (Best: 0.0002192 @iter15940) ([92m↓5.75%[0m) [0.10% of initial]
[Iter 17720/20000] Loss: 0.0002986 (Best: 0.0002192 @iter15940) ([91m↑13.68%[0m) [0.12% of initial]
[Iter 17730/20000] Loss: 0.0002928 (Best: 0.0002192 @iter15940) ([92m↓1.95%[0m) [0.12% of initial]
[Iter 17740/20000] Loss: 0.0002772 (Best: 0.0002192 @iter15940) ([92m↓5.32%[0m) [0.11% of initial]
[Iter 17750/20000] Loss: 0.0002757 (Best: 0.0002192 @iter15940) ([92m↓0.56%[0m) [0.11% of initial]
[Iter 17760/20000] Loss: 0.0002633 (Best: 0.0002192 @iter15940) ([92m↓4.49%[0m) [0.10% of initial]
[Iter 17770/20000] Loss: 0.0002579 (Best: 0.0002192 @iter15940) ([92m↓2.06%[0m) [0.10% of initial]
[Iter 17780/20000] Loss: 0.0002634 (Best: 0.0002192 @iter15940) ([91m↑2.15%[0m) [0.10% of initial]
[Iter 17790/20000] Loss: 0.0003128 (Best: 0.0002192 @iter15940) ([91m↑18.76%[0m) [0.12% of initial]
Iter:17799, L1 loss=0.0004134, Total loss=0.0003697, Time:56
[Iter 17800/20000] Loss: 0.0003255 (Best: 0.0002192 @iter15940) ([91m↑4.06%[0m) [0.13% of initial]
[Iter 17810/20000] Loss: 0.0003184 (Best: 0.0002192 @iter15940) ([92m↓2.19%[0m) [0.13% of initial]
[Iter 17820/20000] Loss: 0.0003035 (Best: 0.0002192 @iter15940) ([92m↓4.67%[0m) [0.12% of initial]
[Iter 17830/20000] Loss: 0.0002745 (Best: 0.0002192 @iter15940) ([92m↓9.58%[0m) [0.11% of initial]
[Iter 17840/20000] Loss: 0.0002583 (Best: 0.0002192 @iter15940) ([92m↓5.90%[0m) [0.10% of initial]
[Iter 17850/20000] Loss: 0.0002555 (Best: 0.0002192 @iter15940) ([92m↓1.07%[0m) [0.10% of initial]
[Iter 17860/20000] Loss: 0.0002536 (Best: 0.0002192 @iter15940) ([92m↓0.74%[0m) [0.10% of initial]
[Iter 17870/20000] Loss: 0.0002659 (Best: 0.0002192 @iter15940) ([91m↑4.85%[0m) [0.11% of initial]
[Iter 17880/20000] Loss: 0.0002881 (Best: 0.0002192 @iter15940) ([91m↑8.36%[0m) [0.11% of initial]
[Iter 17890/20000] Loss: 0.0002561 (Best: 0.0002192 @iter15940) ([92m↓11.11%[0m) [0.10% of initial]
Iter:17899, L1 loss=0.0003068, Total loss=0.0002462, Time:52
[Iter 17900/20000] Loss: 0.0002586 (Best: 0.0002192 @iter15940) ([91m↑0.97%[0m) [0.10% of initial]
[Iter 17910/20000] Loss: 0.0002595 (Best: 0.0002192 @iter15940) ([91m↑0.35%[0m) [0.10% of initial]
[Iter 17920/20000] Loss: 0.0002710 (Best: 0.0002192 @iter15940) ([91m↑4.43%[0m) [0.11% of initial]
[Iter 17930/20000] Loss: 0.0002567 (Best: 0.0002192 @iter15940) ([92m↓5.27%[0m) [0.10% of initial]
[Iter 17940/20000] Loss: 0.0002584 (Best: 0.0002192 @iter15940) ([91m↑0.64%[0m) [0.10% of initial]
[Iter 17950/20000] Loss: 0.0002562 (Best: 0.0002192 @iter15940) ([92m↓0.85%[0m) [0.10% of initial]
[Iter 17960/20000] Loss: 0.0002655 (Best: 0.0002192 @iter15940) ([91m↑3.65%[0m) [0.11% of initial]
[Iter 17970/20000] Loss: 0.0002654 (Best: 0.0002192 @iter15940) ([92m↓0.06%[0m) [0.11% of initial]
[Iter 17980/20000] Loss: 0.0002868 (Best: 0.0002192 @iter15940) ([91m↑8.06%[0m) [0.11% of initial]
[Iter 17990/20000] Loss: 0.0002591 (Best: 0.0002192 @iter15940) ([92m↓9.64%[0m) [0.10% of initial]
Iter:17999, L1 loss=0.0003089, Total loss=0.0002661, Time:53
[Iter 18000/20000] Loss: 0.0002574 (Best: 0.0002192 @iter15940) ([92m↓0.66%[0m) [0.10% of initial]
Pruning 2 points (0.0%) from gaussian0 at iteration 18000
Pruning 1 points (0.0%) from gaussian1 at iteration 18000
[Iter 18010/20000] Loss: 0.0006537 (Best: 0.0002192 @iter15940) ([91m↑153.95%[0m) [0.26% of initial]
[Iter 18020/20000] Loss: 0.0004073 (Best: 0.0002192 @iter15940) ([92m↓37.70%[0m) [0.16% of initial]
[Iter 18030/20000] Loss: 0.0003437 (Best: 0.0002192 @iter15940) ([92m↓15.62%[0m) [0.14% of initial]
[Iter 18040/20000] Loss: 0.0002847 (Best: 0.0002192 @iter15940) ([92m↓17.17%[0m) [0.11% of initial]
[Iter 18050/20000] Loss: 0.0002651 (Best: 0.0002192 @iter15940) ([92m↓6.89%[0m) [0.11% of initial]
[Iter 18060/20000] Loss: 0.0002709 (Best: 0.0002192 @iter15940) ([91m↑2.18%[0m) [0.11% of initial]
[Iter 18070/20000] Loss: 0.0002642 (Best: 0.0002192 @iter15940) ([92m↓2.47%[0m) [0.10% of initial]
[Iter 18080/20000] Loss: 0.0002655 (Best: 0.0002192 @iter15940) ([91m↑0.52%[0m) [0.11% of initial]
[Iter 18090/20000] Loss: 0.0002862 (Best: 0.0002192 @iter15940) ([91m↑7.79%[0m) [0.11% of initial]
Iter:18099, L1 loss=0.0003421, Total loss=0.0003086, Time:51
[Iter 18100/20000] Loss: 0.0002751 (Best: 0.0002192 @iter15940) ([92m↓3.87%[0m) [0.11% of initial]
[Iter 18110/20000] Loss: 0.0002636 (Best: 0.0002192 @iter15940) ([92m↓4.21%[0m) [0.10% of initial]
[Iter 18120/20000] Loss: 0.0002599 (Best: 0.0002192 @iter15940) ([92m↓1.39%[0m) [0.10% of initial]
[Iter 18130/20000] Loss: 0.0002517 (Best: 0.0002192 @iter15940) ([92m↓3.17%[0m) [0.10% of initial]
[Iter 18140/20000] Loss: 0.0002598 (Best: 0.0002192 @iter15940) ([91m↑3.24%[0m) [0.10% of initial]
[Iter 18150/20000] Loss: 0.0002601 (Best: 0.0002192 @iter15940) ([91m↑0.12%[0m) [0.10% of initial]
[Iter 18160/20000] Loss: 0.0002513 (Best: 0.0002192 @iter15940) ([92m↓3.40%[0m) [0.10% of initial]
[Iter 18170/20000] Loss: 0.0002610 (Best: 0.0002192 @iter15940) ([91m↑3.86%[0m) [0.10% of initial]
[Iter 18180/20000] Loss: 0.0003114 (Best: 0.0002192 @iter15940) ([91m↑19.30%[0m) [0.12% of initial]
[Iter 18190/20000] Loss: 0.0003383 (Best: 0.0002192 @iter15940) ([91m↑8.65%[0m) [0.13% of initial]
Iter:18199, L1 loss=0.0003249, Total loss=0.0003006, Time:50
[Iter 18200/20000] Loss: 0.0003314 (Best: 0.0002192 @iter15940) ([92m↓2.02%[0m) [0.13% of initial]
[Iter 18210/20000] Loss: 0.0002894 (Best: 0.0002192 @iter15940) ([92m↓12.68%[0m) [0.11% of initial]
[Iter 18220/20000] Loss: 0.0002635 (Best: 0.0002192 @iter15940) ([92m↓8.94%[0m) [0.10% of initial]
[Iter 18230/20000] Loss: 0.0002992 (Best: 0.0002192 @iter15940) ([91m↑13.54%[0m) [0.12% of initial]
[Iter 18240/20000] Loss: 0.0002813 (Best: 0.0002192 @iter15940) ([92m↓5.97%[0m) [0.11% of initial]
[Iter 18250/20000] Loss: 0.0002775 (Best: 0.0002192 @iter15940) ([92m↓1.35%[0m) [0.11% of initial]
[Iter 18260/20000] Loss: 0.0002692 (Best: 0.0002192 @iter15940) ([92m↓3.01%[0m) [0.11% of initial]
[Iter 18270/20000] Loss: 0.0002742 (Best: 0.0002192 @iter15940) ([91m↑1.86%[0m) [0.11% of initial]
[Iter 18280/20000] Loss: 0.0002677 (Best: 0.0002192 @iter15940) ([92m↓2.38%[0m) [0.11% of initial]
[Iter 18290/20000] Loss: 0.0002975 (Best: 0.0002192 @iter15940) ([91m↑11.15%[0m) [0.12% of initial]
Iter:18299, L1 loss=0.0003791, Total loss=0.0003204, Time:54
[Iter 18300/20000] Loss: 0.0003028 (Best: 0.0002192 @iter15940) ([91m↑1.78%[0m) [0.12% of initial]
[Iter 18310/20000] Loss: 0.0002944 (Best: 0.0002192 @iter15940) ([92m↓2.78%[0m) [0.12% of initial]
[Iter 18320/20000] Loss: 0.0003140 (Best: 0.0002192 @iter15940) ([91m↑6.68%[0m) [0.12% of initial]
[Iter 18330/20000] Loss: 0.0003623 (Best: 0.0002192 @iter15940) ([91m↑15.36%[0m) [0.14% of initial]
[Iter 18340/20000] Loss: 0.0003532 (Best: 0.0002192 @iter15940) ([92m↓2.51%[0m) [0.14% of initial]
[Iter 18350/20000] Loss: 0.0003334 (Best: 0.0002192 @iter15940) ([92m↓5.59%[0m) [0.13% of initial]
[Iter 18360/20000] Loss: 0.0003912 (Best: 0.0002192 @iter15940) ([91m↑17.34%[0m) [0.16% of initial]
[Iter 18370/20000] Loss: 0.0003105 (Best: 0.0002192 @iter15940) ([92m↓20.63%[0m) [0.12% of initial]
[Iter 18380/20000] Loss: 0.0002755 (Best: 0.0002192 @iter15940) ([92m↓11.26%[0m) [0.11% of initial]
[Iter 18390/20000] Loss: 0.0002823 (Best: 0.0002192 @iter15940) ([91m↑2.46%[0m) [0.11% of initial]
Iter:18399, L1 loss=0.0002764, Total loss=0.0002522, Time:51
[Iter 18400/20000] Loss: 0.0002490 (Best: 0.0002192 @iter15940) ([92m↓11.80%[0m) [0.10% of initial]
[Iter 18410/20000] Loss: 0.0002592 (Best: 0.0002192 @iter15940) ([91m↑4.11%[0m) [0.10% of initial]
[Iter 18420/20000] Loss: 0.0002450 (Best: 0.0002192 @iter15940) ([92m↓5.49%[0m) [0.10% of initial]
[Iter 18430/20000] Loss: 0.0002412 (Best: 0.0002192 @iter15940) ([92m↓1.55%[0m) [0.10% of initial]
[Iter 18440/20000] Loss: 0.0002446 (Best: 0.0002192 @iter15940) ([91m↑1.39%[0m) [0.10% of initial]
[Iter 18450/20000] Loss: 0.0002500 (Best: 0.0002192 @iter15940) ([91m↑2.22%[0m) [0.10% of initial]
[Iter 18460/20000] Loss: 0.0002599 (Best: 0.0002192 @iter15940) ([91m↑3.98%[0m) [0.10% of initial]
[Iter 18470/20000] Loss: 0.0002642 (Best: 0.0002192 @iter15940) ([91m↑1.62%[0m) [0.10% of initial]
[Iter 18480/20000] Loss: 0.0002508 (Best: 0.0002192 @iter15940) ([92m↓5.07%[0m) [0.10% of initial]
[Iter 18490/20000] Loss: 0.0002625 (Best: 0.0002192 @iter15940) ([91m↑4.69%[0m) [0.10% of initial]
Iter:18499, L1 loss=0.0003, Total loss=0.0002728, Time:52
[Iter 18500/20000] Loss: 0.0002628 (Best: 0.0002192 @iter15940) ([91m↑0.10%[0m) [0.10% of initial]
Pruning 5 points (0.0%) from gaussian0 at iteration 18500
Pruning 3 points (0.0%) from gaussian1 at iteration 18500
[Iter 18510/20000] Loss: 0.0004883 (Best: 0.0002192 @iter15940) ([91m↑85.82%[0m) [0.19% of initial]
[Iter 18520/20000] Loss: 0.0003317 (Best: 0.0002192 @iter15940) ([92m↓32.06%[0m) [0.13% of initial]
[Iter 18530/20000] Loss: 0.0002844 (Best: 0.0002192 @iter15940) ([92m↓14.26%[0m) [0.11% of initial]
[Iter 18540/20000] Loss: 0.0002985 (Best: 0.0002192 @iter15940) ([91m↑4.95%[0m) [0.12% of initial]
[Iter 18550/20000] Loss: 0.0002814 (Best: 0.0002192 @iter15940) ([92m↓5.74%[0m) [0.11% of initial]
[Iter 18560/20000] Loss: 0.0002852 (Best: 0.0002192 @iter15940) ([91m↑1.35%[0m) [0.11% of initial]
[Iter 18570/20000] Loss: 0.0002890 (Best: 0.0002192 @iter15940) ([91m↑1.33%[0m) [0.11% of initial]
[Iter 18580/20000] Loss: 0.0002662 (Best: 0.0002192 @iter15940) ([92m↓7.88%[0m) [0.11% of initial]
[Iter 18590/20000] Loss: 0.0002744 (Best: 0.0002192 @iter15940) ([91m↑3.09%[0m) [0.11% of initial]
Iter:18599, L1 loss=0.0003414, Total loss=0.0003022, Time:51
[Iter 18600/20000] Loss: 0.0003059 (Best: 0.0002192 @iter15940) ([91m↑11.47%[0m) [0.12% of initial]
[Iter 18610/20000] Loss: 0.0002761 (Best: 0.0002192 @iter15940) ([92m↓9.75%[0m) [0.11% of initial]
[Iter 18620/20000] Loss: 0.0002603 (Best: 0.0002192 @iter15940) ([92m↓5.71%[0m) [0.10% of initial]
[Iter 18630/20000] Loss: 0.0003000 (Best: 0.0002192 @iter15940) ([91m↑15.25%[0m) [0.12% of initial]
[Iter 18640/20000] Loss: 0.0002779 (Best: 0.0002192 @iter15940) ([92m↓7.35%[0m) [0.11% of initial]
[Iter 18650/20000] Loss: 0.0002527 (Best: 0.0002192 @iter15940) ([92m↓9.09%[0m) [0.10% of initial]
[Iter 18660/20000] Loss: 0.0002533 (Best: 0.0002192 @iter15940) ([91m↑0.27%[0m) [0.10% of initial]
[Iter 18670/20000] Loss: 0.0002451 (Best: 0.0002192 @iter15940) ([92m↓3.27%[0m) [0.10% of initial]
[Iter 18680/20000] Loss: 0.0002441 (Best: 0.0002192 @iter15940) ([92m↓0.41%[0m) [0.10% of initial]
[Iter 18690/20000] Loss: 0.0002487 (Best: 0.0002192 @iter15940) ([91m↑1.90%[0m) [0.10% of initial]
Iter:18699, L1 loss=0.000271, Total loss=0.0002425, Time:51
[Iter 18700/20000] Loss: 0.0002362 (Best: 0.0002192 @iter15940) ([92m↓5.02%[0m) [0.09% of initial]
[Iter 18710/20000] Loss: 0.0002483 (Best: 0.0002192 @iter15940) ([91m↑5.13%[0m) [0.10% of initial]
[Iter 18720/20000] Loss: 0.0002396 (Best: 0.0002192 @iter15940) ([92m↓3.53%[0m) [0.10% of initial]
[Iter 18730/20000] Loss: 0.0002495 (Best: 0.0002192 @iter15940) ([91m↑4.16%[0m) [0.10% of initial]
[Iter 18740/20000] Loss: 0.0002514 (Best: 0.0002192 @iter15940) ([91m↑0.75%[0m) [0.10% of initial]
[Iter 18750/20000] Loss: 0.0002427 (Best: 0.0002192 @iter15940) ([92m↓3.48%[0m) [0.10% of initial]
[Iter 18760/20000] Loss: 0.0002473 (Best: 0.0002192 @iter15940) ([91m↑1.92%[0m) [0.10% of initial]
[Iter 18770/20000] Loss: 0.0002586 (Best: 0.0002192 @iter15940) ([91m↑4.56%[0m) [0.10% of initial]
[Iter 18780/20000] Loss: 0.0002616 (Best: 0.0002192 @iter15940) ([91m↑1.17%[0m) [0.10% of initial]
[Iter 18790/20000] Loss: 0.0002655 (Best: 0.0002192 @iter15940) ([91m↑1.50%[0m) [0.11% of initial]
Iter:18799, L1 loss=0.0002686, Total loss=0.0002366, Time:54
[Iter 18800/20000] Loss: 0.0002545 (Best: 0.0002192 @iter15940) ([92m↓4.17%[0m) [0.10% of initial]
[Iter 18810/20000] Loss: 0.0002703 (Best: 0.0002192 @iter15940) ([91m↑6.23%[0m) [0.11% of initial]
[Iter 18820/20000] Loss: 0.0002716 (Best: 0.0002192 @iter15940) ([91m↑0.47%[0m) [0.11% of initial]
[Iter 18830/20000] Loss: 0.0002781 (Best: 0.0002192 @iter15940) ([91m↑2.41%[0m) [0.11% of initial]
[Iter 18840/20000] Loss: 0.0002640 (Best: 0.0002192 @iter15940) ([92m↓5.09%[0m) [0.10% of initial]
[Iter 18850/20000] Loss: 0.0002583 (Best: 0.0002192 @iter15940) ([92m↓2.16%[0m) [0.10% of initial]
[Iter 18860/20000] Loss: 0.0003029 (Best: 0.0002192 @iter15940) ([91m↑17.29%[0m) [0.12% of initial]
[Iter 18870/20000] Loss: 0.0002660 (Best: 0.0002192 @iter15940) ([92m↓12.21%[0m) [0.11% of initial]
[Iter 18880/20000] Loss: 0.0002580 (Best: 0.0002192 @iter15940) ([92m↓3.00%[0m) [0.10% of initial]
[Iter 18890/20000] Loss: 0.0002622 (Best: 0.0002192 @iter15940) ([91m↑1.63%[0m) [0.10% of initial]
Iter:18899, L1 loss=0.0002974, Total loss=0.0002582, Time:54
[Iter 18900/20000] Loss: 0.0002633 (Best: 0.0002192 @iter15940) ([91m↑0.43%[0m) [0.10% of initial]
[Iter 18910/20000] Loss: 0.0002681 (Best: 0.0002192 @iter15940) ([91m↑1.83%[0m) [0.11% of initial]
[Iter 18920/20000] Loss: 0.0002453 (Best: 0.0002192 @iter15940) ([92m↓8.54%[0m) [0.10% of initial]
[Iter 18930/20000] Loss: 0.0002970 (Best: 0.0002192 @iter15940) ([91m↑21.10%[0m) [0.12% of initial]
[Iter 18940/20000] Loss: 0.0002638 (Best: 0.0002192 @iter15940) ([92m↓11.19%[0m) [0.10% of initial]
[Iter 18950/20000] Loss: 0.0002759 (Best: 0.0002192 @iter15940) ([91m↑4.60%[0m) [0.11% of initial]
[Iter 18960/20000] Loss: 0.0002711 (Best: 0.0002192 @iter15940) ([92m↓1.74%[0m) [0.11% of initial]
[Iter 18970/20000] Loss: 0.0002758 (Best: 0.0002192 @iter15940) ([91m↑1.73%[0m) [0.11% of initial]
[Iter 18980/20000] Loss: 0.0002496 (Best: 0.0002192 @iter15940) ([92m↓9.48%[0m) [0.10% of initial]
[Iter 18990/20000] Loss: 0.0002499 (Best: 0.0002192 @iter15940) ([91m↑0.10%[0m) [0.10% of initial]
Iter:18999, L1 loss=0.0002809, Total loss=0.0002458, Time:52
[Iter 19000/20000] Loss: 0.0002410 (Best: 0.0002192 @iter15940) ([92m↓3.55%[0m) [0.10% of initial]
Pruning 1 points (0.0%) from gaussian0 at iteration 19000
Pruning 3 points (0.0%) from gaussian1 at iteration 19000
[Iter 19010/20000] Loss: 0.0005909 (Best: 0.0002192 @iter15940) ([91m↑145.15%[0m) [0.23% of initial]
[Iter 19020/20000] Loss: 0.0004237 (Best: 0.0002192 @iter15940) ([92m↓28.28%[0m) [0.17% of initial]
[Iter 19030/20000] Loss: 0.0003280 (Best: 0.0002192 @iter15940) ([92m↓22.60%[0m) [0.13% of initial]
[Iter 19040/20000] Loss: 0.0002741 (Best: 0.0002192 @iter15940) ([92m↓16.43%[0m) [0.11% of initial]
[Iter 19050/20000] Loss: 0.0002604 (Best: 0.0002192 @iter15940) ([92m↓5.02%[0m) [0.10% of initial]
[Iter 19060/20000] Loss: 0.0002479 (Best: 0.0002192 @iter15940) ([92m↓4.81%[0m) [0.10% of initial]
[Iter 19070/20000] Loss: 0.0002700 (Best: 0.0002192 @iter15940) ([91m↑8.93%[0m) [0.11% of initial]
[Iter 19080/20000] Loss: 0.0002457 (Best: 0.0002192 @iter15940) ([92m↓9.01%[0m) [0.10% of initial]
[Iter 19090/20000] Loss: 0.0002455 (Best: 0.0002192 @iter15940) ([92m↓0.04%[0m) [0.10% of initial]
Iter:19099, L1 loss=0.0002754, Total loss=0.0002372, Time:54
[Iter 19100/20000] Loss: 0.0002673 (Best: 0.0002192 @iter15940) ([91m↑8.87%[0m) [0.11% of initial]
[Iter 19110/20000] Loss: 0.0002883 (Best: 0.0002192 @iter15940) ([91m↑7.85%[0m) [0.11% of initial]
[Iter 19120/20000] Loss: 0.0002514 (Best: 0.0002192 @iter15940) ([92m↓12.80%[0m) [0.10% of initial]
[Iter 19130/20000] Loss: 0.0002468 (Best: 0.0002192 @iter15940) ([92m↓1.84%[0m) [0.10% of initial]
[Iter 19140/20000] Loss: 0.0002721 (Best: 0.0002192 @iter15940) ([91m↑10.26%[0m) [0.11% of initial]
[Iter 19150/20000] Loss: 0.0002541 (Best: 0.0002192 @iter15940) ([92m↓6.60%[0m) [0.10% of initial]
[Iter 19160/20000] Loss: 0.0002546 (Best: 0.0002192 @iter15940) ([91m↑0.20%[0m) [0.10% of initial]
[Iter 19170/20000] Loss: 0.0002433 (Best: 0.0002192 @iter15940) ([92m↓4.45%[0m) [0.10% of initial]
[Iter 19180/20000] Loss: 0.0002400 (Best: 0.0002192 @iter15940) ([92m↓1.37%[0m) [0.10% of initial]
[Iter 19190/20000] Loss: 0.0002600 (Best: 0.0002192 @iter15940) ([91m↑8.34%[0m) [0.10% of initial]
Iter:19199, L1 loss=0.0003791, Total loss=0.0003018, Time:52
[Iter 19200/20000] Loss: 0.0002812 (Best: 0.0002192 @iter15940) ([91m↑8.18%[0m) [0.11% of initial]
[Iter 19210/20000] Loss: 0.0002594 (Best: 0.0002192 @iter15940) ([92m↓7.78%[0m) [0.10% of initial]
[Iter 19220/20000] Loss: 0.0002789 (Best: 0.0002192 @iter15940) ([91m↑7.54%[0m) [0.11% of initial]
[Iter 19230/20000] Loss: 0.0002412 (Best: 0.0002192 @iter15940) ([92m↓13.53%[0m) [0.10% of initial]
[Iter 19240/20000] Loss: 0.0002306 (Best: 0.0002143 @iter19240) ([92m↓4.38%[0m) [0.09% of initial]
[Iter 19250/20000] Loss: 0.0002423 (Best: 0.0002143 @iter19240) ([91m↑5.05%[0m) [0.10% of initial]
[Iter 19260/20000] Loss: 0.0002417 (Best: 0.0002143 @iter19240) ([92m↓0.23%[0m) [0.10% of initial]
[Iter 19270/20000] Loss: 0.0002556 (Best: 0.0002143 @iter19240) ([91m↑5.75%[0m) [0.10% of initial]
[Iter 19280/20000] Loss: 0.0002632 (Best: 0.0002143 @iter19240) ([91m↑2.96%[0m) [0.10% of initial]
[Iter 19290/20000] Loss: 0.0002475 (Best: 0.0002143 @iter19240) ([92m↓5.97%[0m) [0.10% of initial]
Iter:19299, L1 loss=0.0002819, Total loss=0.000239, Time:54
[Iter 19300/20000] Loss: 0.0002412 (Best: 0.0002143 @iter19240) ([92m↓2.54%[0m) [0.10% of initial]
[Iter 19310/20000] Loss: 0.0002537 (Best: 0.0002143 @iter19240) ([91m↑5.21%[0m) [0.10% of initial]
[Iter 19320/20000] Loss: 0.0002542 (Best: 0.0002143 @iter19240) ([91m↑0.19%[0m) [0.10% of initial]
[Iter 19330/20000] Loss: 0.0002710 (Best: 0.0002143 @iter19240) ([91m↑6.62%[0m) [0.11% of initial]
[Iter 19340/20000] Loss: 0.0003074 (Best: 0.0002143 @iter19240) ([91m↑13.41%[0m) [0.12% of initial]
[Iter 19350/20000] Loss: 0.0003476 (Best: 0.0002143 @iter19240) ([91m↑13.08%[0m) [0.14% of initial]
[Iter 19360/20000] Loss: 0.0003126 (Best: 0.0002143 @iter19240) ([92m↓10.07%[0m) [0.12% of initial]
[Iter 19370/20000] Loss: 0.0003070 (Best: 0.0002143 @iter19240) ([92m↓1.78%[0m) [0.12% of initial]
[Iter 19380/20000] Loss: 0.0003277 (Best: 0.0002143 @iter19240) ([91m↑6.73%[0m) [0.13% of initial]
[Iter 19390/20000] Loss: 0.0002668 (Best: 0.0002143 @iter19240) ([92m↓18.57%[0m) [0.11% of initial]
Iter:19399, L1 loss=0.0002733, Total loss=0.000246, Time:53
[Iter 19400/20000] Loss: 0.0002605 (Best: 0.0002143 @iter19240) ([92m↓2.36%[0m) [0.10% of initial]
[Iter 19410/20000] Loss: 0.0002358 (Best: 0.0002143 @iter19240) ([92m↓9.48%[0m) [0.09% of initial]
[Iter 19420/20000] Loss: 0.0002347 (Best: 0.0002143 @iter19240) ([92m↓0.46%[0m) [0.09% of initial]
[Iter 19430/20000] Loss: 0.0002256 (Best: 0.0002143 @iter19240) ([92m↓3.89%[0m) [0.09% of initial]
[Iter 19440/20000] Loss: 0.0002390 (Best: 0.0002143 @iter19240) ([91m↑5.91%[0m) [0.09% of initial]
[Iter 19450/20000] Loss: 0.0002460 (Best: 0.0002143 @iter19240) ([91m↑2.94%[0m) [0.10% of initial]
[Iter 19460/20000] Loss: 0.0003031 (Best: 0.0002143 @iter19240) ([91m↑23.21%[0m) [0.12% of initial]
[Iter 19470/20000] Loss: 0.0003613 (Best: 0.0002143 @iter19240) ([91m↑19.22%[0m) [0.14% of initial]
[Iter 19480/20000] Loss: 0.0004148 (Best: 0.0002143 @iter19240) ([91m↑14.80%[0m) [0.16% of initial]
[Iter 19490/20000] Loss: 0.0003404 (Best: 0.0002143 @iter19240) ([92m↓17.94%[0m) [0.14% of initial]
Iter:19499, L1 loss=0.0003059, Total loss=0.0002834, Time:51
[Iter 19500/20000] Loss: 0.0003154 (Best: 0.0002143 @iter19240) ([92m↓7.35%[0m) [0.13% of initial]
Pruning 1 points (0.0%) from gaussian0 at iteration 19500
Pruning 1 points (0.0%) from gaussian1 at iteration 19500
[Iter 19510/20000] Loss: 0.0005309 (Best: 0.0002143 @iter19240) ([91m↑68.33%[0m) [0.21% of initial]
[Iter 19520/20000] Loss: 0.0003514 (Best: 0.0002143 @iter19240) ([92m↓33.81%[0m) [0.14% of initial]
[Iter 19530/20000] Loss: 0.0002937 (Best: 0.0002143 @iter19240) ([92m↓16.42%[0m) [0.12% of initial]
[Iter 19540/20000] Loss: 0.0002505 (Best: 0.0002143 @iter19240) ([92m↓14.70%[0m) [0.10% of initial]
[Iter 19550/20000] Loss: 0.0002519 (Best: 0.0002143 @iter19240) ([91m↑0.53%[0m) [0.10% of initial]
[Iter 19560/20000] Loss: 0.0002441 (Best: 0.0002143 @iter19240) ([92m↓3.08%[0m) [0.10% of initial]
[Iter 19570/20000] Loss: 0.0002228 (Best: 0.0002124 @iter19570) ([92m↓8.72%[0m) [0.09% of initial]
[Iter 19580/20000] Loss: 0.0002581 (Best: 0.0002124 @iter19570) ([91m↑15.83%[0m) [0.10% of initial]
[Iter 19590/20000] Loss: 0.0002362 (Best: 0.0002124 @iter19570) ([92m↓8.48%[0m) [0.09% of initial]
Iter:19599, L1 loss=0.0002804, Total loss=0.0002357, Time:49
[Iter 19600/20000] Loss: 0.0002352 (Best: 0.0002124 @iter19570) ([92m↓0.43%[0m) [0.09% of initial]
[Iter 19610/20000] Loss: 0.0002485 (Best: 0.0002124 @iter19570) ([91m↑5.64%[0m) [0.10% of initial]
[Iter 19620/20000] Loss: 0.0002759 (Best: 0.0002124 @iter19570) ([91m↑11.05%[0m) [0.11% of initial]
[Iter 19630/20000] Loss: 0.0002603 (Best: 0.0002124 @iter19570) ([92m↓5.66%[0m) [0.10% of initial]
[Iter 19640/20000] Loss: 0.0002420 (Best: 0.0002124 @iter19570) ([92m↓7.04%[0m) [0.10% of initial]
[Iter 19650/20000] Loss: 0.0002635 (Best: 0.0002124 @iter19570) ([91m↑8.91%[0m) [0.10% of initial]
[Iter 19660/20000] Loss: 0.0002319 (Best: 0.0002124 @iter19570) ([92m↓12.00%[0m) [0.09% of initial]
[Iter 19670/20000] Loss: 0.0002300 (Best: 0.0002124 @iter19570) ([92m↓0.83%[0m) [0.09% of initial]
[Iter 19680/20000] Loss: 0.0002342 (Best: 0.0002124 @iter19570) ([91m↑1.85%[0m) [0.09% of initial]
[Iter 19690/20000] Loss: 0.0002308 (Best: 0.0002124 @iter19570) ([92m↓1.48%[0m) [0.09% of initial]
Iter:19699, L1 loss=0.0002702, Total loss=0.000245, Time:48
[Iter 19700/20000] Loss: 0.0002415 (Best: 0.0002124 @iter19570) ([91m↑4.66%[0m) [0.10% of initial]
[Iter 19710/20000] Loss: 0.0002608 (Best: 0.0002124 @iter19570) ([91m↑7.99%[0m) [0.10% of initial]
[Iter 19720/20000] Loss: 0.0002411 (Best: 0.0002124 @iter19570) ([92m↓7.57%[0m) [0.10% of initial]
[Iter 19730/20000] Loss: 0.0002410 (Best: 0.0002124 @iter19570) ([92m↓0.02%[0m) [0.10% of initial]
[Iter 19740/20000] Loss: 0.0002333 (Best: 0.0002124 @iter19570) ([92m↓3.21%[0m) [0.09% of initial]
[Iter 19750/20000] Loss: 0.0002272 (Best: 0.0002124 @iter19570) ([92m↓2.62%[0m) [0.09% of initial]
[Iter 19760/20000] Loss: 0.0002301 (Best: 0.0002124 @iter19570) ([91m↑1.29%[0m) [0.09% of initial]
[Iter 19770/20000] Loss: 0.0002307 (Best: 0.0002124 @iter19570) ([91m↑0.27%[0m) [0.09% of initial]
[Iter 19780/20000] Loss: 0.0002441 (Best: 0.0002124 @iter19570) ([91m↑5.80%[0m) [0.10% of initial]
[Iter 19790/20000] Loss: 0.0002304 (Best: 0.0002124 @iter19570) ([92m↓5.61%[0m) [0.09% of initial]
Iter:19799, L1 loss=0.0003008, Total loss=0.0002491, Time:48
[Iter 19800/20000] Loss: 0.0002369 (Best: 0.0002124 @iter19570) ([91m↑2.84%[0m) [0.09% of initial]
[Iter 19810/20000] Loss: 0.0002445 (Best: 0.0002124 @iter19570) ([91m↑3.19%[0m) [0.10% of initial]
[Iter 19820/20000] Loss: 0.0002433 (Best: 0.0002124 @iter19570) ([92m↓0.48%[0m) [0.10% of initial]
[Iter 19830/20000] Loss: 0.0002998 (Best: 0.0002124 @iter19570) ([91m↑23.22%[0m) [0.12% of initial]
[Iter 19840/20000] Loss: 0.0002934 (Best: 0.0002124 @iter19570) ([92m↓2.15%[0m) [0.12% of initial]
[Iter 19850/20000] Loss: 0.0002762 (Best: 0.0002124 @iter19570) ([92m↓5.85%[0m) [0.11% of initial]
[Iter 19860/20000] Loss: 0.0003293 (Best: 0.0002124 @iter19570) ([91m↑19.24%[0m) [0.13% of initial]
[Iter 19870/20000] Loss: 0.0003113 (Best: 0.0002124 @iter19570) ([92m↓5.47%[0m) [0.12% of initial]
[Iter 19880/20000] Loss: 0.0003059 (Best: 0.0002124 @iter19570) ([92m↓1.75%[0m) [0.12% of initial]
[Iter 19890/20000] Loss: 0.0003403 (Best: 0.0002124 @iter19570) ([91m↑11.27%[0m) [0.14% of initial]
Iter:19899, L1 loss=0.0003168, Total loss=0.0002744, Time:52
[Iter 19900/20000] Loss: 0.0002738 (Best: 0.0002124 @iter19570) ([92m↓19.54%[0m) [0.11% of initial]
[Iter 19910/20000] Loss: 0.0003052 (Best: 0.0002124 @iter19570) ([91m↑11.46%[0m) [0.12% of initial]
[Iter 19920/20000] Loss: 0.0003009 (Best: 0.0002124 @iter19570) ([92m↓1.43%[0m) [0.12% of initial]
[Iter 19930/20000] Loss: 0.0002882 (Best: 0.0002124 @iter19570) ([92m↓4.19%[0m) [0.11% of initial]
[Iter 19940/20000] Loss: 0.0003065 (Best: 0.0002124 @iter19570) ([91m↑6.34%[0m) [0.12% of initial]
[Iter 19950/20000] Loss: 0.0002963 (Best: 0.0002124 @iter19570) ([92m↓3.32%[0m) [0.12% of initial]
[Iter 19960/20000] Loss: 0.0002796 (Best: 0.0002124 @iter19570) ([92m↓5.64%[0m) [0.11% of initial]
[Iter 19970/20000] Loss: 0.0002759 (Best: 0.0002124 @iter19570) ([92m↓1.35%[0m) [0.11% of initial]
[Iter 19980/20000] Loss: 0.0002717 (Best: 0.0002124 @iter19570) ([92m↓1.50%[0m) [0.11% of initial]
[Iter 19990/20000] Loss: 0.0002313 (Best: 0.0002124 @iter19570) ([92m↓14.89%[0m) [0.09% of initial]
Iter:19999, L1 loss=0.0002625, Total loss=0.0002325, Time:46
[Iter 20000/20000] Loss: 0.0002322 (Best: 0.0002124 @iter19570) ([91m↑0.40%[0m) [0.09% of initial]
Testing Speed: 67.73908678695808 fps
Testing Time: 0.738126277923584 s

[ITER 20000] Evaluating test: SSIM = 0.9273344993591309, PSNR = 21.19315607070923
Testing Speed: 78.44414797451466 fps
Testing Time: 0.038243770599365234 s

[ITER 20000] Evaluating train: SSIM = 0.9999991456667582, PSNR = 65.80201975504556
Iter:20000, total_points:183237

[ITER 20000] Saving Gaussians
Pruning 2 points (0.0%) from gaussian0 at iteration 20000
Pruning 4 points (0.0%) from gaussian1 at iteration 20000

Training completed!
==================================================
Body part: chest
Testing Speed: 67 fps
Total time: 24.44 minutes
Test SSIM: 0.9273
Test PSNR: 21.193
Gaussian0 final points count: 183235
Gaussian1 final points count: 183266
Final loss: 0.0002322 (0.09% of initial)
Save path: 2024_11_26_14_53_04
Initial loss: 0.2517052
Best loss: 0.0002124 @iteration 19570 (0.08% of initial)
Train SSIM: 1.0000
Train PSNR: 65.802
==================================================
Final pseudo view loss: 0.0000000
==================================================

Training complete.
Optimizing 
Training parameters: {'iterations': 20000, 'position_lr_init': 0.00019, 'position_lr_final': 1.9e-06, 'position_lr_delay_mult': 0.01, 'position_lr_max_steps': 20000, 'feature_lr': 0.002, 'opacity_lr': 0.008, 'scaling_lr': 0.005, 'rotation_lr': 0.001, 'percent_dense': 0.01, 'lambda_dssim': 0.2, 'densification_interval': 200, 'opacity_reset_interval': 4000, 'densify_from_iter': 500, 'densify_until_iter': 8000, 'densify_grad_threshold': 2.6e-05, 'random_background': False, 'sample_pseudo_interval': 1, 'start_sample_pseudo': 0, 'end_sample_pseudo': 30000}
Create gaussians1
GsDict.keys() is dict_keys(['gs0', 'gs1'])

Starting training...
Total iterations: 20000
==================================================
Optimizing 
Training parameters: {'iterations': 20000, 'position_lr_init': 0.00019, 'position_lr_final': 1.9e-06, 'position_lr_delay_mult': 0.01, 'position_lr_max_steps': 20000, 'feature_lr': 0.002, 'opacity_lr': 0.008, 'scaling_lr': 0.005, 'rotation_lr': 0.001, 'percent_dense': 0.01, 'lambda_dssim': 0.2, 'densification_interval': 200, 'opacity_reset_interval': 4000, 'densify_from_iter': 500, 'densify_until_iter': 8000, 'densify_grad_threshold': 2.6e-05, 'random_background': False, 'sample_pseudo_interval': 1, 'start_sample_pseudo': 0, 'end_sample_pseudo': 30000}
Create gaussians1
GsDict.keys() is dict_keys(['gs0', 'gs1'])

Starting training...
Total iterations: 20000
==================================================
Optimizing 
Training parameters: {'iterations': 20000, 'position_lr_init': 0.00019, 'position_lr_final': 1.9e-06, 'position_lr_delay_mult': 0.01, 'position_lr_max_steps': 20000, 'feature_lr': 0.002, 'opacity_lr': 0.008, 'scaling_lr': 0.005, 'rotation_lr': 0.001, 'percent_dense': 0.01, 'lambda_dssim': 0.2, 'densification_interval': 200, 'opacity_reset_interval': 4000, 'densify_from_iter': 500, 'densify_until_iter': 8000, 'densify_grad_threshold': 2.6e-05, 'random_background': False, 'sample_pseudo_interval': 1, 'start_sample_pseudo': 0, 'end_sample_pseudo': 30000}
Create gaussians1
GsDict.keys() is dict_keys(['gs0', 'gs1'])

Starting training...
Total iterations: 20000
==================================================
Optimizing 
Training parameters: {'iterations': 20000, 'position_lr_init': 0.00019, 'position_lr_final': 1.9e-06, 'position_lr_delay_mult': 0.01, 'position_lr_max_steps': 20000, 'feature_lr': 0.002, 'opacity_lr': 0.008, 'scaling_lr': 0.005, 'rotation_lr': 0.001, 'percent_dense': 0.01, 'lambda_dssim': 0.2, 'densification_interval': 200, 'opacity_reset_interval': 4000, 'densify_from_iter': 500, 'densify_until_iter': 8000, 'densify_grad_threshold': 2.6e-05, 'random_background': False, 'sample_pseudo_interval': 1, 'start_sample_pseudo': 0, 'end_sample_pseudo': 30000}
Create gaussians1
GsDict.keys() is dict_keys(['gs0', 'gs1'])

Starting training...
Total iterations: 20000
==================================================
[Iter 10/20000] Loss: 0.4272577 (Best: 0.4221882 @iter10) [100.00% of initial]
[Iter 20/20000] Loss: 0.3858143 (Best: 0.3798439 @iter19) ([92m↓9.70%[0m) [83.69% of initial]
[Iter 30/20000] Loss: 0.3362649 (Best: 0.3274108 @iter30) ([92m↓12.84%[0m) [72.94% of initial]
[Iter 40/20000] Loss: 0.2812108 (Best: 0.2715389 @iter40) ([92m↓16.37%[0m) [61.00% of initial]
[Iter 50/20000] Loss: 0.2309219 (Best: 0.2239401 @iter50) ([92m↓17.88%[0m) [50.09% of initial]
[Iter 60/20000] Loss: 0.1870836 (Best: 0.1816229 @iter60) ([92m↓18.98%[0m) [40.58% of initial]
[Iter 70/20000] Loss: 0.1485783 (Best: 0.1438986 @iter70) ([92m↓20.58%[0m) [32.23% of initial]
[Iter 80/20000] Loss: 0.1185747 (Best: 0.1151163 @iter79) ([92m↓20.19%[0m) [25.72% of initial]
[Iter 90/20000] Loss: 0.0996288 (Best: 0.0952262 @iter89) ([92m↓15.98%[0m) [21.61% of initial]
Iter:99, L1 loss=0.05612, Total loss=0.0859, Time:19
[Iter 100/20000] Loss: 0.0852998 (Best: 0.0817075 @iter98) ([92m↓14.38%[0m) [18.50% of initial]
[Iter 110/20000] Loss: 0.0720435 (Best: 0.0685760 @iter109) ([92m↓15.54%[0m) [15.63% of initial]
[Iter 120/20000] Loss: 0.0621897 (Best: 0.0587540 @iter120) ([92m↓13.68%[0m) [13.49% of initial]
[Iter 130/20000] Loss: 0.0554251 (Best: 0.0526331 @iter129) ([92m↓10.88%[0m) [12.02% of initial]
[Iter 140/20000] Loss: 0.0498623 (Best: 0.0473141 @iter139) ([92m↓10.04%[0m) [10.82% of initial]
[Iter 150/20000] Loss: 0.0462464 (Best: 0.0433821 @iter149) ([92m↓7.25%[0m) [10.03% of initial]
[Iter 160/20000] Loss: 0.0420008 (Best: 0.0400462 @iter160) ([92m↓9.18%[0m) [9.11% of initial]
[Iter 170/20000] Loss: 0.0395048 (Best: 0.0376183 @iter170) ([92m↓5.94%[0m) [8.57% of initial]
[Iter 180/20000] Loss: 0.0383310 (Best: 0.0358858 @iter178) ([92m↓2.97%[0m) [8.31% of initial]
[Iter 190/20000] Loss: 0.0369114 (Best: 0.0341638 @iter187) ([92m↓3.70%[0m) [8.01% of initial]
Iter:199, L1 loss=0.02038, Total loss=0.03216, Time:16
[Iter 200/20000] Loss: 0.0342124 (Best: 0.0321637 @iter199) ([92m↓7.31%[0m) [7.42% of initial]
[Iter 210/20000] Loss: 0.0330080 (Best: 0.0309398 @iter208) ([92m↓3.52%[0m) [7.16% of initial]
[Iter 220/20000] Loss: 0.0307644 (Best: 0.0295579 @iter220) ([92m↓6.80%[0m) [6.67% of initial]
[Iter 230/20000] Loss: 0.0302543 (Best: 0.0286542 @iter229) ([92m↓1.66%[0m) [6.56% of initial]
[Iter 240/20000] Loss: 0.0294089 (Best: 0.0277329 @iter239) ([92m↓2.79%[0m) [6.38% of initial]
[Iter 250/20000] Loss: 0.0289554 (Best: 0.0270381 @iter247) ([92m↓1.54%[0m) [6.28% of initial]
[Iter 260/20000] Loss: 0.0277337 (Best: 0.0261494 @iter258) ([92m↓4.22%[0m) [6.02% of initial]
[Iter 270/20000] Loss: 0.0268530 (Best: 0.0253660 @iter268) ([92m↓3.18%[0m) [5.83% of initial]
[Iter 280/20000] Loss: 0.0254823 (Best: 0.0245091 @iter280) ([92m↓5.10%[0m) [5.53% of initial]
[Iter 290/20000] Loss: 0.0251244 (Best: 0.0239073 @iter289) ([92m↓1.40%[0m) [5.45% of initial]
Iter:299, L1 loss=0.01436, Total loss=0.02326, Time:13
[Iter 300/20000] Loss: 0.0244455 (Best: 0.0232612 @iter299) ([92m↓2.70%[0m) [5.30% of initial]
[Iter 310/20000] Loss: 0.0237670 (Best: 0.0226627 @iter309) ([92m↓2.78%[0m) [5.16% of initial]
[Iter 320/20000] Loss: 0.0234300 (Best: 0.0221433 @iter318) ([92m↓1.42%[0m) [5.08% of initial]
[Iter 330/20000] Loss: 0.0225528 (Best: 0.0215500 @iter329) ([92m↓3.74%[0m) [4.89% of initial]
[Iter 340/20000] Loss: 0.0222240 (Best: 0.0211285 @iter337) ([92m↓1.46%[0m) [4.82% of initial]
[Iter 350/20000] Loss: 0.0211086 (Best: 0.0204850 @iter350) ([92m↓5.02%[0m) [4.58% of initial]
[Iter 360/20000] Loss: 0.0212243 (Best: 0.0200529 @iter359) ([91m↑0.55%[0m) [4.60% of initial]
[Iter 370/20000] Loss: 0.0203080 (Best: 0.0195307 @iter370) ([92m↓4.32%[0m) [4.41% of initial]
[Iter 380/20000] Loss: 0.0204408 (Best: 0.0192653 @iter376) ([91m↑0.65%[0m) [4.43% of initial]
[Iter 390/20000] Loss: 0.0196714 (Best: 0.0187493 @iter388) ([92m↓3.76%[0m) [4.27% of initial]
Iter:399, L1 loss=0.01213, Total loss=0.01987, Time:21
[Iter 400/20000] Loss: 0.0192066 (Best: 0.0183368 @iter398) ([92m↓2.36%[0m) [4.17% of initial]
[Iter 410/20000] Loss: 0.0189711 (Best: 0.0180099 @iter406) ([92m↓1.23%[0m) [4.12% of initial]
[Iter 420/20000] Loss: 0.0181610 (Best: 0.0174679 @iter420) ([92m↓4.27%[0m) [3.94% of initial]
[Iter 430/20000] Loss: 0.0179912 (Best: 0.0172053 @iter427) ([92m↓0.93%[0m) [3.90% of initial]
[Iter 440/20000] Loss: 0.0173675 (Best: 0.0167213 @iter440) ([92m↓3.47%[0m) [3.77% of initial]
[Iter 450/20000] Loss: 0.0169879 (Best: 0.0163914 @iter450) ([92m↓2.19%[0m) [3.69% of initial]
